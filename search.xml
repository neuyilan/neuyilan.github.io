<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[IOTDB-Cluster-源码解析-查询]]></title>
    <url>%2F2020%2F04%2F15%2FIOTDB-Cluster-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[单机查询流程QueryPlan类：QueryPlan是有关查询相关的类，所有的plan都继承了PhysicalPlan这个虚类，QueryPlan也是一个虚类。其UML类图如下所示： IotDB中的各种reader此章节请酌情参考查询基础组件IoTDB中有太多的reader，容易让人迷乱，此处给其分类然后逐个攻破。从类别上来看，IoTDB中的reader主要有3类，tsfile、server以及cluster模块的。 下面分别进行讲解一下： Tsfile模块的reader TODO Server模块的reader原始数据查询接口原始数据查询接口，返回 BatchData，可带时间过滤条件或值过滤条件，两种过滤不可同时存在 如上是server模块的reader的UML类图。IBatchReader和ManagedSeriesReader都是接口，其中IBatchReader是Tsfile模块中的。 IBatchReader接口函数如下： 12345// 判断是否还有 BatchDataboolean hasNextBatch() throws IOException;// 获得下一个 BatchData，并把游标后移BatchData nextBatch() throws IOException; 使用流程如下：123456while (batchReader.hasNextBatch()) &#123; BatchData batchData = batchReader.nextBatch(); // use batchData to do some work ...&#125; 实现类是SeriesRawDataBatchReader，主要属性和方法如下：12345678// 这是这个类的精髓所在，所有的数据的读取都是由seriesReader发起的。private final SeriesReader seriesReader;private boolean hasRemaining;private boolean managedByQueryManager;private BatchData batchData;private boolean hasCachedBatchData = false; 主要方法：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * This method overrides the AbstractDataReader.hasNextOverlappedPage for pause reads, to achieve * a continuous read */@Overridepublic boolean hasNextBatch() throws IOException &#123; // 首先判断是否还有缓存好的数据。如果有则直接消费 if (hasCachedBatchData) &#123; return true; &#125; /* * consume page data firstly */ // 读page的数据 if (readPageData()) &#123; hasCachedBatchData = true; return true; &#125; /* * consume chunk data secondly */ // 读chunk的数据 if (readChunkData()) &#123; hasCachedBatchData = true; return true; &#125; /* * consume next file finally */ // 读文件的数据 while (seriesReader.hasNextFile()) &#123; if (readChunkData()) &#123; hasCachedBatchData = true; return true; &#125; &#125; return hasCachedBatchData;&#125;@Override// 很简单，无需多说。public BatchData nextBatch() throws IOException &#123; if (hasCachedBatchData || hasNextBatch()) &#123; hasCachedBatchData = false; return batchData; &#125; throw new IOException(&quot;no next batch&quot;);&#125; 聚合查询接口聚合查询接口 （主要用于聚合查询和降采样查询）UML类图如上所示，方法也都是围绕着一个tsfile的3大部分展开的：file、chunk和page。除此之外，每部分都有statistics相关的内容，便于快速聚合函数的计算。 SeriesAggregateReader类中属性就一个，也是最重要的一个就是SeriesReader，所有的方法也都是调用的SeriesReader的方法。 按递增时间戳查询按递增时间戳查询对应值的接口（主要用于带值过滤的查询） Cluster模块的readerBatchData类：封装了存取数据的一些操作。 本质上是一个二维的数组List&lt;Object[]&gt;，第一维是list，长度可变，长度无限制；第二维是长度初始为16，最大为1000的数组，在没超过阈值1000的情况下，会以2倍的速度扩张。如果达到了阈值1000，则会调用list.add(Object[])，此时object数组的长度就是1000了。 SeriesReader类分析参考http://iotdb.apache.org/zh/SystemDesign/5-DataQuery/2-SeriesReader.html 数据按照粒度从大到小分成五种：文件，TimeSeriesMetadata，Chunk，Page，相交数据点。在原始数据查询中，最大的数据块返回粒度是一个 page，如果一个 page 和其他 page 由于乱序写入相互覆盖了，就解开成数据点做合并。聚合查询中优先使用 Chunk 的统计信息，其次是 Page 的统计信息，最后是相交数据点。 设计原则是能用粒度大的就不用粒度小的。 SeriesReader类中几个重要的字段：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/* * file cache，文件层的cache，分别保存了顺序和乱序的一些文件描述 */private final List&lt;TsFileResource&gt; seqFileResource;private final List&lt;TsFileResource&gt; unseqFileResource;/* * TimeSeriesMetadata cache */// 第一个TimeSeriesMetadata，这个是开始时间最小的那个timeseriesMetaprivate TimeseriesMetadata firstTimeSeriesMetadata;// 保存了顺序文件的TimeseriesMetadataprivate final List&lt;TimeseriesMetadata&gt; seqTimeSeriesMetadata = new LinkedList&lt;&gt;();// 保存了乱序文件的TimeseriesMetadata，是一个优先级队列，按照开始时间排序private final PriorityQueue&lt;TimeseriesMetadata&gt; unSeqTimeSeriesMetadata = new PriorityQueue&lt;&gt;(Comparator.comparingLong(timeSeriesMetadata -&gt; timeSeriesMetadata.getStatistics().getStartTime()));/* * chunk cache */// 开始时间最小的那个chunkMetaDataprivate ChunkMetadata firstChunkMetadata;// 按照开始时间的一个优先级队列，保存ChunkMetaData。private final PriorityQueue&lt;ChunkMetadata&gt; cachedChunkMetadata = new PriorityQueue&lt;&gt;(Comparator.comparingLong(ChunkMetadata::getStartTime));/* * page cache */// 开始时间最小的那个VersionPageReaderprivate VersionPageReader firstPageReader;// 按照开始时间的一个优先级队列，保存的VersionPageReader。private PriorityQueue&lt;VersionPageReader&gt; cachedPageReaders = new PriorityQueue&lt;&gt;(Comparator.comparingLong(VersionPageReader::getStartTime));/* * point cache */// 相交数据点层private PriorityMergeReader mergeReader = new PriorityMergeReader();/* * result cache，相交数据点产出结果的缓存 */// 是否缓存了下一个batchprivate boolean hasCachedNextOverlappedPage;// 缓存的下一个batch的引用private BatchData cachedBatchData; 重点函数分析基本上读操作的流程如下：123456789while (seriesReader.hasNextFile()) &#123; // do something while (seriesReader.hasNextChunk()) &#123; // do something while (seriesReader.hasNextPage()) &#123; // do something &#125; &#125;&#125; 下面就会分析上述三个方法。 hasNextFile 判断本文件是否读完：如果firstPageReader还没读完，或是相交数据点还有数据，或是cachedPageReaders不为空，则说明上一个文件还未读完。则抛出异常。 判断本chunk是否读完：如果firstChunkMetadata != null||!cachedChunkMetadata.isEmpty()，则抛出异常。 判断是否还有文件。如果firstTimeSeriesMetadata != null，则返回true，否则执行第4步。 走到这里说明已经解开的page、chunk、page都读完了，所以需要解(unpack)下一个文件，会调用tryToUnpackAllOverlappedFilesToTimeSeriesMetadata函数解下一个文件。 第4步解完数据之后，在判断firstTimeSeriesMetadata != null，决定是否还有文件。 hasNextChunk主要功能：判断该时间序列还有没有下一个chunk。 约束：在调用这个方法前，需要保证 SeriesReader 内已经没有 page 和 数据点 层级的数据了，也就是之前解开的 chunk 都消耗完了。 判断文件是否读完：与hasNextFile步骤1一致。(代码写的不好，没有把这个判断条件封装为一个函数) 如果 firstChunkMetaData 不为空，则代表当前已经缓存了第一个 ChunkMetaData，且未被使用，直接返回true； 尝试去解开第一个顺序文件和第一个乱序文件，填充 chunk 层。并解开与 firstChunkMetadata 相重合的所有文件。原理与解开TimeSeriesMetadata一致。 tryToUnpackAllOverlappedFilesToTimeSeriesMetadata解压所有重叠的seq/unseq文件，找到第一个TimeSeriesMetadata，因为在用户使用的场景中可能有太多的文件，无法一次性把所有的文件都打开，这可能导致OOM，所以一次只解压缩一个文件。 填充seqTimeSeriesMetadata，直到它不为空。如果seqTimeSeriesMetadata为空，则按序遍历seqFileResource，去获取此文件的timeseriesMetadata，直到找到一个timeseriesMetadata!=null, 然后加入到seqTimeSeriesMetadata中。备注：此时seqTimeSeriesMetadata长度长度为1 同理填充unSeqTimeSeriesMetadata，直到它不为空。 找出seqTimeSeriesMetadata和unSeqTimeSeriesMetadata两个中，开始时间最早的那个timeseriesMetadata。 然后调用unpackAllOverlappedTsFilesToTimeSeriesMetadata函数，遍历所有的seqFileResource和unseqFileResource，找出所有的和第3步的timeseriesMetadata有重叠的timeseriesMetadata，分别填充到seqTimeSeriesMetadata和unSeqTimeSeriesMetadata。 重叠的定义如下：只要这个file的开始时间小于第三步找到的那个timeseriesMetadata结束时间，则认为数据在时间上有重叠的。 从填充的seqTimeSeriesMetadata和unSeqTimeSeriesMetadata中，找出开始时间最早的那个作为firstTimeSeriesMetadata。(个人认为其实没必要，因为在第3步中已经找出来了) 分布式查询流程从Cli是怎么路由到IotDB Server处理？ Client模块 Client::main()函数，首先解析下host，port，username等参数，根据这些参数建立conn； 建立conn之后，走到了receiveCommands函数，此函数是一个while(true)，处理用户输入的一些sql； Client::receiveCommands-&gt;AbstractClient::processCommand-&gt;AbstractClient::handleInputCmd。(做一些参数解析(主要是help，SET_TIMESTAMP_DISPLAY，SET_TIME_ZONE等操作)) -&gt; AbstractClient::executeQuery -&gt; IoTDBStatement::execute -&gt;IoTDBStatement::executeSQL 下面分析下IoTDBStatement中的executeSQL函数，此函数首先构造了一个TSExecuteStatementReq execReq，execReq封装了sql等信息，然后调用client.executeStatement(execReq)这个rpc接口，client就是调用的本地的单机的6667端口的JDBC服务。 executeStatement的处理逻辑 做一些参数校验，权限校验的工作。 解析SQL为PhysicalPlan。然后根据plan是query还是update，执行不同的分支，由于此篇文章讲解query的流程。所以会走到internalExecuteQueryStatement。这个函数的主要任务其实就是创建QueryDataSet，然后作为TSExecuteStatementResp的一个属性返回。 createQueryDataSet。最终调用了executor.processQuery，根据executor是ClusterPlanExecutor还是PlanExecutor，决定走cluster流程还是单机流程。(不过目前实现都是一样的) processDataQuery。会根据query的种类(groupBy、groupByFill、Aggregate、FillQuery、LastQuery、RawDataQuery)等走不同的分支，在此以RawDataQueryPlan为例。 QueryRouter::rawDataQuery() 首先对之前生成PhysicalPlan一起生成的过滤表达式IExpression进行优化，具体的优化算法请参考：过滤条件和查询表达式 根据filter是GLOBAL_TIME(全局只有一个时间的过滤条件)，还是有值的过滤条件，走不同的分支，我们以值过滤条件为例，走到了RawDataQueryExecutor::executeWithValueFilter() 创建TimeGenerator，这里面主要有3个属性： HashMap&lt;Path, List&gt; leafCache，根据过滤表达式IExpression构造出来的，key是seriesPath，value是List，LeafNode是一个包装了一个IBatchReader的类； Node operatorNode，是一个树，每个叶子节点都是一个LeafNode。 boolean hasOrNode，这颗树是否有or 节点。 上面提到LeafNode里面包含一个IBatchReader，那么这个IBatchReader是怎么创建的呢？请参考下面具体解析。 遍历所有的序列，为所有的序列调用getReaderByTimestamp创建SeriesReaderByTimestamp. 调用new RawQueryDataSetWithValueFilter()返回。 LeafNode中的IBatchReader创建逻辑在TimeGenerator::construct函数中，调用了generateNewBatchReader来创建TimeGenerator，目前主要有3种实现：ClusterTimeGenerator、ServerTimeGenerator以及TsFileTimeGenerator。 对于每个leafNode，其对应一个SingleSeriesExpression，也对应一个IBatchReader ClusterTimeGenerator::generateNewBatchReader 主要函数：MetaGroupMember::getSeriesReader，首先根据filter和path构造出List。如果没有时间过滤条件，则需要广播到所有的partitionGroup；如果有时间过滤条件，则根据时间过滤条件计算出要查询的数据所在的partitionGroup：先找到partitionGroup所在的header(算法如下)，然后在根据header找到partition Group。 123456789101112131415 public static void getIntervalHeaders(String storageGroupName, long timeLowerBound, long timeUpperBound, PartitionTable partitionTable, Set&lt;Node&gt; result) &#123; // 获取配置的partitionInterval long partitionInterval = StorageEngine.getTimePartitionInterval(); // 计算时间下限用于计算slot id时间，之所以先除以partitionInterval，然后在乘以partitionInterval。 // 是因为在routeToHeaderByTime中根据path和time定位slot id的时候有这个逻辑：time / timePartitionInterval。 // 个人认为此处操作是多此一举，比如时间戳是10，partitionInterval是3，则最终计算出来的partitionInstance=10/3=3, 如果先开始10/3*3=9,然后在计算9/3还是3，所以此处转化没有用，唯一的用处就是后面计算都是整数了 long currPartitionStart = timeLowerBound / partitionInterval * partitionInterval; // 然后遍历，每次增加partitionInterval，直到上限达到timeUpperBound为止。 // 此处有性能问题，如果只设置了下限，没有设置上限，则上限默认为Long.MAX_VALUE，会循环遍历亿亿次；对下限也是同理。 while (currPartitionStart &lt;= timeUpperBound) &#123; result.add(partitionTable.routeToHeaderByTime(storageGroupName, currPartitionStart)); currPartitionStart += partitionInterval; &#125;&#125; 为每个partition 构建一个SeriesReader。然后merge到ManagedMergeReader中，ManagedMergeReader维护了一个最小堆，根据reader的nextTimeValuePair的时间戳进行比较。 ServerTimeGenerator::generateNewBatchReader todo TsFileTimeGenerator::generateNewBatchReader todo]]></content>
      <tags>
        <tag>IoTDB</tag>
        <tag>cluster</tag>
        <tag>查询流程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IOTDB-Cluster 源码解析]]></title>
    <url>%2F2020%2F04%2F08%2FIOTDB-Cluster-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Log 相关loglog记录了集群中发生过的操作，每一条log都有如下4个变量： 属性 含义 currLogIndex 此条log的索引 currLogTerm 此条log的term previousLogIndex 前一条log的索引 previousLogTerm 前一条log的term 类图非常清晰了，无需详细解析，目前支持4种类型的log：1.AddNodeLog; 2.RemoveNodeLog; 3.CloseFileLog; 4.PhysicalPlanLog snapshot解析 所有的snapshot都继承自父类Snapshot，父类有两个成员变量：lastLogIndex和lastLogTerm，指的是快照中最后的一条logid和log term SimpleSnapshot： 是最简单的快照实现方式，所有的log都保存在内存中，快照是一个list的数组实现方式。 FileSnapshot：继承了Snapshot的同时，也实现了TimeseriesSchemaSnapshot接口，主要保存了两个属性：12private Collection&lt;MeasurementSchema&gt; timeseriesSchemasprivate List&lt;RemoteTsFileResource&gt; dataFiles RemoteTsFileResource是tsfile的文件描述，而MeasurementSchema也是描述的各个measurement。所以可以看出来FileSnapshot之所以称之为”文件快照“，主要是由于保存了TsFileResource的原因。 PullSnapshotTaskDescriptor：首先看一下官方注释：PullSnapshotTaskDescriptor describes a pull-snapshot-task with the slots to pull.， 有如下两个数据结构. 12private PartitionGroup previousHolders;private List&lt;Integer&gt; slots; PartitionedSnapshot: 使用一个map保存了每个slot id对应的快照。核心数据结构如下 12345// key是slot id，value是Snapshot的泛型(具体的某一种Snapshot)private Map&lt;Integer, T&gt; slotSnapshots;// SnapshotFactory是一个函数式接口，用来反序列化的时候创建Snapshot的实例，// 可以通过下图看到函数接口的实现都是Snapshot的默认构造函数。private SnapshotFactory&lt;T&gt; factory; MetaSimpleSnapshot：继承自SimpleSnapshot：多了一个属性storageGroups，保存了所有的storageGroups。除了需要保存storageGroups之外，MetaSimpleSnapshot还需要保存所有需要的元数据，例如ttl，dataAuth等。 PullSnapshotTask， 他并不是一个快照的实现类，而是封装了执行快照任务的一个类，它实现了Callable接口，可以简单的理解为实现了Runnable接口一样，关于Runnable和Callable接口的差异请google。call 函数是具体实现pullSnapshot 任务的函数，返回值是一个map，key是slot id，value是snapshot，以供调用者可以读取snapshot的数据来进行数据追赶。PullSnapshotTask持有PullSnapshotTaskDescriptor，用来描述从哪儿拉取快照。 log applier核心函数就是apply(Log)，使得log所描述的action在本机生效，这里其实跟wal有点类似了，wal的作用就是执行任何事情之前先写wal，防止崩溃的时候wal可以恢复，只不过log applier是raft状态机中的组件。 DataLogApplier目前提供的apply类型：PhysicalPlanLog和CloseFileLogMetaLogApplier目前提供的apply类型：AddNodeLog，RemoveNodeLog和PhysicalPlanLog。 两种都提供PhysicalPlanLog，会根据具体提供的操作(数据操作、元数据操作)不一样而执行不同的方法。 log managerlog manager也就是管理log的一些类，也就是log 存在哪里，怎么存？怎么取？等，目前的实现都是基于内存的实现方式。如果一个复制组所有的节点都crash的话，则log也就丢失了，基于文件(可持久化)的log管理目前正在开发中(https://issues.apache.org/jira/browse/IOTDB-351) LogManager接口，主要提供了一些访问Log类属性的一些方法以及存储，读取log的一些函数。其中着重指出void commitLog(long maxLogIndex)函数，此函数会调用log applier中的apply函数。主要函数如下 1234567891011121314151617181920212223// 追加一条log，目前实现是在内存中的logbuffer中追加一条log，// 并且更新lastLogId以及lastLogTgitermboolean appendLog(Log log);// 应用(apply) maxLogIndex之前的所有的日志void commitLog(long maxLogIndex);// 用于判断logIndex这条log是否还在内存中，如果不在内存中，说明已经形成快照了boolean logValid(long logIndex);Snapshot getSnapshot();/** * Take a snapshot of the committed logs instantly and discard the committed logs. */// 把commitIndex之前的日志形成快照，并且删除commitIndex之前的日志。void takeSnapshot() throws IOException;LogApplier getApplier();void setLastLogId(long lastLogId);void setLastLogTerm(long lastLogTerm); MemoryLogManager，虚类，把所有的log都保存在内存中。 MetaSingleSnapshotLogManager， 继承自MemoryLogManager，其中的snapshot也是内存持有的，不过其getSnapshot函数返回的是MetaSimpleSnapshot。 PartitionedSnapshotLogManager， 继承自MemoryLogManager，其中的snapshot也是内存持有的，不过其getSnapshot函数返回的是PartitionedSnapshot。实际中没有 FilePartitionedSnapshotLogManager，继承自PartitionedSnapshotLogManager，与PartitionedSnapshotLogManager不同之处在于：PartitionedSnapshotLogManager在更新快照的时候会把快照保存在内存中，而FilePartitionedSnapshotLogManager想从tsfile中实时获取快照。笔者理解就是FilePartitionedSnapshotLogManager不想在内存中保存快照，而是在需要的时候实时从tsfile中获取快照，用以减少内存的使用。但是目前并未达到理想的效果，因为在LogManager append log的时候，已经把log保存在内存中了，所以iotdb作者起了一个issue来研究这个问题 https://issues.apache.org/jira/browse/IOTDB-439 总结Log,LogManager和LogApplier以及Snapshot之间的关系几个组件的关系为： LogManager管理了log，包括log的存储，读取； LogManager调用了LogApplier来执行真正的操作； Snapshot是一种特殊的log，为了解决log过多的问题，Snapshot主要有三种： 包含了一组log的集合(目前实现是保存在内存中); 包含了一组tsfile(文件的snapshot)的集合; 包含了一些元数据集合的MetaSimpleSnapshot。 Server先看一下UML类图，一图胜千言，RaftServer是一个虚类，里面的函数主要是根据一些节点的配置（ip，port）等进行初始化，然后启动服务。其有两个子类，一个是DataClusterServer,主要负责data 操作相关的事情，然后通过rpc传到同一个raft组的其他节点上；另外一个是MetaClusterServer，主要负责meta数据的操作，然后把操作通过rpc传到同一个raft组的其他节点上。 RaftServer主要包括两个大的功能：1. 初始化节点配置；2. 启动节点并且监听rpc端口服务(start()函数) MetaClusterServer首先看一下官方注释：MetaCluster manages the whole cluster’s metadata, such as what nodes are in the cluster and the data partition. Each node has one MetaClusterServer instance, the single-node IoTDB instance is started-up at the same time. 意思就是MetaClusterServer管理了集群中所有的meta信息，主要信息就是数据的分布了(后面会详细讲解节点的分布策略)。每个节点上都会有和一个MetaClusterServer实例（也就是说iotdb的cluster版本每个节点是同质的，假设有1000个节点，则1000个节点上都有meta服务，1000个节点组成了meta信息的raft复制组。这在性能，结果一致性上面还是有问题的），同时也会启动一个iotdb的实例。 MetaClusterServer主要变量如下:123456// 每个clusterServer会有一个MetaGroupMember对象，后面会详细介绍。private MetaGroupMember member;// iotdb实例private IoTDB ioTDB;// 保存cluster集群一些状态，在此先不做详细介绍。private RegisterManager registerManager = new RegisterManager(); DataClusterServerDataClusterServer的作用就是来管理data一些数据的操作的，与MetaClusterServer不同之处在于他的RaftMember是一个map，12345// key 是raft 复制组的header node， value是DataGroupMember，也就代表了一个复制组。private Map&lt;Node, DataGroupMember&gt; headerGroupMap = new ConcurrentHashMap&lt;&gt;();// 这个数据结构是关键，后面会详细讲解。private PartitionTable partitionTable; DataClusterServer的任何操作都是先要在这个map中根据rpc传过来的header，找到对应的DataGroupMember，然后再进行操作。 todo@戚厚亮，后面记得讲解PartitionTable RaftMember先上一个官方注释。RaftMember process the common raft logic like leader election, log appending, catch-up and so on. 也就是说，raft算法中的一些核心逻辑：领导选举、log复制、快照追赶等逻辑都是RaftMember干的。 下面介绍一下核心属性的作用：12345678910111213141516171819202122232425262728293031323334353637383940414243444546// the name of the member, to distinguish several members from the logs// name就是“META/DATA”+ip+portString name;// to choose nodes to join cluster request randomly// 当有新的节点加入到集群中的时候，从已经在集群中的节点中随机的选择一个节点来处理加入操作。Random random = new Random();protected Node thisNode;// the nodes known by this nodeprotected volatile List&lt;Node&gt; allNodes;AtomicLong term = new AtomicLong(0);// 默认开始都是ELECTOR的状态volatile NodeCharacter character = NodeCharacter.ELECTOR;volatile Node leader;volatile long lastHeartbeatReceivedTime;// the raft logs are all stored and maintained in the log manager// logManager管理了所有的log操作，详细分析请看上面logManagerLogManager logManager;// the single thread pool that runs the heartbeat thread// 心跳线程池，其心跳服务在HeartbeatThread类中。ExecutorService heartBeatService;// the thread pool that runs catch-up tasks// catch up线程池private ExecutorService catchUpService;// lastCatchUpResponseTime records when is the latest response of each node&apos;s catch-up. There// should be only one catch-up task for each node to avoid duplication, but the task may time out and in that case, the next catch up should be enabled.// 记录节点上次追赶log的时间，用于防止一个节点建立多个追赶任务，也用于判断追赶日志超时使用。private Map&lt;Node, Long&gt; lastCatchUpResponseTime = new ConcurrentHashMap&lt;&gt;();// the pool that provides reusable clients to connect to other RaftMembers. It will be initialized according to the implementation of the subclasses// client 连接池，用于复用已经创建的连接，不用每次通信都需要建立连接private ClientPool clientPool;// when the commit progress is updated by a heart beat, this object is notified so that we may// know if this node is synchronized with the leader// 两个地方用到了这个锁；1.syncLeader函数，用来同步leader的commint log index的。2.在心跳算法sendHeartbeat中，也是用来同步leader 的commit log index的。private Object syncLock = new Object();// when the header of the group is removed from the cluster, the members of the group should no longer accept writes, but they still can be read candidates for weak consistency reads and provide snapshots for the new holdersvolatile boolean readOnly = false; 这里要强调一下上面的allNodes变量，这里的allNodes并不是所有的节点，而仅仅是这个复制组里面的节点，通常节点个数与副本数一致 RaftMember中的重点函数解析首先明确一点，心跳是leader发送给follower的。发送的内容是leader的commitLogTerm和commitLogIndex，来让follower更新自己的commitLogTerm和commitLogIndex。 心跳算法 如果follower收到的请求的term小于本地节点的term，则拒绝这次心跳请求，并且把本地节点保存的term返回给发送者，让leader更新自己的状态。 如果校验心跳请求参数合格的话，则会把本节点log最后更新的index发送给leader，让leader知道下次给”我”这个节点发送log的时候从哪里开始发送。 执行回调HeartbeatHandler.onComplete。leader会根据返回的信息做如下几个事情： 如果follower还承认我是leader，则根据follower发送过来的logIndex让follower追赶我的日志 ，注意LogCatchUpTask和SnapshotCatchUpTask都是leader主动给follower发送数据的，让follower进行追赶leader的日志(快照)。 如果follower的term比我leader的term还大，则leader主动发起退休(retireFromLeader)。 备注：此函数中有一个syncLock锁，是用来同步leader的CommitLogIndex的，只所以用锁是因为在syncLeader函数中也有同步leader的CommitLogIndex的的功能。 小提示：与etcd raft中心跳算法的不同之处在于：ectd raft心跳仅仅是leader把commitLogIndex发送给其他follower，并未返回一些信息让leader发起follower.catchUp的操作。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * Process the HeartBeatRequest from the leader. If the term of the leader is smaller than the * local term, turn it down and tell it the newest term. ELse if the local logs catch up the * leader&apos;s, commit them. Else help the leader find the last match log. Also update the * leadership, heartbeat timer and term of the local node. * * @param request * @param resultHandler */@Overridepublic void sendHeartbeat(HeartBeatRequest request, AsyncMethodCallback resultHandler) &#123; logger.trace(&quot;&#123;&#125; received a heartbeat&quot;, name); // term可以理解为一个全局锁 synchronized (term) &#123; long thisTerm = term.get(); // 解析request中的数据，主要有leader的term long leaderTerm = request.getTerm(); HeartBeatResponse response = new HeartBeatResponse(); // 如果leader的term比这个复制组的本节点的term小，则这个leader一定是过期的，忽略这个请求。并且把本节点的term告诉leader if (leaderTerm &lt; thisTerm) &#123; // a leader with term lower than this node is invalid, send it the local term to inform this response.setTerm(thisTerm); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;&#123;&#125; received a heartbeat from a stale leader &#123;&#125;&quot;, name, request.getLeader()); &#125; &#125; else &#123; // the heartbeat comes from a valid leader, process it with the sub-class logic processValidHeartbeatReq(request, response); response.setTerm(Response.RESPONSE_AGREE); // tell the leader who I am in case of catch-up response.setFollower(thisNode); // TODO-CLuster: the log being sent should be chosen wisely instead of the last log, so that the leader would be able to find the last match log // 注释已经说的很好了，发送给leader的log应该让leader知道下次发送给follwer的index是多少！ response.setLastLogIndex(logManager.getLastLogIndex()); response.setLastLogTerm(logManager.getLastLogTerm()); // The term of the last log needs to be the same with leader&apos;s term in order to preserve safety, otherwise it may come from an invalid leader and is not committed if (logManager.getLastLogTerm() == leaderTerm) &#123; // syncLock只有两个地方有使用，除了此处之外，还在syncLeader函数中用到了，这里加这个锁就是防止本节点获取的leader信息是过时的。 synchronized (syncLock) &#123; logManager.commitLog(request.getCommitLogIndex()); syncLock.notifyAll(); &#125; &#125; // if the log is not consistent, the commitment will be blocked until the leader makes the node catch up term.set(leaderTerm); setLeader(request.getLeader()); if (character != NodeCharacter.FOLLOWER) &#123; setCharacter(NodeCharacter.FOLLOWER); &#125; setLastHeartbeatReceivedTime(System.currentTimeMillis()); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;&#123;&#125; received heartbeat from a valid leader &#123;&#125;&quot;, name, request.getLeader()); &#125; &#125; resultHandler.onComplete(response); &#125;&#125; catchUp函数在心跳算法中讲到leader发起heartBeat请求的时候，会有一个回调，即HeartbeatHandler.onComplete，在这里面会有执行catchUp方法。下面分析下这个函数。 catchUp函数也是leader主动发起的。当leader给follower发送心跳的时候，follower会把自己的lastLogIndex返回给leader，leader据此来发起catchUp操作，给follower推送lastLogIndex之后的所有的数据。 流程如下 首先检查一下follower上次catchUp的时间距离现在是否小于一个阈值(20000ms)，如果是则不进行catchUp。(限制catchUp的时间间隔，心跳是1000ms)， 判断内存中log信息是否有效，有效的定义如下：要请求的日志都在内存中则视为有效(即follower还未落下很远，无需通过拉取快照进行追赶)。 构造follower所需的数据，根据第2步是否有效执行如下不同的内容。 若2有效，则通过logManager获取followerLastLogIndex之后的所有日志； 若2无效，则证明follower所需的日志有些已经成为快照了，所以需要拉取快照和内存中日志两部分数据。 向catchUpService服务(catchUpService是一个ExecutorService的线程池)中提交一个任务，任务根据第2步是否有效分为两种：(LogCatchUpTask和SnapshotCatchUpTask都实现了Runnable接口) 若2有效：则会创建一个LogCatchUpTask任务。这个任务就是与follower建立连接，并且遍历所有需要追赶的log，执行followr.appendEntry()即可。 注意：这里有一个优化点：可以调用followr.appendEntries()一次性把所有的log发送给follower，而不是每次都发送一条数据。 若2无效：则会创建一个SnapshotCatchUpTask任务。SnapshotCatchUpTask继承了LogCatchUpTask，其run方法主要内容如下： 把第3步中构造的快照序列化之后调用follower.sendSnapshot发送给follower； 发送log：执行LogCatchUpTask中doLogCatchUp，即上面讲的LogCatchUpTask类中的主要内容。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/** * Update the followers&apos; log by sending logs whose index &gt;= followerLastMatchedLogIndex to the * follower. If some of the logs are not in memory, also send the snapshot. * &lt;br&gt;notice that if a part of data is in the snapshot, then it is not in the logs&lt;/&gt; * * @param follower * @param followerLastLogIndex */public void catchUp(Node follower, long followerLastLogIndex) &#123; // TODO-Cluster: use lastMatchLogIndex instead of lastLogIndex // for one follower, there is at most one ongoing catch-up synchronized (follower) &#123; // check if the last catch-up is still ongoing // 首先检查一下follower上次catchUp的时间距离现在是否小于一个阈值(20000ms)，如果是则不进行catchUp Long lastCatchupResp = lastCatchUpResponseTime.get(follower); if (lastCatchupResp != null &amp;&amp; System.currentTimeMillis() - lastCatchupResp &lt; RaftServer.connectionTimeoutInMS) &#123; logger.debug(&quot;&#123;&#125;: last catch up of &#123;&#125; is ongoing&quot;, name, follower); return; &#125; else &#123; // record the start of the catch-up lastCatchUpResponseTime.put(follower, System.currentTimeMillis()); &#125; &#125; if (followerLastLogIndex == -1) &#123; // if the follower does not have any logs, send from the first one followerLastLogIndex = 0; &#125; // 连接到远程节点 AsyncClient client = connectNode(follower); if (client != null) &#123; List&lt;Log&gt; logs; boolean allLogsValid; Snapshot snapshot = null; synchronized (logManager) &#123; // check if the very first log has been snapshot // 根据follower请求的logIndex判断要请求的所有log是否都在内存中，如果都在内存中，则只需要把内存中的log发送给follower即可，否则还需要把snapshot一起发送给follower allLogsValid = logManager.logValid(followerLastLogIndex); logs = logManager.getLogs(followerLastLogIndex, Long.MAX_VALUE); // 所需要的数据不都在内存中，所以需要把快照数据发送给follower。 if (!allLogsValid) &#123; // if the first log has been snapshot, the snapshot should also be sent to the // follower, otherwise some data will be missing // 获取snapshot snapshot = logManager.getSnapshot(); &#125; &#125; // 根据是否需要给follower发送snapshot，生成LogCatchUpTask或是SnapshotCatchUpTask if (allLogsValid) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(&quot;&#123;&#125; makes &#123;&#125; catch up with &#123;&#125; cached logs&quot;, name, follower, logs.size()); &#125; catchUpService.submit(new LogCatchUpTask(logs, follower, this)); &#125; else &#123; logger.debug(&quot;&#123;&#125;: Logs in &#123;&#125; are too old, catch up with snapshot&quot;, name, follower); catchUpService.submit(new SnapshotCatchUpTask(logs, snapshot, follower, this)); &#125; &#125; else &#123; lastCatchUpResponseTime.remove(follower); logger.warn(&quot;&#123;&#125;: Catch-up failed: node &#123;&#125; is currently unavailable&quot;, name, follower); &#125;&#125; 选举算法处理选举过程的算法也很简单： 首先看发送过来的请求的node(也就是要当leader的node)是否与本地节点保存的leader一致，如果一致我就同意。 如果不一致，则根据请求过来的信息与本地信息进行对比来决定是否同意发起选举的节点作为leader，其算法在processElectionRequest中。主要判断逻辑如下： thatTerm &lt;= thisTerm 拒绝，发起选举的term都比我本地的要小，怎么可能给你投票！ thatLastLogTerm &lt; thisLastLogTerm 拒绝，发起选举的节点的最后一条日志的term都比我本地的要小，拒绝你。 (thatLastLogTerm == thisLastLogTerm &amp;&amp; thatLastLogId &lt; thisLastLogIndex) 拒绝，虽然最后一条日志的term一致，但是发起选举节点的最后一条日志的索引比我本地的要小，拒绝你。 其他情况下就给发起选举的节点投票了。 1234567891011121314151617181920212223/** * Process an ElectionRequest. If the request comes from the last leader, agree with it. Else * decide whether to accept by examining the log status of the elector. * * @param electionRequest * @param resultHandler */@Overridepublic void startElection(ElectionRequest electionRequest, AsyncMethodCallback resultHandler) &#123; synchronized (term) &#123; if (electionRequest.getElector().equals(leader)) &#123; // always agree with the last leader resultHandler.onComplete(Response.RESPONSE_AGREE); return; &#125; // check the log status of the elector long response = processElectionRequest(electionRequest); logger.info(&quot;&#123;&#125; sending response &#123;&#125; to the elector &#123;&#125;&quot;, name, response, electionRequest.getElector()); resultHandler.onComplete(response); &#125;&#125; syncLeader函数在分析心跳算法的时候，在代码注释里面提到了syncLeader这个函数。这个函数主要用来同步leader的commint log index的。 首先保证所有的快照下载任务都已经完成了。否则就等待所有的快照任务执行结束。 rpc与leader节点建立通信，请求CommitLogIndex。 判断本地的CommitLogIndex与leader的CommitLogIndex是否一致，即follwer是否追赶上了leader的日志。如果追赶上了，则返回成功；否则未超时的情况下(20秒)，等待心跳(默认1000ms，因为心跳的时候leader会使得follwer追赶自己的日志)。当等待超过20s之后若还未追赶上，则返回失败。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/** * Request and check the leader&apos;s commitId to see whether this node has caught up. If not, wait * until this node catches up. * * @return true if the node has caught up, false otherwise */public boolean syncLeader() &#123; // make sure all snapshot pulling are done, otherwise some data will remain in the old nodes // 首先要保证所有的的下载快照任务都已经完成了 logManager.waitRemoteSnapshots(); if (character == NodeCharacter.LEADER) &#123; return true; &#125; if (leader == null) &#123; // the leader has not been elected, we must assume the node falls behind return false; &#125; logger.debug(&quot;&#123;&#125;: try synchronizing with the leader &#123;&#125;&quot;, name, leader); long startTime = System.currentTimeMillis(); long waitedTime = 0; AtomicReference&lt;Long&gt; commitIdResult = new AtomicReference&lt;&gt;(Long.MAX_VALUE); // syncLeaderMaxWaitMs = 20 * 1000，也就是20s while (waitedTime &lt; RaftServer.syncLeaderMaxWaitMs) &#123; // 连接到leader节点 AsyncClient client = connectNode(leader); if (client == null) &#123; // cannot connect to the leader return false; &#125; try &#123; synchronized (commitIdResult) &#123; // rpc调用，请求leader的commitIndex client.requestCommitIndex(getHeader(), new GenericHandler&lt;&gt;(leader, commitIdResult)); commitIdResult.wait(RaftServer.syncLeaderMaxWaitMs); &#125; long leaderCommitId = commitIdResult.get(); long localCommitId = logManager.getCommitLogIndex(); logger.debug(&quot;&#123;&#125;: synchronizing commitIndex &#123;&#125;/&#123;&#125;&quot;, name, localCommitId, leaderCommitId); // 判断本地的commitIndex是否已经追赶上leader的commitIndex if (leaderCommitId &lt;= localCommitId) &#123; // before the response comes, the leader may commit new logs and the localCommitId may be updated by catching up, so it is possible that localCommitId &gt; leaderCommitId at this time,this node has caught up // 个人理解这种情况发生在requestCommitIndex请求还未返回的时候，leader提交了新的日志，并且也把这些日志同步给了follower。 if (logger.isDebugEnabled()) &#123; waitedTime = System.currentTimeMillis() - startTime; logger.debug(&quot;&#123;&#125;: synchronized with the leader after &#123;&#125;ms&quot;, name, waitedTime); &#125; return true; &#125; // wait for next heartbeat to catch up // the local node will not perform a commit here according to the leaderCommitId because // the node may have some inconsistent logs with the leader // 真是一个骚操作啊！！！！！等待heartbeat追赶日志？heartbeat还有这个功能，在心跳章节中解释了为什么心跳的时候还有日志追赶的功能。 waitedTime = System.currentTimeMillis() - startTime; synchronized (syncLock) &#123; // heartBeatIntervalMs 1000ms syncLock.wait(RaftServer.heartBeatIntervalMs); &#125; &#125; catch (TException | InterruptedException e) &#123; logger.error(&quot;&#123;&#125;: Cannot request commit index from &#123;&#125;&quot;, name, leader, e); &#125; &#125; return false;&#125; DataGroupMember里面就是具体的功能实现了，// TODO1234567891011121314151617181920212223242526/*** When a DataGroupMember pulls data from another node, the data files will be firstly stored in the &quot;REMOTE_FILE_TEMP_DIR&quot;, and then load file functionality of IoTDB will be used to load the files into the IoTDB instance.*/private static final String REMOTE_FILE_TEMP_DIR = &quot;remote&quot;;/*** The MetaGroupMember that in charge of the DataGroupMember. Mainly for providing partition table and MetaLogManager.*/private MetaGroupMember metaGroupMember;/*** The thread pool that runs the pull snapshot tasks. Pool size is the # of CPU cores.*/private ExecutorService pullSnapshotService;/*** &quot;logManager&quot; manages the logs of this DataGroupMember. Although the logs of different data partitions (slots) are mixed together before a snapshot is taken, after the taking of snapshot, logs of different logs will be stored separately.*/private PartitionedSnapshotLogManager logManager;/*** &quot;queryManger&quot; records the remote nodes which have queried this node, and the readers or* executors this member has created for those queries. When the queries end, an* EndQueryRequest will be sent to this member and related resources will be released.*/private ClusterQueryManager queryManager; MetaGroupMember里面就是具体的功能实现了，// TODO123// nodes in the cluster and data partitioning// 重点是这个PartitionTable的实现，这里面包含了tsfile的数据分布，在partition章节将详细分析 private PartitionTable partitionTable; 详细函数分析####List getSeriesTypesByPath(List paths, List aggregations)此函数的功能是获取paths中每个path所对应的tsDataType，函数思路： 从本地获取，如果本地有则返回，否则执行第2步骤； 从远端获取： 先执行pullTimeSeriesSchemas(path) 获取这个path的schema，然后从schema中获取dataType。 把这些schema cache在本地。 注意：此处有一个优化点：第1步从本地获取的时候，是批量执行的，即如果有一个path对应的schema在本地不存在，则这批操作都失败，然后都去远程去拿，其实可以优化为对于失败的那些去远程读取，这样会少远程读取的操作 数据分布partition数据分布主要有以下几个类： PartitionGroup. 这个类继承了ArrayList，所以实际上就是一个Node的列表，这个列表中所有的节点组成了一个raft复制组的所有节点，第一个节点记为header。 PartitionTable是一个interface，其实现是SlotPartitionTable，其核心就是如下几个属性 12345678910111213//The following fields are used for determining which node a data item belongs to.// the slots held by each node// key:node，value是这个node上面所有的slot，slot就是tsfile的逻辑单位private Map&lt;Node, List&lt;Integer&gt;&gt; nodeSlotMap = new ConcurrentHashMap&lt;&gt;();// each slot is managed by whom// key:slot id，value是这个slot id所在的nodeprivate Map&lt;Integer, Node&gt; slotNodeMap = new ConcurrentHashMap&lt;&gt;();//TODO a List is enough// the nodes that each slot belongs to before a new node is added, used for the new node to find the data sourceprivate Map&lt;Node, Map&lt;Integer, Node&gt;&gt; previousNodeMap = new ConcurrentHashMap&lt;&gt;();//the filed is used for determining which nodes need to be a group. the data groups which this node belongs to.// localGroups保存了本节点上的所有的复制组private List&lt;PartitionGroup&gt; localGroups; 下面详细讲解一下SlotPartitionTable中的一些函数 SlotPartitionTable初始化123456789private void init(Collection&lt;Node&gt; nodes) &#123; logger.info(&quot;Initializing a new partition table&quot;); nodeRing.addAll(nodes); // 根据nodeIdentifier对所有的node进行排序，nodeIdentifier的生成是在MetaGroupMember::genNodeIdentifier()函数中，会根据ip，port和当前时间进行hash生成一个数字 nodeRing.sort(Comparator.comparingInt(Node::getNodeIdentifier)); // getPartitionGroups 这个函数算法详解在下面单独列出 localGroups = getPartitionGroups(thisNode); assignPartitions();&#125; getPartitionGroups函数详解 首先找到这个节点的索引index（所有的节点都已经按照nodeIdentifier排序了） 找到这个节点的上面的所有的复制组，算法如下。 首先把所有的节点按照索引大小排列为一个环，如下所示（假设有10个节点）,假设此节点的index为1，副本数replicaNum为3，则这个节点上面就会形成3个data group(复制组) 首先以节点1为起始点，顺时针找replicaNum个节点组成一个复制组，即节点1，2，3组成了一个复制组data group0，这个复制组的header为节点1； 然后在以节点0(节点1的上一个节点)为起始点，顺时针找replicaNum个节点组成一个复制组，即节点0，1，2组成了一个复制组data group1，这个复制组的header为节点0； 然后在以节点9(节点0的上一个节点)为起始点，顺时针找replicaNum个节点组成一个复制组，即节点9，0，1组成了和一个复制组data group2，这个复制组的header为节点9； 可以看到每个节点上面最多有replicaNum个复制组。getHeaderGroup(Node node)的作用就是以参数node为起始点，顺时针找到replicaNum个节点作为一个复制组然后返回。 123456789101112131415161718// find replicationNum groups that a node is inprivate List&lt;PartitionGroup&gt; getPartitionGroups(Node node) &#123; List&lt;PartitionGroup&gt; ret = new ArrayList&lt;&gt;(); int nodeIndex = nodeRing.indexOf(node); for (int i = 0; i &lt; replicationNum; i++) &#123; // the previous replicationNum nodes (including the node itself) are the headers of the // groups the node is in int startIndex = nodeIndex - i; if (startIndex &lt; 0) &#123; startIndex = startIndex + nodeRing.size(); &#125; ret.add(getHeaderGroup(nodeRing.get(startIndex))); &#125; logger.debug(&quot;The partition groups of &#123;&#125; are: &#123;&#125;&quot;, node, ret); return ret;&#125; Slot与node的映射关系计算可以详细看如下代码，思路如下： 计算出总的node个数： nodeRingSize； 求出配置的totalSlotNumbers，默认是10000个： 则每个node上面分配的slot数量为：slotsPerNode = totalSlotNumbers / nodeRingSize，因为这个结果并不一定是整数，则最后面的一个node上面的slot num个数可能会多一些。所以Map&lt;Node, List&gt; nodeSlotMap这个数据结构就初始化好了。 有了Map&lt;Node, List&gt; nodeSlotMap这个数据结构，则每个slot在哪个节点，即Map&lt;Integer, Node&gt; slotNodeMap这个数结构也就很清楚了。 12345678910111213141516171819202122232425262728private void assignPartitions() &#123; // evenly assign the slots to each node int nodeNum = nodeRing.size(); // 计算出每个节点上面有多少个slot int slotsPerNode = totalSlotNumbers / nodeNum; for (Node node : nodeRing) &#123; nodeSlotMap.put(node, new ArrayList&lt;&gt;()); &#125; for (int i = 0; i &lt; totalSlotNumbers; i++) &#123; int nodeIdx = i / slotsPerNode; if (nodeIdx &gt;= nodeNum) &#123; // the last node may receive a little more if total slots cannot de divided by node number // 对于最后一个节点，则可能会多一些slot nodeIdx--; &#125; // nodeSlotMap key就是node，value就是slot num的list nodeSlotMap.get(nodeRing.get(nodeIdx)).add(i); &#125; // build the index to find a node by slot for (Entry&lt;Node, List&lt;Integer&gt;&gt; entry : nodeSlotMap.entrySet()) &#123; for (Integer slot : entry.getValue()) &#123; // slotNodeMap key是slot id，value是node slotNodeMap.put(slot, entry.getKey()); &#125; &#125;&#125; 数据路由以insert数据为例，其数据路由如下： 根据要插入数据的storageGroupName以及timestamp 进行hash，确定slotId。算法如下： 123456789public static int calculateStorageGroupSlotByTime(String storageGroupName, long timestamp, int slotNum) &#123; // 获取partitionId，partitionInstance=timestamp / timePartitionInterval; long partitionInstance = StorageEngine.getTimePartition(timestamp); // 计算hash值 int hash = Murmur128Hash.hash(storageGroupName, partitionInstance, HASH_SALT); // 对slotNum取模，计算出所属的slotId。 return Math.abs(hash % slotNum);&#125; 从slotNodeMap这个map中获取这个slot所在的node； 以此node作为header，寻找这个node作为header的PartitionGroup. 获取此PartitionGroup(其实是用了这个partitionGroup的header)所在的GroupMember，后续对数据的操作皆用此GroupMember对象。(代码在DataGroupMember::getDataMember中，即iotdb为每个header创建了一个DataGroupMember，然后保存在Map&lt;Node, DataGroupMember&gt; headerGroupMap这个数据结构中，所以通过header就可以找到这个header所属的DataGroupMember)。 节点管理添加节点增加节点这个操作是MetaClusterServer来处理的，MetaClusterServer收到请求之后，调用MetaGroupMember.joinCluster()来处理，这个函数的处理逻辑如下： 从种子节点中随机的选择一个节点(假设选择的为节点A，待加入节点为B，这个操作也是在节点B操作的，节点B执行shell脚本把自己加入到集群中)，把这个添加节点的请求转发给这个节点(节点A)； 连接节点A,给节点A发送addNode的rpc请求（最终请求到达节点A之后，是MetaGroupMember调用的add方法）； 节点A首先看本机是否是复制组的leader，如果不是，则把请求转发给leader；否则自己处理； 当发现自己是所在的复制组的leader的时候： 检查一下节点B的参数与节点A本地的参数是否一致（PartitionInterval、HashSalt、ReplicationNum等），不一致则返回。 构造AddNodeLog，并且把其发送给同一个复制组(目前的实现也就是所有的节点，因为metaDataGroup所有的节点组成了一个复制组)，当大多数节点收到回复的时候，即认为成功；注意：leader发送日志给follower的时候，follower仅仅把日志保存下来，并不去commit日志，commit日志只有在leader与follower通过心跳通信的时候，才让follower去commit日志，然后在走一遍状态机 当节点A收到大多数节点的回复的时候，节点A提交日志，提交日志的时候会走状态机，对于AddNodeLog，状态机会更新本地的partitionTabel； 节点A序列化本地最新的partitionTabel，返回给节点B; 若rpc请求返回成功：接受节点A返回的partitionTable信息，然后调用dataGroupMember.pullSnapshot 方法来追赶新复制组的数据。 其他情况下都是失败的，等待重试。 当加入成功之后，设置自己的角色为FOLLOWER，并且启动心跳服务。结束； 若第一步加入失败，则等待1000ms(心跳时间)之后，再次重试，最多重试10次； 删除节点删除节点与添加节点逻辑类似，再次不再分析]]></content>
      <tags>
        <tag>IoTDB</tag>
        <tag>cluster</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转载]大话 Select、Poll、Epoll]]></title>
    <url>%2F2020%2F01%2F16%2F%E8%BD%AC%E8%BD%BD-%E5%A4%A7%E8%AF%9D-Select%E3%80%81Poll%E3%80%81Epoll%2F</url>
    <content type="text"><![CDATA[大话 Select、Poll、Epoll点评：很详细并且有深度的的讲解了这三者之间的关系]]></content>
      <tags>
        <tag>转载</tag>
        <tag>select</tag>
        <tag>poll</tag>
        <tag>epoll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[InfluxDB vs IotDB]]></title>
    <url>%2F2020%2F01%2F05%2FInfluxDB-vs-IotDB%2F</url>
    <content type="text"><![CDATA[本篇文章主要对比一下时序数据库InfluxDB与IotDB的差异。 周边生态 influxdb： TICK.(Telegraf：数据采集；Influxdb：数据存储；Chronograf：web ui 数据展示；Kapacitor：监控报警等)； InfluxQL Flux查询语句，对流处理支持良好 iotdb：支持数据采集；数据存储(TsFile，hadoop生态)；数据可视化(Grafana)；数据分析(Spark);结论：生态相对都比较完善。但是InfluxDB对于sql语言和流处理支持较好。 参考文献： Introduction to InfluxData’s InfluxDB and TICK Stack Chapter 1: Overview 性能 Load 场景：数据先load到内存，然后直接load到数据库中； IotDB(772,406 points/s) &gt; InfluxDB (631,227 points/s) Append 吞吐 IotDB = InfluxDB， client number &lt;= 300， IotDB(329,172 points/s) &lt; InfluxDB(502,900 points/s)， client number &gt; 300. 延迟 IotDB &gt; InfluxDB， client number &lt;= 300， IotDB = InfluxDB， client number &gt; 300. Query 吞吐 IotDB &gt; InfluxDB 延迟 IotDB &lt; InfluxDB Append压力测试(同时有一个查询程序在运行着) 吞吐 IotDB = InfluxDB 总体结论：IotDB查询性能是InfluxDB的2倍，插入性能稍微优于InfluxDB；不过在高并发(client&gt;300)Append的情况下，IotDB稍微逊色于InfluxDB。 参考文献：]]></content>
  </entry>
  <entry>
    <title><![CDATA[小白理财]]></title>
    <url>%2F2019%2F12%2F21%2F%E5%B0%8F%E7%99%BD%E7%90%86%E8%B4%A2%2F</url>
    <content type="text"><![CDATA[穷人+富人思维+钱生钱技能+行动=慢慢变富 第一课学习理财-遇见不一样的自己 理财技能就跟生活中必备的生存技能一样，人人都要学会 思维很重要 股票投资骗局如果以投机的方式对待股市，那么股市就是一个大赌场，风险自然是非常大的，而且很容易上当受骗；如果是以投资的心态，关注公司的好坏，那么财富就会随着好公司的成长而增长。 人生穷富的关键 资产的内涵是现金流 生钱资产：能给你持续带来净现金流入的东西。有了生钱资产，你就可以躺着数钱了，这就是所谓的睡后收入。 可以让你变富有 举例：房子，没有贷款，并且租出去； 耗钱资产：能给你持续带来净现金流出的东西。有了耗钱资产，你躺着的时候还在付钱，这就是所谓的睡后支出。 可以让你变贫穷 举例：有贷款的房子，车子 其他资产：产生的净现金流为0的东西。有了其他资产，你躺着的时候就是你躺着。 可能让你变富有，也可能让你变贫穷 举例：无贷款的自住房 生钱资产和耗钱资产的现金流 富人的秘密： 生钱资产占总资产的80%以上； 好支出占总支出的80%以上；（好支出：可以带来更多收入的支出：花钱买生钱资产，花钱投资自己，花钱学理财等） 财务自由：你的非工资收入能够覆盖日常的支出。 财务自由度：年非工资收入(年投资收入)/年生活支出；数字越大代表自由度越高。 第二课如何从穷人思维过渡到富人思维 量化思维：有数据支撑的逻辑比拍脑门决策靠谱的多！ 要重视时间的价值 p2p投资骗局p2p本质上个人to个人的借贷 你盯着别人的利息，别人盯着你的本金。 从1万到100万的理财法则世界第八大奇迹：复利 复利的公式：最终受益=本金*(1+收益率)^时间思考：时间因素很重要，这其实是一个随着时间的增长而指数级增长的公式 第三课普通人如何通过复利法则变富的？ 长时间持有 长期稳定的高收益率 原始投资的骗局原始股一般只属于公司的创始团队和公司高管，外人想得到，只有两种方式：1、增发，这种方式一般都是给和公司有特殊关系的合作伙伴等人；2、转让。公司既然要上市了，为什么会转让股份给你？]]></content>
      <tags>
        <tag>理财</tag>
        <tag>小白</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[InfluxDB cluster 高可用方案]]></title>
    <url>%2F2019%2F12%2F15%2FInfluxDB-cluster-%E9%AB%98%E5%8F%AF%E7%94%A8%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[本文基于InfluxDB之前开源的cluster版本(v0.11)来分析一下其cluster方案，主要分析如下问题： InfluxDB cluster方案提供了哪些模块？ InfluxDB cluster方案中meta node元数据都有哪些？ InfluxDB cluster方案提供的与集群交互的client客户端是什么方式？ data node之间是如何交互的呢？数据格式什么样？ InfluxDB cluster方案中 meta node 一致性怎么保证的？ InfluxDB cluster方案提供的hintedoff具体是怎么工作的？ 下面就会针对每个问题进行分析 InfluxDB cluster方案提供了哪些模块？关于这个问题，这篇文章讲解的非常好，主要包括Meta Node和Data Node两个模块。各个模块的工作模式和原理已经在这篇文章中讲解的很清楚了，这里就不在赘述。 InfluxDB cluster方案中meta node元数据都有哪些？cluster meta data 都在influxdb/services/meta/data.go中，其结构体如下所示，之前我们介绍过单机版的InfluxDB中的meta，相比单机版的，cluster版本的元数据主要多了MetaNodes和DataNodes两个属性，其对应的数据结构也很简单，就是描述了一个node的信息。1234567891011121314151617181920type Data struct &#123; Term uint64 // associated raft term Index uint64 // associated raft index ClusterID uint64 MetaNodes []NodeInfo DataNodes []NodeInfo Databases []DatabaseInfo Users []UserInfo MaxNodeID uint64 MaxShardGroupID uint64 MaxShardID uint64&#125;// NodeInfo represents information about a single node in the cluster.type NodeInfo struct &#123; ID uint64 Host string TCPHost string&#125; 由此可见，meta信息中除了汇总了单机版的meta信息之外，还多了一些meta node和data node的信息。 InfluxDB cluster方案提供的与集群交互的client客户端是什么方式？InfluxDB提供了两种与集群交互的方式：(1)influxDB CLI/Shell (2) influx client。首先声明一下，client是直接与data node交互的。下面我们简单介绍一下这两种方式是怎么与cluster交互的。 influxDB CLI/Shell在其官方文档中描述了具体的用法，但是这里我们主要想知道其实怎么发现cluster集群的，通过阅读文档，我们可以知道，有如下两个参数，可以指定连接的node(这里的node如果是meta node则就可以对meta 信息操作；如果是data node，则就可以对data node的数据进行操作)的ip和port 12-host &apos;host name&apos; // The host to which influx connects. By default, InfluxDB runs on localhost.-port &apos;port #&apos; // The port to which influx connects. By default, InfluxDB runs on port 8086. 所以，可以说influxDB CLI是通过指定ip和port来连接到cluster集群中的。 HTTP API Client Libraries在其官方文档中列出了一些可以使用的能够连接到influxdb cluster的客户端，我们以其官方提供的influxdb1-client(备注：这里面的代码与influxdb/client包下面的类似)为例来简单介绍下;其官方提供了如下的一个例子：1234567891011121314func ExampleClient_query() &#123; c, err := client.NewHTTPClient(client.HTTPConfig&#123; Addr: &quot;http://localhost:8086&quot;, // 这个localhost可以换成meta/data node中的任何一台机器。(这里的node如果是meta node则就可以对meta 信息操作；如果是data node，则就可以对data node的数据进行操作) &#125;) if err != nil &#123; fmt.Println(&quot;Error creating InfluxDB Client: &quot;, err.Error()) &#125; defer c.Close() q := client.NewQuery(&quot;SELECT count(value) FROM cpu_load&quot;, &quot;mydb&quot;, &quot;&quot;) if response, err := c.Query(q); err == nil &amp;&amp; response.Error() == nil &#123; fmt.Println(response.Results) &#125;&#125; 小结可以看到用户与Influxdb cluster交互的方式本质就是rest api；Influxdb cluster提供的与cluster交互的cli比较特殊，如果ip指定了是meta node，则就可以跟meta node交互，如果指定了是data node，则就可以读取/写入数据(这个也可以在Cluster Node Configuration文章中得到证明)。这个还是真的第一次见。。。为什么不在封装一下，提供一个统一的操纵meta/data数据的入口呢？ client是怎么与集群cluster交互的(以数据写入为例)关于influxdb cluster中的数据写入，此文章进行了深入的分析，我们这里主要补充前半部分，即代码是如何走到WritePoints函数的： 1services/httpd/handler.go::NewHandler()102行-&gt;services/httpd/handler.go::serveQuery()305行-&gt;cluster/query_executor.go::ExecuteQuery()73行-&gt;cluster/query_executor.go::executeQuery()120行-&gt;cluster/query_executor.go::executeSelectStatement()472行-&gt;cluster/query_executor.go::writeInto()849行-&gt;cluster/points_writer.go::WritePointsInto()230行-&gt;cluster/points_writer.go::WritePoints()234行 综上，还是通过rest api访问的！points_writer.go中的PointsWriter struct中，有MetaClient接口，MetaClient的实现就是在service/meta/client.go里面的，这个client拥有meta server相关的元信息以及cluster meta data(InfluxDB cluster方案中meta node元数据都有哪些？章节提供的代码)，所以就能够知道往哪个节点上写数据。以写数据为例，可能要写的数据的shard group并不一定存在，所以就会在此client中调用CreateShardGroup函数调用retryUntilExec去meta server中创建shard group，然后client中还有一个pollForUpdates函数，一直去meta server上面拉取meta data 元信息，当有变化的时候就去更新本地的cache。具体详情请参考Influxdb Cluster版本中的Meta。123456789101112131415161718192021func (c *Client) pollForUpdates() &#123; for &#123; data := c.retryUntilSnapshot(c.index()) if data == nil &#123; // this will only be nil if the client has been closed, // so we can exit out return &#125; // update the data and notify of the change c.mu.Lock() idx := c.cacheData.Index c.cacheData = data c.updateAuthCache() if idx &lt; data.Index &#123; close(c.changed) c.changed = make(chan struct&#123;&#125;) &#125; c.mu.Unlock() &#125;&#125; Metadata Client概述 定义在 services/meta/client.go中; Cluster 版本中的Meta是本地的一个内存缓存，数据来源MetaServer; 对Meta的所有写操作，也将通过http+pb的方式发送到MetaServer, 然后阻塞等待从MetaServer返回的新的Metadata通知; MetaClient通过http long polling来及时获取Metadata的变化; data node之间是如何交互的呢？数据格式什么样？各个data node之间实际上是通过建立tcp，然后发送数据交互的。建立连接的代码是在cluster/shard_writer.go 中的connFactory::dial()函数。 其request，response数据格式如下：123456789101112cluster/internal/data.protomessage WriteShardRequest &#123; required uint64 ShardID = 1; repeated bytes Points = 2; optional string Database = 3; optional string RetentionPolicy = 4;&#125;message WriteShardResponse &#123; required int32 Code = 1; optional string Message = 2;&#125; InfluxDB cluster方案中 meta node 一致性怎么保证的？raft实现：https://github.com/hashicorp/raft存储: https://github.com/hashicorp/raft-boltdb InfluxDB cluster方案提供的hintedoff具体是怎么工作的？请参考Influxdb Cluster下的数据写入 数据按一致性要求写入 章节 参考文献Influxdb · 源码分析 · Influxdb cluster实现探究InfluxDB CLI/ShellHTTP API Client LibrariesInfluxdb Cluster下的数据写入Cluster Node ConfigurationInfluxdb Cluster版本中的Met]]></content>
      <tags>
        <tag>cluster</tag>
        <tag>高可用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[InfluxDB-倒排索引之Index文件]]></title>
    <url>%2F2019%2F12%2F13%2FInfluxDB-%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%E4%B9%8BIndex%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[本文主要讲解InfluxDB中的倒排索引，InfluxDB和传统的LSM-tree(hbase使用的模型)不一样的地方是其内部中多了一个倒排索引，这也是让InfluxDB查询较快的秘诀。 在1.7的代码版本中，InfluxDB提供了两种类型的倒排索引，内存和磁盘的，之前只有内存版本的，但是随着数据的增多，内存倒排索引可能导致OOM，所以引入了基于磁盘的倒排索引。下面会基于此进行简单的分析。 内存中的倒排索引一句话总结，内存中的倒排索引主要由如下两个map组成的： map&lt;SeriesID, SeriesKey&gt;的映射。用于查询seriesId所对应的SeriesKey。 map&lt;tagKey, map&lt;tagValue, List&gt;&gt;。第一层的key是tag key，value也是一个map；第二层的key是tag value，value是SeriesID的集合。查询时如果有多个tag条件，则分别拿出对应的series做交集。 下面是详细代码分析，首先看一下代码的结构图，代码路径在influxdb/tsdb/index/inmem下面： 磁盘中的倒排索引参考资料Influxdb中基于磁盘的倒排索引文件TSI结构解析]]></content>
      <tags>
        <tag>InfluxDB</tag>
        <tag>Index</tag>
        <tag>倒排索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[InfluxDB-TSM File解析]]></title>
    <url>%2F2019%2F12%2F12%2FInfluxDB-TSM%20File%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文主要讲解influxdb数据的组织形式，分为内存中的形式和磁盘中的形式。代码路径influxdb/tsdb/engine/，内存Cache结构在influxdb/tsdb/engine/tsm1/cache下面， 内存中组织形式结构说明1234567891011121314151617181920212223242526272829type Cache struct &#123; // Due to a bug in atomic size needs to be the first word in the struct, as // that&apos;s the only place where you&apos;re guaranteed to be 64-bit aligned on a // 32 bit system. See: https://golang.org/pkg/sync/atomic/#pkg-note-BUG size uint64 snapshotSize uint64 mu sync.RWMutex store storer maxSize uint64 // snapshots are the cache objects that are currently being written to tsm files // they&apos;re kept in memory while flushing so they can be queried along with the cache. // they are read only and should never be modified snapshot *Cache snapshotting bool // This number is the number of pending or failed WriteSnaphot attempts since the last successful one. snapshotAttempts int stats *CacheStatistics lastSnapshot time.Time lastWriteTime time.Time // A one time synchronization used to initial the cache with a store. Since the store can allocate a // a large amount memory across shards, we lazily create it. initialize atomic.Value initializedCount uint32&#125; Cache里面有一个store，数据就是存在这个store里面。Cache里面还有一个snapshot， 定时把store里的数据复制到snapshot.store里，然后store清空。然后再把snapshot.store里的内容写入文件。 那这个store里到底是什么结构呢？store被初始化成一个含有16个partitions(节点)的ring。这个ring我称之为伪一致性哈希，因为它并没有成环。 123456789func (c *Cache) init() &#123; if !atomic.CompareAndSwapUint32(&amp;c.initializedCount, 0, 1) &#123; return &#125; c.mu.Lock() c.store, _ = newring(ringShards) // ringShards = 16 c.mu.Unlock()&#125; 每一个partition都初始化成一个map，key是string, value是一个数组12345678910111213141516171819func newring(n int) (*ring, error) &#123; if n &lt;= 0 || n &gt; partitions &#123; return nil, fmt.Errorf(&quot;invalid number of paritions: %d&quot;, n) &#125; r := ring&#123; partitions: make([]*partition, n), // maximum number of partitions. &#125; // The trick here is to map N partitions to all points on the continuum, // such that the first eight bits of a given hash will map directly to one // of the N partitions. for i := 0; i &lt; len(r.partitions); i++ &#123; r.partitions[i] = &amp;partition&#123; store: make(map[string]*entry), &#125; &#125; return &amp;r, nil&#125; 通过跟踪发现，这个map的key就是和TSM文件结构里面的key一致(代码路径在influxdb/tsdb/engine/tsm1/engine下面)： measurement,tags#!~#field ；而这个entry呢，是一组data，每个data由timestamp和value 两个部分构成 。 那key是怎么映射到具体某个partition的呢?12345// getPartition retrieves the hash ring partition associated with the provided// key.func (r *ring) getPartition(key []byte) *partition &#123; return r.partitions[int(xxhash.Sum64(key)%partitions)]&#125; xxhash.sum64，再与partition的数量（16）求余，得到下标，找到partition. 具体xxhash.sum64这个哈希值怎么计算的呢，以后在研究。 结构图现在已经知道了Cache中数据的存储方式了，来张表更清楚一点每次写入同一个key的数据，那就找到其Entries, 把新的数据直接append到后面。 排序与去重这样就又有问题了，如果 timestamp旧的数据后来，那这一组数据的就不是按照timestamp的大小顺序了。这里怎么解决的呢，这里并没有解决，不管是来的更旧的timestamp的数据 还是duplicated数据，统统加后面。 去重和排序在两个地方做： select xx from xx的时候 snapshot写入TSM文件的时候 这个去重和排序代码如下， 先检查顺序，需要的话就sort.最后检查去重。这个sort算法有时间可以看看，应该是针对大部分都是按顺序的情况下效率可以的排序。1234567891011121314151617181920212223242526272829303132333435// Deduplicate returns a new slice with any values that have the same timestamp removed.// The Value that appears last in the slice is the one that is kept. The returned// Values are sorted if necessary.func (a Values) Deduplicate() Values &#123; if len(a) &lt;= 1 &#123; return a &#125; // See if we&apos;re already sorted and deduped var needSort bool for i := 1; i &lt; len(a); i++ &#123; if a[i-1].UnixNano() &gt;= a[i].UnixNano() &#123; needSort = true break &#125; &#125; if !needSort &#123; return a &#125; // 先排序 sort.Stable(a) // 下面这段代码实际上就是去重，如果i和j指向的数据一样，则i不移动，让i指向的数据变成j指向的数据(本来就一样)，然后j++；如果i和j指向的数据不一样，则都进行++，然后i指向j的数据。 var i int for j := 1; j &lt; len(a); j++ &#123; v := a[j] if v.UnixNano() != a[i].UnixNano() &#123; i++ &#125; a[i] = v &#125; return a[:i+1]&#125; 内存数据分布的简单版本如下：即seriesKey+field到timeValues的映射： TSMFile 结构TSM file的结构如下: 从TSM file读取数据的流程如下： 首先读取footer，获取SeriesIndexSection的offset，可以只加载seriesIndexBlock。 根据Key（即SeriesKey+field）二分查找匹配的SeriesIndexBlock。 根据时间二分查找在SeriesIndexBlock中找到对应的IndexEntry。 根据IndexEntry加载并解压SeriesDataBlock。 参考文献In-memory indexing and the Time-Structured Merge Tree (TSM)Influxdb中TSM文件结构解析之读写TSM时序数据库技术体系 – InfluxDB TSM存储引擎之TSMFileinfluxdb内存中Cache数据结构详解]]></content>
      <tags>
        <tag>InfluxDB</tag>
        <tag>TSM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[InfluxDB-存储引擎(store engine)模型介绍]]></title>
    <url>%2F2019%2F12%2F12%2FInfluxDB-%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E(store%20engine)%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[InfluxDB的存储引擎主要由内存中的索引(In-Memory Index)、WAL、Cache以及FileStore中的TSM Files组成。代码关系如下图所示： Cache实际上就是数据在内存中的组织形式；TSM File实际上就是数据在磁盘中的组织形式。 后面会分几个专题进行简单的介绍。 参考文献In-memory indexing and the Time-Structured Merge Tree (TSM)]]></content>
      <tags>
        <tag>InfluxDB</tag>
        <tag>store engine</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[InfluxDB-基本概念和meta介绍]]></title>
    <url>%2F2019%2F12%2F12%2FInfluxDB-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8Cmeta%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[本篇文章主要介绍InfluxDB的基本概念以及Meta信息的组成，代码基于1.7版本。 基本概念首先看一下InfluxDB的一些基本概念。 InfluxDB主要有以上图中的几个概念：Point，Measurement，Tags，Fields，Timestamp，Series，下面依次简单介绍下每个概念的含义。 Database：InfluxDB可以创建数据库，一个数据库可以包含多个user、保存策略等，schemaless ，支持随时灵活创建mersurement； Measurement：相当于表的概念； Tags：是一些kv的结构，标签会被用来建立索引； Fields：是保存真实数据的结构，也是kv结构，但是不会被用来建立索引； Point： 代表了一条记录，可以理解为关系型数据库中的一条记录； Timestamp：既然InfluxDB被称之为时序数据库，少了时间是不可能的，每条记录必须要有一个时间戳； Series：是由Measurement+Tags组成的 Meta信息那么这些基本概念在InfluxDB代码中是怎么体现的呢？首先上图，一图胜千言。可以看到，meta主要包含一些元信息，主要包含如下几个部分： database信息：meta信息中包含了一个database的数组，包括了所有的database信息。 RetentionPolicy：数据过期策略。包含数据过期时间，副本数（单机无用），shardGroup Duration； ShardGroupInfo：分片组。包含时间范围[start, end)，删除时间，截断时间； ShardInfo：分片。owners，所属节点，单机无用； ShardGroupDuration：决定了一个ShardGroup内部数据时间的最大差； Duration：决定了这个RetentionPolic的数据的过期时间； ContinuousQuery：持续的query，有点类似预处理。 user信息：包含用户名，密码的hash，是否为管理员，以及授予的权限 raft协议同步所需信息：(暂不展开) Term Index 其关系可以用下面这张图展示： 数据过期删除策略Shard Group是InfluxDB中一个重要的逻辑概念，从字面意思来看Shard Group会包含多个Shard，每个Shard Group只存储指定时间段的数据，不同Shard Group对应的时间段不会重合。比如2017年9月份的数据落在Shard Group0上，2017年10月份的数据落在Shard Group1上。 过期策略，过期是直接整个shardGroup删除，一个shard对应多个TSM Files。 当shardGroup的end+过期时间小于当前时间，就会shardGroup就会被当成过期的并被清理掉。 每个Shard Group对应多长时间是通过Retention Policy中字段”SHARD DURATION”指定的，如果没有指定，也可以通过Retention Duration（数据过期时间）计算出来，两者的对应关系为： 参考文献Influxdb的Meta data分析时序数据库技术体系 – 初识InfluxDB]]></content>
      <tags>
        <tag>InfluxDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang-由浅入深聊聊Golang的sync Pool]]></title>
    <url>%2F2019%2F11%2F28%2FGolang-%E7%94%B1%E6%B5%85%E5%85%A5%E6%B7%B1%E8%81%8A%E8%81%8AGolang%E7%9A%84sync%20Pool%2F</url>
    <content type="text"><![CDATA[前言今天在思考优化GC的套路，看到了sync.Pool，那就来总结下，希望可以有个了断。用最通俗的话，讲明白知识。以下知识点10s后即将到来。 pool是什么？ 为什么需要sync.Pool？ 如何使用sync.Pool？ 走一波源码 源码关键点解析 正文1.sync.Pool是什么？Golang在 1.3 版本的时候，在sync包中加入一个新特性：Pool。简单的说：就是一个临时对象池。 2.为什么需要sync.Pool？保存和复用临时对象，减少内存分配，降低GC压力。（对象越多GC越慢，因为Golang进行三色标记回收的时候，要标记的也越多，自然就慢了） 3.如何使用sync.Pool？123456789101112131415161718192021222324func main() &#123; // 初始化一个pool pool := &amp;sync.Pool&#123; // 默认的返回值设置，不写这个参数，默认是nil New: func() interface&#123;&#125; &#123; return 0 &#125;, &#125; // 看一下初始的值，这里是返回0，如果不设置New函数，默认返回nil init := pool.Get() fmt.Println(init) // 设置一个参数1 pool.Put(1) // 获取查看结果 num := pool.Get() fmt.Println(num) // 再次获取，会发现，已经是空的了，只能返回默认的值。 num = pool.Get() fmt.Println(num)&#125; 使用较为简单。总的思路就是：搞一个池子，预先放入临时产生的对象，然后取出使用。可能有同学问了，这个玩意儿官方出的，那他自己有在用吗？答案是有的，其实你也一直在用。就是fmt包啦，由于fmt总是需要很多[]byte对象，索性就直接建了一个[]byte对象的池子，来走一波代码。12345678910111213141516171819type buffer []byte// printer状态的结构体（）type pp struct &#123; ...&#125;// pp的对象池， 《====这里用到了。var ppFree = sync.Pool&#123; New: func() interface&#123;&#125; &#123; return new(pp) &#125;,&#125;// 每次需要pp结构体的时候，都过sync.Pool进行获取。func newPrinter() *pp &#123; p := ppFree.Get().(*pp) p.panicking = false p.erroring = false p.fmt.init(&amp;p.buf) return p&#125; 4.走一波源码4.1 基础数据结构1234567891011121314151617181920212223242526272829type Pool struct &#123; // noCopy，防止当前类型被copy，是一个有意思的字段，后文详说。 noCopy noCopy // [P]poolLocal 数组指针 local unsafe.Pointer // 数组大小 localSize uintptr // 选填的自定义函数，缓冲池无数据的时候会调用，不设置默认返回nil New func() interface&#123;&#125; //新建对象函数&#125;type poolLocalInternal struct &#123; // 私有缓存区 private interface&#123;&#125; // 公共缓存区 shared []interface&#123;&#125; // 锁 Mutex &#125;type poolLocal struct &#123; // 每个P对应的pool poolLocalInternal // 这个字段很有意思，是为了防止“false sharing/伪共享”，后文详讲。 pad [128 - unsafe.Sizeof(poolLocalInternal&#123;&#125;)%128]byte&#125; 来一张全景图，更有利于全局角度看这个结构体：这边有两个小问题： noCopy的作用？ poolLocal中pad的作用？ 如何确定要获取的数据在哪个poolLocal里头？ 带着问题，继续往下看，看完就能懂这两个小问题拉。 4.2 pin在介绍get/put前，关键的基础函数pin需要先了解一下。一句话说明用处：确定当前P绑定的localPool对象（这里的P，是MPG中的P，如果看不懂请点这里：!关于goroutine的一些小理解）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465func (p *Pool) pin() *poolLocal &#123; // 返回当前 P.id &amp;&amp; 设置禁止抢占（避免GC） pid := runtime_procPin() // 根据locaSize来获取当前指针偏移的位置 s := atomic.LoadUintptr(&amp;p.localSize) l := p.local // 有可能在运行中动调调整P，所以这里进行需要判断是否越界 if uintptr(pid) &lt; s &#123; // 没越界，直接返回 return indexLocal(l, pid) &#125; // 越界时，会涉及全局加锁，重新分配poolLocal，添加到全局列表 return p.pinSlow()&#125;var ( allPoolsMu Mutex allPools []*Pool)func (p *Pool) pinSlow() *poolLocal &#123; // 取消P的禁止抢占（因为后面要进行metux加锁） runtime_procUnpin() // 加锁 allPoolsMu.Lock() defer allPoolsMu.Unlock() // 返回当前 P.id &amp;&amp; 设置禁止抢占（避免GC） pid := runtime_procPin() // 再次检查是否符合条件，有可能中途已被其他线程调用 s := p.localSize l := p.local if uintptr(pid) &lt; s &#123; return indexLocal(l, pid) &#125; // 如果数组为空，则新建Pool，将其添加到 allPools，GC以此获取所有 Pool 实例 if p.local == nil &#123; allPools = append(allPools, p) &#125; // 根据 P 数量创建 slice size := runtime.GOMAXPROCS(0) local := make([]poolLocal, size) // 将底层数组起始指针保存到 Pool.local，并设置 P.localSize // 这里需要关注的是：如果GOMAXPROCS在GC间发生变化，则会重新分配的时候，直接丢弃老的，等待GC回收。 atomic.StorePointer(&amp;p.local, unsafe.Pointer(&amp;local[0])) atomic.StoreUintptr(&amp;p.localSize, uintptr(size)) // 返回本次所需的 poolLocal return &amp;local[pid]&#125;// 根据数据结构的大小来计算指针的偏移量func indexLocal(l unsafe.Pointer, i int) *poolLocal &#123; lp := unsafe.Pointer(uintptr(l) + uintptr(i)*unsafe.Sizeof(poolLocal&#123;&#125;)) return (*poolLocal)(lp)&#125; 流程小记：12禁止抢占GC -&gt; 寻找偏移量 -&gt; 检查越界 -&gt;返回poolLocal -&gt;加锁重建pool，并添加到allPool 4.3 put先说结论：优先放入private空间，后面再放入shared空间现在开始分析：12345678910111213141516171819202122232425262728293031323334353637func (p *Pool) Put(x interface&#123;&#125;) &#123; if x == nil &#123; return &#125; // 这段代码，不需要关心，降低竞争的 if race.Enabled &#123; if fastrand()%4 == 0 &#123; // Randomly drop x on floor. return &#125; race.ReleaseMerge(poolRaceAddr(x)) race.Disable() &#125; // 获取当前的poolLocal l := p.pin() // 如果private为nil，则优先进行设置，并标记x if l.private == nil &#123; l.private = x x = nil &#125; runtime_procUnpin() // 如果标记x不为nil，则将x设置到shared中 if x != nil &#123; l.Lock() l.shared = append(l.shared, x) l.Unlock() &#125; // 设置竞争可用了。 if race.Enabled &#123; race.Enable() &#125;&#125; 4.4 get先说结论：优先从private空间拿，再加锁从shared空间拿，还没有再从其他的PoolLocal的shared空间拿，还没有就直接new一个返回。现在进行分析：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172func (p *Pool) Get() interface&#123;&#125; &#123; // 竞争相关的设置 if race.Enabled &#123; race.Disable() &#125; // 获取当前的poolLocal l := p.pin() // 从private中获取 x := l.private l.private = nil runtime_procUnpin() // 不存在，则继续从shared空间拿， if x == nil &#123; // 加锁了，防止并发 l.Lock() last := len(l.shared) - 1 if last &gt;= 0 &#123; x = l.shared[last] // 从尾巴开始拿起 l.shared = l.shared[:last] &#125; l.Unlock() if x == nil &#123; // 从其他的poolLocal中的shared空间看看有没有可返回的。 x = p.getSlow() &#125; &#125; // 竞争解除 if race.Enabled &#123; race.Enable() if x != nil &#123; race.Acquire(poolRaceAddr(x)) &#125; &#125; // 如果还是没有的话，就直接new一个了 if x == nil &amp;&amp; p.New != nil &#123; x = p.New() &#125; return x&#125;func (p *Pool) getSlow() (x interface&#123;&#125;) &#123; // 获取poolLocal数组的大小 size := atomic.LoadUintptr(&amp;p.localSize) // load-acquire local := p.local // load-consume // 尝试从其他procs获取一个P对象 pid := runtime_procPin() runtime_procUnpin() for i := 0; i &lt; int(size); i++ &#123; // 获取一个poolLocal，注意这里是从当前的local的位置开始获取的，目的是防止取到自身 l := indexLocal(local, (pid+i+1)%int(size)) // 加锁从尾部获取shared的数据 l.Lock() last := len(l.shared) - 1 // 若长度大于1 if last &gt;= 0 &#123; x = l.shared[last] l.shared = l.shared[:last] l.Unlock() break &#125; l.Unlock() &#125; return x&#125; 5.源码关键点解析5.1 定时清理Q：这里的pool的是永久保存的吗？还是？A：是会进行清理的，时间就是两次GC间隔的时间。 12345678910111213141516171819202122232425262728293031// 注册清理函数，随着runtime进行的，也就是每次GC都会跑一下func init() &#123; runtime_registerPoolCleanup(poolCleanup)&#125;// 清理函数也很粗暴，直接遍历全局维护的allPools将private和shared置为nilfunc poolCleanup() &#123; // 遍历allPools for i, p := range allPools &#123; // pool置为nil allPools[i] = nil // 遍历localSIze的数量次 for i := 0; i &lt; int(p.localSize); i++ &#123; l := indexLocal(p.local, i) // private置为nil l.private = nil // 遍历shared，都置为nil for j := range l.shared &#123; l.shared[j] = nil &#125; l.shared = nil &#125; p.local = nil p.localSize = 0 &#125; // allPools重置 allPools = []*Pool&#123;&#125;&#125; 所以呢，这也说明为什么sync.Pool不适合放做“数据库连接池”等带持久性质的数据，因为它会定期回收啊～ 5.2 为什么获取shared要加锁，而private不用？我们知道golang是MPG的方式运行的，（!关于goroutine的一些小理解）大概这么个感觉吧： M------P----- poolLocal | G - G | G ... M------P----- poolLocal | G---G | G ... 也就是说，每个P都分配一个localPool，在同一个P下面只会有一个Gouroutine在跑，所以这里的private，在同一时间就只可能被一个Gouroutine获取到。而shared就不一样了，有可能被其他的P给获取走，在同一时间就只可能被多个Gouroutine获取到，为了保证数据竞争，必须加一个锁来保证只会被一个G拿走。 5.3 noCopy的作用？防止Pool被拷贝，因为Pool 在Golang是全剧唯一的 这里又衍生一个问题，这里的noCopy如何实现被防止拷贝的？？？ Golang中没有原生的禁止拷贝的方式，所以结构体不希望被拷贝，所以go作者做了这么一个约定：只要包含实现 sync.Locker 这个接口的结构体noCopy，go vet 就可以帮我们进行检查是否被拷贝了。 5.4 pad的作用？这个挺有意思的，源代码出现这么一个词：false sharing，翻译为“伪共享”。也就是说这个字段，主要就是用来防止“伪共享”的。 为什么会有false sharing？ 简单说明一下：缓存系统中是以缓存行为单位存储的。缓存行通常是 64 字节，当缓存行加载其中1个字节时候，其他的63个也会被加载出来，加锁的话也会加锁整个缓存行，当下图所示x、y变量都在一个缓存行的时候，当进行X加锁的时候，正好另一个独立线程要操作Y，这会儿Y就要等X了，此时就不无法并发了。由于这里的竞争冲突来源自共享，所以称之为伪共享。 （图片来自https://www.cnblogs.com/cyfonly/p/5800758.html） 如何防止？ 补齐缓存行，让每个数据都是独立的缓存行就不会出现false sharding了。 5.5 怎么确定我的数据应该存储在LocalPool数组的哪个单元？根据数据结构的大小来计算指针的偏移量，进而算出是LocalPool数组的哪个。 5.6 sync.Pool的设计哲学？Goroutine能同一时刻在并行的数量有限，是由runtime.GOMAXPROCS(0)设置的，这里的Pool将数据与P进行绑定了，分散在了各个真正并行的线程中，每个线程优先从自己的poolLocal中获取数据，很大程度上降低了锁竞争。 作者：咖啡色的羊驼链接：https://juejin.im/post/5d4087276fb9a06adb7fbe4a来源：掘金著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。]]></content>
      <tags>
        <tag>golang</tag>
        <tag>sync.Pool</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis常见技巧]]></title>
    <url>%2F2019%2F11%2F19%2Fredis%E5%B8%B8%E8%A7%81%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[redis 命令行相关当搭建好一个redis 或是twemproxy之后，可以直接采用如下命令来操作redis集群12# 连接上redis serverredis-cli -p port 剩下的就是操作redis的命令了命令参考 https://redis.io/commands Error: Protocol error, got “{“ as reply type byteIt is because twemproxy will listen on two ports when it starts, one of them is port 22222(by default, you can set it with ‘-s’), it is used for monitoring, if you connect to it, the error occurs So, connect to another port 参考：https://stackoverflow.com/questions/27688394/twemproxynutcracker-error-protocol-error-got-as-reply-type-byte Hash Tags 功能Hash Tags enables you to use part of the key for calculating the hash. When the hash tag is present, we use part of the key within the tag as the key to be used for consistent hashing. Otherwise, we use the full key as is. Hash tags enable you to map different keys to the same server as long as the part of the key within the tag is the same. For example, the configuration of server pool beta, also shown below, specifies a two character hash_tag string - “{}”. This means that keys “user:{user1}:ids” and “user:{user1}:tweets” map to the same server because we compute the hash on “user1”. For a key like “user:user1:ids”, we use the entire string “user:user1:ids” to compute the hash and it may map to a different server. 12345678910111213beta: listen: 127.0.0.1:22122 hash: fnv1a_64 hash_tag: &quot;&#123;&#125;&quot; distribution: ketama auto_eject_hosts: false timeout: 400 redis: true servers: - 127.0.0.1:6380:1 server1 - 127.0.0.1:6381:1 server2 - 127.0.0.1:6382:1 server3 - 127.0.0.1:6383:1 server4 参考链接：https://github.com/twitter/twemproxy/blob/master/notes/recommendation.md#hash-tags]]></content>
      <tags>
        <tag>redis</tag>
        <tag>twemproxy</tag>
        <tag>nutcracker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[raft 相关资料]]></title>
    <url>%2F2019%2F11%2F08%2Fraft-%E7%9B%B8%E5%85%B3%E8%B5%84%E6%96%99%2F</url>
    <content type="text"><![CDATA[raft 介绍raft 论文中文版https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md raft 算法演示http://thesecretlivesofdata.com/raft/ tikv raft相关资料https://pingcap.com/blog-cn/#Raft multi raft实现方案Dragonboat go语言编写 实现了multi raft实现 不知道是否在生产环境中试验过，是个人写的 github地址：https://github.com/lni/dragonboat 资料： 开源一个千万级多组Raft库 https://zhuanlan.zhihu.com/p/52487803 百亿次的锤炼-带逛Dragonboat的各类测试 https://zhuanlan.zhihu.com/p/52600636 elasticell go语言编写 实现了multi raft协议 github地址： https://github.com/deepfabric/elasticell 资料： Elasticell-Multi-Raft实现 https://zhuanlan.zhihu.com/p/33047950 sofa-jraft java语言 蚂蚁金服开源 github：https://github.com/sofastack/sofa-jraft 资料： SOFAJRaft-RheaKV MULTI-RAFT-GROUP 实现分析 | SOFAJRaft 实现原理 https://www.sofastack.tech/blog/sofa-jraft-rheakv-multi-raft-group/ 官方资料介绍 https://www.sofastack.tech/projects/sofa-jraft/overview/ 详解蚂蚁金服 SOFAJRaft：生产级高性能 Java 实现 https://www.infoq.cn/article/acwsyQNTRtQof*3ivY4X]]></content>
      <tags>
        <tag>raft</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++ mmap读写示例]]></title>
    <url>%2F2019%2F10%2F24%2Fc-mmap%E8%AF%BB%E5%86%99%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[分3种情况测试文件读写情况下，page cache 等使用情况，也算是一个使用mmap的例子。文件大小为1GB。 直接读文件到内存中，内存中开辟文件大小的buffer进行读； mem_total_uesed = 2GB page_cache = 1GB 程序堆栈占用= 1GB 使用mmap进行文件映射: 仅仅映射，并不把其内容拷贝到内存缓冲区：mem_total_uesed = page_cache = 1GB，程序堆栈占用=0； 将其内容拷贝到内存缓冲区；mem_total_uesed = 2GB；page_cache = 1GB；程序堆栈占用= 1GB mmap开辟一块1GB匿名内存进行写入： mem_total_uesed = page_cache = 1GB;程序堆栈占用=0； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109/** * @Author yilan * @create 2019/10/23 2:39 PM * @brief 测试mmap内存使用情况 */#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;string&gt;#include &lt;cstring&gt;#include &lt;sys/mman.h&gt;#include &lt;sys/types.h&gt;#include &lt;fcntl.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/stat.h&gt;#include &lt;unistd.h&gt;using namespace std;/*直接读文件到内存中，内存中开辟文件大小的buffer进行读*/void read_file_to_buffer(const char* file_name) &#123; ifstream fl(file_name); fl.seekg(0, ios::end ); size_t len = fl.tellg(); cout &lt;&lt; len &lt;&lt; endl; char *ret = new char[len]; fl.seekg(0, ios::beg); fl.read(ret, len); fl.close();// cout &lt;&lt; ret &lt;&lt; endl;&#125;size_t get_file_size(const char* file_name) &#123; struct stat st; stat(file_name, &amp;st); return st.st_size;&#125;/*使用mmap进行文件映射*/void read_file_from_mmap(const char* file_name) &#123; size_t file_size = get_file_size(file_name); cout &lt;&lt; file_size &lt;&lt; endl; //Open file int fd = open(file_name, O_RDONLY, 0); if (fd == -1)&#123; cout &lt;&lt; &quot;Error open file for read&quot; &lt;&lt; endl; return; &#125; //Execute mmap void* mmapped_data = mmap(NULL, file_size, PROT_READ, MAP_PRIVATE | MAP_POPULATE, fd, 0); if (mmapped_data == MAP_FAILED) &#123; close(fd); cout &lt;&lt; &quot;Error mmapping the file&quot; &lt;&lt; endl; return; &#125; //Write the mmapped data buffer /*将其内容拷贝到内存缓冲区*/ char *ret = new char[file_size]; memcpy(ret, mmapped_data, file_size); //Cleanup int rc = munmap(mmapped_data, file_size); if (rc != 0) &#123; close(fd); cout &lt;&lt; &quot;Error un-mmapping the file&quot; &lt;&lt; endl; return; &#125;// cout &lt;&lt; ret &lt;&lt; endl; close(fd);&#125;/*map开辟一块1GB匿名内存进行写入*/void write_data_to_anoymous_mmap()&#123; cout &lt;&lt; &quot;come in the write_data_to_anoymous_mmap&quot; &lt;&lt; endl; long file_size = 1 * 1024 * 1024 * 1024; char* mmapped_data = (char*)mmap(NULL, file_size, PROT_READ | PROT_WRITE, MAP_SHARED | MAP_ANONYMOUS, -1, 0); if (mmapped_data == MAP_FAILED) &#123; cout &lt;&lt; &quot;mmap failed: &quot; &lt;&lt; stderr &lt;&lt; endl; return; &#125; cout &lt;&lt; &quot;start to write&quot; &lt;&lt; endl; for (long i = 0; i &lt; file_size; i++)&#123; mmapped_data[i] = &apos;\0&apos;; &#125; cout &lt;&lt; &quot;end of write&quot; &lt;&lt; endl;&#125;int main(int argc, char **argv) &#123; if (argc != 2) &#123; printf(&quot;wrong params! ./memory_consume_test [file_name]\n&quot;); return -1; &#125; char *file_name = argv[1]; read_file_to_buffer(file_name); sleep(600); read_file_from_mmap(file_name); sleep(600); write_data_to_anoymous_mmap(); sleep(600); return 0;&#125; #参考链接A simple mmap() readonly examplemmap and read/write string to fileWhat is the purpose of MAP_ANONYMOUS flag in mmap system call?]]></content>
      <tags>
        <tag>mmap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang-go slice解析]]></title>
    <url>%2F2019%2F10%2F16%2FGolang-go%20slice%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[golang slice到底是值传递还是引用传递？答案：值传递下面通过几个例子分析一下： 1234567891011121314151617181920212223slice := []int&#123;1, 2, 3&#125;newSlice := slice[0:2] //这个地方是关键fmt.Printf(&quot;before the origin slice address=%p \n&quot;, slice)fmt.Printf(&quot;before the new slice address=%p \n&quot;, slice)fmt.Println(&quot;before the origin slice data = &quot;, slice)fmt.Println(&quot;before the new slice data = &quot;, newSlice)newSlice = append(newSlice, 4)fmt.Println(&quot;after append the new slice data = &quot;, newSlice)// 并不会改变原来数组的值for _, v := range newSlice &#123; v += 10&#125;fmt.Println(&quot;after first change the new slice data = &quot;, newSlice)// 会改变slice和newSlice的值for i := range newSlice &#123; newSlice[i] += 10&#125;fmt.Printf(&quot;after the origin slice address=%p \n&quot;, slice)fmt.Printf(&quot;after the new slice address=%p \n&quot;, newSlice)fmt.Println(&quot;after second change the new slice data = &quot;, newSlice)fmt.Println(&quot;after second change the origin slice data = &quot;, slice) 如上代码会有如下输出，可以看到subSlice改变了原来slice的值，那么是不是slice就是指针传递呢？12345678910before the origin slice address=0xc000090000before the new slice address=0xc000090000before the origin slice data = [1 2 3]before the new slice data = [1 2]after append the new slice data = [1 2 4]after first change the new slice data = [1 2 4]after the origin slice address=0xc000090000after the new slice address=0xc000090000after second change the new slice data = [11 12 14]after second change the origin slice data = [11 12 14] 请看上述代码的简单修改版：1234567891011121314151617181920212223slice := []int&#123;1, 2, 3&#125;newSlice := slice[1:] // 相比上一个代码，把slice[0:2]修改了slice[1:]fmt.Printf(&quot;before the origin slice address=%p \n&quot;, slice)fmt.Printf(&quot;before the new slice address=%p \n&quot;, slice)fmt.Println(&quot;before the origin slice data = &quot;, slice)fmt.Println(&quot;before the new slice data = &quot;, newSlice)newSlice = append(newSlice, 4)fmt.Println(&quot;after append the new slice data = &quot;, newSlice)// 并不会改变原来数组的值for _, v := range newSlice &#123; v += 10&#125;fmt.Println(&quot;after first change the new slice data = &quot;, newSlice)// 会改变slice和newSlice的值for i := range newSlice &#123; newSlice[i] += 10&#125;fmt.Printf(&quot;after the origin slice address=%p \n&quot;, slice)fmt.Printf(&quot;after the new slice address=%p \n&quot;, newSlice)fmt.Println(&quot;after second change the new slice data = &quot;, newSlice)fmt.Println(&quot;after second change the origin slice data = &quot;, slice) 会发现有如下输出：可以发现，subSlice并未改变slice的值，并且他们的地址也发生了变化。这个问题的本质原因是什么呢？12345678910before the origin slice address=0xc00008a000before the new slice address=0xc00008a000before the origin slice data = [1 2 3]before the new slice data = [2 3]after append the new slice data = [2 3 4]after first change the new slice data = [2 3 4]after the origin slice address=0xc00008a000after the new slice address=0xc00008a040after second change the new slice data = [12 13 14]after second change the origin slice data = [1 2 3] 本质原因：第一个代码subSlice在做修改的时候，并未超过slice的容量，所以所有的修改都会反映到slice的底层数组上。但是第二个代码在做append的时候，已经超过了subSlice的容量，底层数组已经放不下了，所以需要扩容，当扩容的时候，newSlice的地址就与之前的地址不一致了。所以对他的改变就不会体现到原有slice上，所以slice本质上还是值传递。 参考链接Subslice 例子golang slice实践以及底层实现]]></content>
      <tags>
        <tag>golang</tag>
        <tag>slice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang-redigo源码分析之连接池]]></title>
    <url>%2F2019%2F10%2F08%2FGolang-redigo%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B9%8B%E8%BF%9E%E6%8E%A5%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[连接池一直是系统设计中很重要的一个话题，其主要的作用是复用系统中已经创建好的连接，避免重复创建连接加重系统负荷，下面看一下golang中redigo中连接池的使用和原理。 使用示例首先看下redigo中连接池的使用 123456789101112131415161718192021222324252627282930package mainimport ( &quot;fmt&quot; &quot;time&quot;)import ( &quot;github.com/gomodule/redigo/redis&quot;)func main() &#123; pool := &amp;redis.Pool&#123; MaxIdle: 4, MaxActive: 4, Dial: func() (redis.Conn, error) &#123; rc, err := redis.Dial(&quot;tcp&quot;, &quot;127.0.0.1:6379&quot;) if err != nil &#123; return nil, err &#125; return rc, nil &#125;, IdleTimeout: time.Second, Wait: true, &#125; con := pool.Get() str, err := redis.String(con.Do(&quot;get&quot;, &quot;aaa&quot;)) con.Close() fmt.Println(&quot;value: &quot;, str, &quot; err:&quot;, err)&#125; 我们可以看到Redigo使用连接池还是很简单的步骤： 创建连接池 简单设置连接池的最大连接数等参数 注入拨号函数（设置redis地址 端口号等） 调用pool.Get() 获取连接 使用连接Do函数请求redis 关闭连接 源码分析首先看下连接池对象Pool的定义12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667type Pool struct &#123; // Dial is an application supplied function for creating and configuring a // connection. // // The connection returned from Dial must not be in a special state // (subscribed to pubsub channel, transaction started, ...). // 拨号函数，从外部注入 Dial func() (Conn, error) // DialContext is an application supplied function for creating and configuring a // connection with the given context. // // The connection returned from Dial must not be in a special state // (subscribed to pubsub channel, transaction started, ...). DialContext func(ctx context.Context) (Conn, error) // TestOnBorrow is an optional application supplied function for checking // the health of an idle connection before the connection is used again by // the application. Argument t is the time that the connection was returned // to the pool. If the function returns an error, then the connection is // closed. // 检测连接的可用性，从外部注入。如果返回error，则直接关闭连接。 TestOnBorrow func(c Conn, t time.Time) error // Maximum number of idle connections in the pool. // 最大闲置连接数量 MaxIdle int // Maximum number of connections allocated by the pool at a given time. // When zero, there is no limit on the number of connections in the pool. // 最大活动连接数 MaxActive int // Close connections after remaining idle for this duration. If the value // is zero, then idle connections are not closed. Applications should set // the timeout to a value less than the server&apos;s timeout. // 闲置过期时间，在get函数中会有逻辑，删除过期的连接 IdleTimeout time.Duration // If Wait is true and the pool is at the MaxActive limit, then Get() waits // for a connection to be returned to the pool before returning. // 设置如果活动连接达到上限 再获取时候是等待还是返回错误 // 如果是false 系统会返回redigo: connection pool exhausted // 如果是true 会利用p 的ch 属性让线程等待，直到有连接释放出来 Wait bool // Close connections older than this duration. If the value is zero, then // the pool does not close connections based on age. // 连接最长生存时间 如果超过时间会被从链表中删除 MaxConnLifetime time.Duration // 判断ch 是否被初始化了 chInitialized uint32 // set to 1 when field ch is initialized // 锁，这块也给出了以后使用锁的时候一些经验，锁尽量要在某个对象的内部，并且指明哪些变量会用到该锁。 mu sync.Mutex // mu protects the following fields closed bool // set to true when the pool is closed. active int // the number of open connections in the pool // 当p.Wait为true的时候，利用此channel实现阻塞 ch chan struct&#123;&#125; // limits open connections when p.Wait is true // 存放闲置连接的链表 idle idleList // idle connections // 等待获取连接的数量 waitCount int64 // total number of connections waited for. waitDuration time.Duration // total time waited for new connections.&#125; 我们可以看到，其中有几个关键性的字段比如最大活动连接数、最大闲置连接数、闲置链接过期时间、连接生存时间等。 我们知道 连接池最重要的就是两个方法，一个是获取连接，一个是关闭连接。我们来看一下代码： Get源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106// get prunes stale connections and returns a connection from the idle list or// creates a new connection.func (p *Pool) get(ctx context.Context) (*poolConn, error) &#123; // Handle limit for p.Wait == true. // 处理是否需要等待, pool Wait如果是true, 则等待连接释放 var waited time.Duration if p.Wait &amp;&amp; p.MaxActive &gt; 0 &#123; // 这里用到了懒加载的方式，也是单例模式中常用的一种模式，初始化pool中的ch channel p.lazyInit() // wait indicates if we believe it will block so its not 100% accurate // however for stats it should be good enough. // 如果len(p.ch) == 0，则意味着所有的连接都正在被使用中，则需要等待别的client使用完之后，释放此连接 wait := len(p.ch) == 0 var start time.Time if wait &#123; start = time.Now() &#125; if ctx == nil &#123; &lt;-p.ch // 阻塞在此处，一旦有别的连接被close，则就相当于获取到了一个连接。 &#125; else &#123; select &#123; case &lt;-p.ch: case &lt;-ctx.Done(): return nil, ctx.Err() &#125; &#125; if wait &#123; // 计算等待的时间，time.Since是time.Now().Sub(t)的封装 waited = time.Since(start) &#125; &#125; p.mu.Lock() if waited &gt; 0 &#123; p.waitCount++ p.waitDuration += waited &#125; // Prune stale connections at the back of the idle list. // 所有的连接通过链表链接在一起，链表头部是最新的连接，尾部是最旧的连接，因为close方法会把释放的连接放到链表的头部。 // 删除链表尾部的陈旧连接，删除超时的连接 // 连接close之后，连接会回到pool的idle(闲置)链表中 if p.IdleTimeout &gt; 0 &#123; n := p.idle.count for i := 0; i &lt; n &amp;&amp; p.idle.back != nil &amp;&amp; p.idle.back.t.Add(p.IdleTimeout).Before(nowFunc()); i++ &#123; pc := p.idle.back p.idle.popBack() p.mu.Unlock() // 疑问?,为什么要在conn close的时候，要先释放锁呢？ pc.c.Close() p.mu.Lock() p.active-- &#125; &#125; // Get idle connection from the front of idle list. // 从链表的头部获取空闲连接 for p.idle.front != nil &#123; pc := p.idle.front p.idle.popFront() p.mu.Unlock() // 调用验证函数如果返回错误不为nil 关闭连接拿下一个 // 判断连接生存时间 大于生存时间则关闭拿下一个 if (p.TestOnBorrow == nil || p.TestOnBorrow(pc.c, pc.t) == nil) &amp;&amp; (p.MaxConnLifetime == 0 || nowFunc().Sub(pc.created) &lt; p.MaxConnLifetime) &#123; // 如果验证通过，则直接返回此 return pc, nil &#125; pc.c.Close() p.mu.Lock() p.active-- &#125; // Check for pool closed before dialing a new connection. // 判断连接池是否被关闭 如果关闭则解锁报错 if p.closed &#123; p.mu.Unlock() return nil, errors.New(&quot;redigo: get on closed pool&quot;) &#125; // Handle limit for p.Wait == false. // 如果活动连接大于最大连接数，则返回错误 if !p.Wait &amp;&amp; p.MaxActive &gt; 0 &amp;&amp; p.active &gt;= p.MaxActive &#123; p.mu.Unlock() return nil, ErrPoolExhausted &#125; // 如果在链表中没有获取到可用的连接 并添加active数量添加 p.active++ p.mu.Unlock() c, err := p.dial(ctx) // 如果调用失败 则减少active数量 if err != nil &#123; c = nil p.mu.Lock() p.active-- if p.ch != nil &amp;&amp; !p.closed &#123; // 这里主要是处理Wait=true的情况，因为在Wait=true的时候，由于 &lt;-p.ch 代码已经浪费一个channel，所以此处需要补充一个。 p.ch &lt;- struct&#123;&#125;&#123;&#125; &#125; p.mu.Unlock() &#125; return &amp;poolConn&#123;c: c, created: nowFunc()&#125;, err&#125; 下面是lazyInit的源码，跟单例模式中的懒加载模式是一样的。这里叫Fast Path 和 Slow Path。12345678910111213141516171819202122func (p *Pool) lazyInit() &#123; // Fast path. if atomic.LoadUint32(&amp;p.chInitialized) == 1 &#123; return &#125; // Slow path. p.mu.Lock() if p.chInitialized == 0 &#123; p.ch = make(chan struct&#123;&#125;, p.MaxActive) if p.closed &#123; close(p.ch) &#125; else &#123; // 有多少个最大连接数，则初始化多少个channel for i := 0; i &lt; p.MaxActive; i++ &#123; p.ch &lt;- struct&#123;&#125;&#123;&#125; &#125; &#125; // 这里用到了atomic包中的原子操作 atomic.StoreUint32(&amp;p.chInitialized, 1) &#125; p.mu.Unlock()&#125; Close方法和put源码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071func (ac *activeConn) Close() error &#123; pc := ac.pc if pc == nil &#123; return nil &#125; ac.pc = nil // 判断连接的状态 发送取消事务 取消watch if ac.state&amp;connectionMultiState != 0 &#123; pc.c.Send(&quot;DISCARD&quot;) ac.state &amp;^= (connectionMultiState | connectionWatchState) &#125; else if ac.state&amp;connectionWatchState != 0 &#123; pc.c.Send(&quot;UNWATCH&quot;) ac.state &amp;^= connectionWatchState &#125; if ac.state&amp;connectionSubscribeState != 0 &#123; pc.c.Send(&quot;UNSUBSCRIBE&quot;) pc.c.Send(&quot;PUNSUBSCRIBE&quot;) // To detect the end of the message stream, ask the server to echo // a sentinel value and read until we see that value. sentinelOnce.Do(initSentinel) pc.c.Send(&quot;ECHO&quot;, sentinel) pc.c.Flush() for &#123; p, err := pc.c.Receive() if err != nil &#123; break &#125; if p, ok := p.([]byte); ok &amp;&amp; bytes.Equal(p, sentinel) &#123; ac.state &amp;^= connectionSubscribeState break &#125; &#125; &#125; pc.c.Do(&quot;&quot;) // 把连接放入链表 ac.p.put(pc, ac.state != 0 || pc.c.Err() != nil) return nil&#125;// 将连接 重新放入闲置链表func (p *Pool) put(pc *poolConn, forceClose bool) error &#123; p.mu.Lock() if !p.closed &amp;&amp; !forceClose &#123; pc.t = nowFunc() // 放入链表头部，保证了头部的连接都是最新的。 p.idle.pushFront(pc) if p.idle.count &gt; p.MaxIdle &#123; pc = p.idle.back // 释放连接的时候，优先从尾部释放 p.idle.popBack() &#125; else &#123; pc = nil &#125; &#125; if pc != nil &#123; p.mu.Unlock() pc.c.Close() p.mu.Lock() p.active-- &#125; // 如果连接的ch 不为空 并且连接池没有关闭 则给channel中输入一个struct&#123;&#125;&#123;&#125; // 如果在连接达到最大活动数量之后 再获取连接并且pool的Wait为ture 会阻塞线程等待连接被释放 if p.ch != nil &amp;&amp; !p.closed &#123; p.ch &lt;- struct&#123;&#125;&#123;&#125; &#125; p.mu.Unlock() return nil&#125; 总结整个Pool整体流程，我大概画了一个图。从初始化 =》获取 -》创建连接 =》返回连接 =》关闭连接 =》其中还有一条线是Pool.Wait = true 会一直阻塞 一直到有连接Close 释放活动连接数 线程被唤醒返回闲置的连接其实大部分的连接池都是类似的流程，比如goroutine，redis。 参考链接Go Redigo 源码分析(二) 连接池Redigo源码分析]]></content>
      <tags>
        <tag>golang</tag>
        <tag>redigo</tag>
        <tag>pool</tag>
        <tag>连接池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[转载]crontab无法执行脚本原因及解决方法]]></title>
    <url>%2F2019%2F09%2F26%2F%E8%BD%AC%E8%BD%BD-crontab%E6%97%A0%E6%B3%95%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC%E5%8E%9F%E5%9B%A0%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[[原文链接]：https://blog.csdn.net/GX_1_11_real/article/details/86535942我们通常会将一些脚本放入crontab计划任务中，来让系统定时执行一些任务。但是，有的时候会遇到任务无法达成原目标的情况。下面介绍的是关于这些问题的原因及解决方法。 脚本的执行权限问题写好脚本后，要赋予脚本执行权限，避免权限问题 赋予脚本执行权限1chmod +x test1.sh 脚本在crontab里的路径问题 查看crontab中，你的脚本的位置是否有错误 注意是否字符错误 查看计划任务：1crontab -l 写入计划任务：1crontab -e 如无错误，查看路径是否错误在计划任务里要使用全路径 例如：1*/5 * * * * /App/test/test1.sh 修改有问题的字符或路径即可 计划任务的时间设置问题 查看计划任务的时间是否设置的有问题 如果crontab设置的时间错误，会导致无法按原计划时间进行任务 计划任务的５个*号依次代表的含义：1分 时 日 月 周 如有问题，修改为正确的时间即可 脚本的头部问题 查看一下你的脚本的头部，查看你为脚本设置的解释器 1#!/bin/bash 查看当前系统是否包含此解释器或路径是否与脚本头部相同 可以用whereis 查看,例如1whereis sh 如果是脚本的解释器的路径有误，将脚本的头部的路径修改为当前系统的路径即可 脚本的执行命令问题在Linux系统中，使用crontab执行脚本，由于crontab没有环境变量，它是找不到你使用的命令的，需要使用命令的全路径，才可使用命令 例如：使用python10 1 * * * /usr/bin/python /App/test/test1.py 环境变量的问题这个是最常见的问题。crontab执行计划任务时，它并不会从用户的profile文件中读取环境变量，所以会导致命令执行失败。 让脚本读取环境变量 在你的脚本的开头中添加如下几行，也可读取其他变量文件 1234#!/bin/bashsource /etc/profilesource ~/.bash_profile 补充 crontab的环境变量如果遇到crontab中脚本的执行命令无法使用的情况，也可以通过修改/etc/crontab中的PATH添加变量 crontab特殊字符在crontab中，%是用来表示换行的。因此，如果有使用%，需要在前方加入 \ 进行脱义。 例如 date +%F，在crontab中，要写为date +\%F]]></content>
      <tags>
        <tag>crontab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式常用知识]]></title>
    <url>%2F2019%2F08%2F16%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%B8%B8%E7%94%A8%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[常用正则表达式 配置中括号里面的内容1\[(.*?)\]]]></content>
      <tags>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang-go语言常用知识]]></title>
    <url>%2F2019%2F08%2F10%2FGolang-go%E8%AF%AD%E8%A8%80%E5%B8%B8%E7%94%A8%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[main函数接收参数1234567891011121314import ( &quot;flag&quot;)func main() &#123; showHelp := flag.Bool(&quot;h&quot;, false, &quot;show help&quot;) name := flag.String(&quot;n&quot;, &quot;everyone&quot;, &quot;The greeting object&quot;) flag.Parse() if *showHelp &#123; flag.PrintDefaults() return &#125; fmt.Println(&quot;hello &quot;, *name)&#125; golang profile的使用方法 代码中添加如下代码： 123456789import ( _ &quot;net/http/pprof&quot; //采集 HTTP Server 的运行时数据进行分析)func main() &#123; go func() &#123; addr := fmt.Sprintf(&quot;:%d&quot;, 8090) http.ListenAndServe(addr, nil) &#125;()&#125; 编译pprof 12git clone https://github.com/google/pprof.gitgo build 编译命令请参考 https://github.com/google/pprof/blob/master/appveyor.yml 编译go-torch 1https://github.com/uber-archive/go-torch 火焰图生成命令： cpu 使用火焰图： 1go-torch -u http://127.0.0.1:4500 mem 火焰图：– –inuse_space 分析常驻内存： 1go-torch -inuse_space http://127.0.0.1:4500/debug/pprof/heap --colors=mem – –alloc_space 分析程序启动以来所有分配的内存1go-torch -alloc_space http://127.0.0.1:4500/debug/pprof/heap --colors=mem 不用火焰图，分析cpu、mem使用 –inuse_space 分析常驻内存： 1go tool pprof -inuse_space http://127.0.0.1:4500/debug/pprof/heap –alloc_space 分析程序启动以来分配的所有内存： 1go tool pprof -alloc_space http://127.0.0.1:4500/debug/pprof/heap 分析cpu 热点代码 1go tool pprof http://127.0.0.1:4500/debug/pprof/profile 一些显示项的含义 flat代表的是该函数自身代码的执行时长; cum代表的是该函数自身代码+所有调用的函数的执行时长; flat%和cum%指的就是flat耗时和cum耗时占总耗时的百分比; sum%指的就是每一行的flat%与上面所有行的flat%总和，代表从上到下的累计值; 参考链接: https://golang.org/pkg/net/http/pprof/ https://golang.org/doc/diagnostics.html?h=flamegraph https://github.com/google/pprof http://io.upyun.com/2018/01/21/debug-golang-application-with-pprof-and-flame-graph/ https://www.jianshu.com/p/162f44022eb7 http://blog.sina.com.cn/s/blog_48c95a190102xtse.html 判断某个时间是否在某个时间区间之内注意判断边界条件，比如： “2019-09-25 15:40:45” 并不在 “2019-09-25 15:40:45” 与 “2019-09-25 15:40:46”之间。 1234567891011121314151617181920212223242526272829303132333435363738394041 package mainimport ( &quot;fmt&quot; &quot;time&quot;)const ( checkTimeFormat string = &quot;%04d-%02d-%02d %02d:%02d:%02d&quot; timeFormat = &quot;2006-01-02 15:04:05&quot;)func testTimeDuration() &#123; startHour := 15 startMin := 40 endHour := 16 endMin := 59 startTimeStr := fmt.Sprintf(checkTimeFormat, time.Now().Year(), time.Now().Month(), time.Now().Day(), startHour, startMin, 0) endTimeStr := fmt.Sprintf(checkTimeFormat, time.Now().Year(), time.Now().Month(), time.Now().Day(), endHour, endMin, 0) nowTimeStr := time.Now().Format(timeFormat) startTime, err := time.Parse(timeFormat, startTimeStr) endTime, err := time.Parse(timeFormat, endTimeStr) nowTime, err := time.Parse(timeFormat, nowTimeStr) fmt.Println(&quot;start:&quot;, startTime) fmt.Println(&quot;now:&quot;, nowTime) fmt.Println(&quot;end:&quot;, endTime) if err == nil &amp;&amp; nowTime.After(startTime) &amp;&amp; nowTime.Before(endTime) &#123; fmt.Println(&quot;yes&quot;) &#125; else &#123; fmt.Println(&quot;no&quot;) &#125;&#125;func main() &#123; testTimeDuration()&#125; Golang通过GOTRACEBACK生成程序崩溃后core文件方法1：1export GOTRACEBACK=crash eg:12export GOTRACEBACK=crashnohup $&#123;LIMIT&#125; $&#123;BIN_DIR&#125;/proxy $&#123;CONFIG_PARAM&#125; -log $&#123;LOG_DIR&#125;/$&#123;PROXY_LOG&#125; 1&gt;&gt;$&#123;LOG_DIR&#125;/$&#123;PROXY_LOG&#125; 2&gt;&amp;1 &lt;/dev/null &amp; 方法2：1nohup env GOTRACEBACK=crash eg:1/usr/bin/nohup env GOTRACEBACK=crash /home/work/goworkpace/bin/main &gt;&gt; /home/work/goworkpace/log/crash.log]]></content>
      <tags>
        <tag>golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python常用工具大全]]></title>
    <url>%2F2019%2F08%2F01%2Fpython%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[文件操作读文件按行读取文件1234f = open(file_name, &apos;r&apos;)for line in f: // do something with linef.close() 读取文件最后一行12f = open(file_name, &apos;r&apos;)last_line = f.readlines()[-1] list 操作把list形状的字符串转化为list 方法1 123import astline_str = &apos;[12569, 399470, 387182]&apos;line_list = ast.literal_eval(line_str) 方法2 12line_str = &apos;[12569, 399470, 387182]&apos;line_list = line_str[1:len(line_str)-1].split(&quot;,&quot;) 注意，这里有个坑，就是ast.literal_eval耗时超级大，如果能够很简单的转换list，则不要用ast.literal_eval，推荐使用方法2 两个list数组相加 方法1 1234567import numpyfirst = [1, 2, 3, 4, 5]second = [6, 7, 8, 9, 10]a = numpy.array(first)b = numpy.array(second)c= a+bprint c 方法2 1234first = [1, 2, 3, 4, 5]second = [6, 7, 8, 9, 10]result = [x + y for x, y in zip(first, second)]print result lists中list相加123lists_of_lists = [[1, 2, 3], [4, 5, 6]]result = [sum(x) for x in zip(*lists_of_lists)]print result 数字类型相关操作float float 保留两位小数12f_num = 3.1415926round(f_num, 2) 格式化输出12msg = &quot;&#123;&#125;\t&#123;&#125;\t&#123;&#125;&quot;.format(&quot;hello&quot;, &quot;world&quot;, &quot;!&quot;)print(msg) python main函数接收参数123456789101112from argparse import ArgumentParserparser = ArgumentParser()parser.add_argument(&quot;-o&quot;, &quot;--object&quot;, help=&quot;the cmd to master or master&apos;s http services&quot;, metavar=&quot;master/press&quot;)parser.add_argument(&quot;-c&quot;, &quot;--command&quot;, help=&quot;the command to execute&quot;, metavar=&quot;start/stop&quot;)parser.add_argument(&quot;-i&quot;, &quot;--idc&quot;, help=&quot;the idc want to start, default is [all]&quot;, metavar=&quot;[nj/njjs/hba/tc/hna/hnb]&quot;)args = parser.parse_args()print vars(args) # vars(args)可以使参数转化为字典 ssh 相关ssh 远程执行命令1234567891011121314151617import subprocessimport sysHOST=&quot;www.example.org&quot;# Ports are handled in ~/.ssh/config since we use OpenSSHCOMMAND=&quot;uname -a&quot;ssh = subprocess.Popen([&quot;ssh&quot;, &quot;%s&quot; % HOST, COMMAND], shell=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE)result = ssh.stdout.readlines()if result == []: error = ssh.stderr.readlines() print &gt;&gt;sys.stderr, &quot;ERROR: %s&quot; % errorelse: print result python 执行shell命令123456import subprocess# 用subprocess库获取返回值。p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)out, err = p.communicate()for line in out.splitlines(): print (line) python开启http ftp服务可以用来文件传输，方法一1python -m SimpleHTTPServer 方法二12jumbo install python-httpfileserverhttpfileserver profile函数和程序耗时 采样，运行： 1python -m cProfile -o pprof.out main.py argv1 #如果如参数，则可以不用输入argv1 分析，按照时间排序 1python -c &quot;import pstats; p=pstats.Stats(&apos;pprof.out&apos;); p.sort_stats(&apos;time&apos;).print_stats()&quot; | less 中括号[]的正则匹配1234567891011121314import redef test_regex(): pattern = re.compile(r&apos;.*\[([^\[\]]*)\].*uniq=((\d+))&apos;) line = &apos;what is [most import] this is not what i want, uniq=1235235 and others&apos; m = pattern.match(line) if not m or len(m.groups()) != 3: print (&quot;error&quot;) else: print (m.group(0)) print (m.group(1)) print (m.group(2))if __name__ == &apos;__main__&apos;: test_regex() python中文乱码123import sysreload(sys)sys.setdefaultencoding(&apos;utf-8&apos;) python md512345678910from hashlib import md5def test_md5(): str = &quot;hello world, 世界你好&quot; res1 = md5(str).digest() # 8进制，即byte字节 res2 = md5(str).hexdigest() # 16进制 print (res1) print (res2)if __name__ == &apos;__main__&apos;: test_md5()]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++ 常用知识]]></title>
    <url>%2F2019%2F07%2F24%2Fc-%E5%B8%B8%E7%94%A8%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[类型转换string 转 intHow can I convert a std::string to int?Easiest way to convert int to string in C++In C++11 there are some nice new convert functions from std::string to a number type.So instead of string-&gt;int1atoi( str.c_str() ) you can use string-&gt;int1std::stoi( str ) where str is your number as std::string. There are version for all flavours of numbers:1long stol(string), float stof(string), double stod(string),... int-&gt;string1std::string s = std::to_string(42); see http://en.cppreference.com/w/cpp/string/basic_string/stol 文件操作Input/output with files 写文件123456789101112131415// writing on a text file#include &lt;iostream&gt;#include &lt;fstream&gt;using namespace std;int main () &#123; ofstream myfile (&quot;example.txt&quot;); if (myfile.is_open()) &#123; myfile &lt;&lt; &quot;This is a line.\n&quot;; myfile &lt;&lt; &quot;This is another line.\n&quot;; myfile.close(); &#125; else cout &lt;&lt; &quot;Unable to open file&quot;; return 0;&#125; 读文件12345678910111213141516171819// reading a text file#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;string&gt;using namespace std;int main () &#123; string line; ifstream myfile (&quot;example.txt&quot;); if (myfile.is_open()) &#123; while ( getline (myfile,line) ) &#123; cout &lt;&lt; line &lt;&lt; &apos;\n&apos;; &#125; myfile.close(); &#125; else &#123; cout &lt;&lt; &quot;Unable to open file&quot;; &#125; return 0;&#125; 字符串操作字符串splitParse (split) a string in C++ using string delimiter (standard C++)1234567891011121314151617std::string s = &quot;scott&gt;=tiger&gt;=mushroom&quot;;std::string delimiter = &quot;&gt;=&quot;;size_t pos = 0;std::string token;while ((pos = s.find(delimiter)) != std::string::npos) &#123; token = s.substr(0, pos); std::cout &lt;&lt; token &lt;&lt; std::endl; s.erase(0, pos + delimiter.length());&#125;std::cout &lt;&lt; s &lt;&lt; std::endl;// Output:scotttigermushroom md5代码1234567891011121314151617181920212223#include &lt;openssl/md5.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;int main() &#123; MD5_CTX ctx; unsigned char outmd[16]; int i = 0; memset(outmd, 0, sizeof(outmd)); MD5_Init(&amp;ctx); MD5_Update(&amp;ctx, &quot;hel&quot;, 3); MD5_Update(&amp;ctx, &quot;lo&quot;, 2); MD5_Final(outmd, &amp;ctx); for (i = 0; i &lt; 16; i &lt; i++) &#123; printf(&quot;%02X&quot;, outmd[i]); &#125; printf(&quot;\n&quot;); return 0;&#125; 进制转换16进制字符串转为10进制数字1234uint64_t number;std::stringstream ss;ss &lt;&lt; std::hex &lt;&lt; hex_str;ss &gt;&gt; number;]]></content>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux shell常用脚本命令]]></title>
    <url>%2F2019%2F07%2F22%2Flinux-shell%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[sed用法 打印第n行到最后一行的内容sed -n ‘1673523, $p’ bs.gi.log &gt; /home/work/qhl/bs.gi.log 打印匹配abc的行到最后一行的内容。sed -n ‘/abc/,$p’ file.txt注意是单引号 匹配特定字符串1echo $line | sed &apos;s/pattern//g&apos; eg1echo &apos;DB_TYPE_ARR1: rts-weibo:0.80, dnews-weibo:0.80&apos; |grep DB_TYPE_ARR | sed &apos;s/DB_TYPE_ARR//&apos; | sed &apos;s/:[0-9].[0-9]\+//g&apos; 注意，g是全部替换的意思 替换换行1sed &apos;:a;N;$!ba;s/\n/ /g&apos; 示例追加：12$ echo -e &quot;1\n2&quot; | sed &apos;:a;N;$!ba;s/\n/ /g&apos;1 2 替换某个字符串将某个文件中的jack字符串替换为tom 1sed -i &quot;s/jack/tom/g&quot; test.txt 如果替换字符串中有”/“的替换要使用#来替换 1sed -i &apos;s#origin/string#newstring#g&apos;file_name eg 如果一个路径是data/path/abc,现在要将此路径换成一个字符串cde1sed -i &apos;s#data/path/abc#cde#g&apos; file_name shell 10进制转16进制1echo 10 | awk &apos;&#123;printf (&quot;%x\n&quot;, $0)&#125;&apos; tar 用法 打包排除某些文件或文件夹tar -zcvf test.tar.gz -X exclude.list test/其中exclude.list里面包含了那些你不想要的文件或是文件夹 字符串前面补0start=printf &quot;%02d&quot; &quot;$i&quot; shell for循环for ((i=0; i&lt;60; i++));do echo $i;done vim配置vim配置显示中文12345678910111213141516syntax onfiletype indent plugin onset modelineset tabstop=4set expandtabset softtabstop=4set shiftwidth=4set nonumberset cmdheight=1set laststatus=2set fileencodings=utf-8,gb2312,gb18030,gbk,ucs-bom,cp936,latin1set enc=utf8set fencs=utf8,gbk,gb2312,gb18030map &lt;F4&gt; &lt;Esc&gt;:%!python -m json.tool&lt;CR&gt;set numberset expandtab ts=4 sw=4 sts=4 tw=100 copy文件1rsync -avnL twemproxy --exclude=twemproxy/log work@host.com:/home/work/scp_tmp -avnL: 显示哪些要被copy，但是实际上并未执行copy，-avL是真的copy 随机数生成使用系统的 $RANDOM 变量1echo $RANDOM $RANDOM 的范围是 [0, 32767] 想生成2~10范围之间的随机数1echo $(($RANDOM%9+2)) 获取函数的返回值1234567891011121314#!/bin/bashfunction test() &#123; RET_VALUE=15 res=17 return 1&#125;function test1() &#123; test echo $&#123;RET_VALUE&#125; echo $&#123;res&#125;&#125;test1 如上代码会输出 15，17. 即通过全局变量的方式来获取函数的返回值 统计文件的大小1du * -csh awk用法awk 分割之后打印所有的列1awk &apos;BEGIN&#123;RS=&quot;&quot;&#125;&#123;for(a=1;a&lt;=NF;a++) print $a&#125;&apos; connext.txt 查看进程系统运行时间1ps -eo pid,lstart,etime,cmd | grep nginx 杀进程吐core1kill -4 pid kill -l 可以列出所有的数字信号代表的含义。 nc 命令nc是netcat的简写，有着网络界的瑞士军刀美誉。因为它短小精悍、功能实用，被设计为一个简单、可靠的网络工具 nc的作用 （1）实现任意TCP/UDP端口的侦听，nc可以作为server以TCP或UDP方式侦听指定端口 （2）端口的扫描，nc可以作为client发起TCP或UDP连接 （3）机器之间传输文件 （4）机器之间网络测速 eg：开启http服务监听12345端口1nc -l -p 12345 挂在分区，mount，umount分区 查看已挂载分区和文件系统类型1df -T Filesystem Type 1K-blocks Used Available Use% Mounted on/dev/sda1 ext4 20642428 3698868 15894984 19% /tmpfs tmpfs 32947160 0 32947160 0% /dev/shm 格式化分区 1sudo mkfs -t ext4 /dev/sde1 挂载分区 1sudo mount -t ext4 /dev/sde1 /home/ssd1/ grep 命令grep 正则某些字段，并且把其输出为同一行有如下数据1234id=13 name=xiaoming age=35id=15 name=zhangsan age=33id=17 name=lisi age=31id=45 name=wangwu age=25 把grep出的两行合并为1行1grep -oP &quot;(id=\d+)|(name=.+? )&quot; info.txt 输出如下12345678id=13name=xiaomingid=15name=zhangsanid=17name=lisiid=45name=wangwu 如下命令可以让其输出为一行数据1grep -oP &quot;(id=\d+)|(name=.+? )&quot; info.txt | sed &apos;N; s/\n/ /g&apos; 输出如下1234id=13 name=xiaomingid=15 name=zhangsanid=17 name=lisiid=45 name=wangwu 把grep出的三行合并为1行命令1grep -oP &quot;(id=\d+)|(name=.+? )|(age=\d+)&quot; info.txt 输出123456789101112id=13name=xiaomingage=35id=15name=zhangsanage=33id=17name=lisiage=31id=45name=wangwuage=25 命令1grep -oP &quot;(id=\d+)|(name=.+? )|(age=\d+)&quot; info.txt | sed &apos;N;N;s/\n/ /g&apos; 输出1234id=13 name=xiaoming age=35id=15 name=zhangsan age=33id=17 name=lisi age=31id=45 name=wangwu age=2 也可以用 ‘\t’ 来替换1grep -oP &quot;(id=\d+)|(name=.+? )|(age=\d+)&quot; info.txt | sed &apos;N;N;s/\n/\t/g 取出某个字符串后面的字符Can grep output only specified groupings that match?假设有如下文件，想取出foobar后面的字符串12345# file: &apos;test.txt&apos;foobar bash 1bashfoobar happyfoobar 12345678910111213sed -n &quot;s/^.*foobar\s*\(\S*\).*$/\1/p&quot;-n suppress printings substitute^.* anything before foobarfoobar initial search match\s* any white space character (space)\( start capture group\S* capture any non-white space character (word)\) end capture group.*$ anything after the capture group\1 substitute everything with the 1st capture groupp print it 好的参考资料 Shell脚本IF条件判断和判断条件总结 Shell脚本IF条件判断和判断条件总结]]></content>
      <categories>
        <category>常用命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[邮轮邮注意事项]]></title>
    <url>%2F2019%2F05%2F11%2F%E9%82%AE%E8%BD%AE%E9%82%AE%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[邮轮游一些注意事项： 注意事项 轮船上没有信号，需要办理wifi，约140$ 登船过程可能会比较费时（3小时） 船上有自助餐厅免费，但是人多需要排队，可以去收费餐厅，不排队。 提前办理好各个国家的签证。旅行社也可以办理需要交钱。 日期：受限于邮轮出行的时间。 东南亚国家：新加坡、马来西亚、泰国5晚7天花费：1.船费：大约花费8000左右(仅船票，不同房间价格不一致。内舱房:7000,；海景房8000；阳台房：9000)2.小费3.吃，玩(收费项目)+购物 特色：（1）在船上玩(棋牌，游泳，看表演，看海)，吃。（2）靠岸观光城市特色出发：北京飞新加坡-&gt;邮轮-&gt;新加坡飞北京参考链接: https://cruise.ctrip.com/p/804301.html?departurecityid=1#departure=2019-06-16&amp;sellerid=35428_13177 日本：6天5晚长崎+福冈花费：船票6000左右， https://cruise.ctrip.com/c/9467.html#departure=2019-07-14&amp;sellerid=37117_24335]]></content>
      <tags>
        <tag>游玩</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多维度资源分配问题-如何提高集群的资源利用率？]]></title>
    <url>%2F2019%2F04%2F16%2F%E5%A4%9A%E7%BB%B4%E5%BA%A6%E8%B5%84%E6%BA%90%E5%88%86%E9%85%8D%E9%97%AE%E9%A2%98-%E5%A6%82%E4%BD%95%E6%8F%90%E9%AB%98%E9%9B%86%E7%BE%A4%E7%9A%84%E8%B5%84%E6%BA%90%E5%88%A9%E7%94%A8%E7%8E%87%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[多维度资源分配问题，在系统调度中是经常遇到的一个问题，比如，集群中有cpu，mem，ssd等资源，每个作业需要的每个维度的资源不一样，如何分配集群中的资源给哪些job才能够使得集群的资源利用率最大呢？这就是比较典型的资源分配问题。对于一些需要一定执行时间的job，这个问题的优化目标还会牵扯到最小化总的执行之间以及让各个job等待处理的时间尽量的平均，即公平性。下面主要介绍论文《Multi-dimensional Resource Integrated Scheduling In a Shared Data Center》中的一些思想方法。 论文主要关注分配率的问题，即如何分配job才能够使得整机的资源利用率最佳，这个对于服务部署类很有借鉴作用。当部署服务的时候，比如通过k8s来部署服务，这类服务一般为常驻服务，即没有特殊情况，会一直存活，没有所谓的job执行时间（换句话说，job执行时间也就是永远，除非job出故障）。对于这类服务，一般服务本身都会说明对资源的需求，比如内存=2GB，cpu=2核，ssd=500GB。那么如何调度job才能够使得机器的资源利用率最高呢？ 问题引入假设一个机器拥有12 CPU core，12GB memory，12TB disk。我们写成&lt;12,12,12&gt;。现在有4个待分配的任务，每个任务对资源的需求分别描述为如下所示：app1=&lt;6,2,2&gt;， app2=&lt;5,4,8&gt;, app3=&lt;4,6,2&gt;, app4=&lt;2,2,6&gt;。那么如何安排这几个app到这个机器上面，能够使得这个机器的资源利用率最大呢？ 下面对比一下如下两种调度策略： S1:FIFO，显然只能够调度app1和app2，因为这两个app对cpu的资源需求都比较大。这样调度的话就会浪费mem和disk的资源。如下所示： S2:按照对资源的需求来调度，这就是作者的核心思想，实时计算哪种资源是稀缺的，哪种资源是比较充足的，根据此来调度。后面会详细讲述，但是假设我们调度app4，app3，app1的话，则资源利用率明显要高于S1。 建模 R=&lt;r1,r1,ri … rm&gt;：设一个数据中心的资源为R，一共有m个维度的资源，比如cpu，mem，disk等等，第i个资源用资源ri来表述，则数据中心的资源R=&lt; r1,r2,ri … rm &gt; AP=&lt; AP1, AP2…APN &gt; :APj：假设任务中共有n个应用任务，每个任务记为APj。 wj：任务j对资源的需求，比如任务j对资源i的需求记为wij， 则wj= &lt; w1j, w2j, wij … wmj &gt; uj：任务APj的个数。即有多少个待调度的APj任务。 A=&lt; x1, x2 … xn &gt;：一种调度策略，调度了xj的APj的任务。 有了上面几个]]></content>
      <tags>
        <tag>多维度</tag>
        <tag>Multi-dimensional</tag>
        <tag>资源分配</tag>
        <tag>resource allocation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法-24 | 二叉树基础（下）：有了如此高效的散列表，为什么还需要二叉树?]]></title>
    <url>%2F2019%2F03%2F21%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-24-%E4%BA%8C%E5%8F%89%E6%A0%91%E5%9F%BA%E7%A1%80%EF%BC%88%E4%B8%8B%EF%BC%89%EF%BC%9A%E6%9C%89%E4%BA%86%E5%A6%82%E6%AD%A4%E9%AB%98%E6%95%88%E7%9A%84%E6%95%A3%E5%88%97%E8%A1%A8%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E9%9C%80%E8%A6%81%E4%BA%8C%E5%8F%89%E6%A0%91%2F</url>
    <content type="text"><![CDATA[我们在散列表那节中讲过，散列表的插入、删除、查找操作的时间复杂度可以做到常量级的O(1)，非常高效。而二叉查找树在比较平衡的情况下，插入、删除、查找操作时间复杂度才是O(logn)，相对散列表，好像并没有什么优势，那我们为什么还要用二叉查找树呢？ 第一，散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在 O(n) 的时间复杂度内，输出有序的数据序列。 第二，散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在O(logn)。 第三，笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logn 小，所以实际的查找速度可能不一定比 O(logn) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高 第四，散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。 最后，为了避免过多的散列冲突，散列表装载因子不能太大，特别是基于开放寻址法解决冲突的散列表，不然会浪费一定的存储空间 综合这几点，平衡二叉查找树在某些方面还是优于散列表的，所以，这两者的存在并不冲突。我们在实际的开发过程中，需要结合具体的需求来选择使用哪一个。]]></content>
  </entry>
  <entry>
    <title><![CDATA[区块链-一些基本概念]]></title>
    <url>%2F2019%2F03%2F17%2F%E5%8C%BA%E5%9D%97%E9%93%BE-%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[Hashcollision resistance（collision free）: 是说在密码学中，对于一个hash函数，很难找到两个不同的输入使得其hash之后的输出是一样的。Collision resistance is a property of cryptographic hash functions: a hash function H is collision resistant if it is hard to find two inputs that hash to the same output; that is, two inputs a and b such that H(a) = H(b), and a ≠ b. hiding：hash函数的计算过程是单向的，是不可逆的。给定一个输入X，可以计算其hash值 H(X)，但是通过H(X)，很难知道其输出为X。当然蛮力求解也是一个办法。hiding性质的前提是：（1）输入空间较大；（2）输入的分布比较分散。 digital commitment（digital equivalent of a sealed envelope）：通过collision resistance和hiding的性质，就可以得到digital commitment这个性质。请参考Digital Envelopes and Signatures puzzle friendly: 要想使得计算得到的hash值在某一个范围之内，则只能够一个一个的输入去尝试，很难直接找到某个值使得其hash值在某一个范围内。挖矿 也是就是这个意思。挖矿就是把区块中的一些信息+随机数进行hash，使得其结果前K位数为0，才能够满足要求。挖矿无捷径，只能够去大量的试。所以也就产生了 工作量证明（POW）。 工作量证明（POW) : 就是做了大量的尝试之后，才得到符合要求的结果，这个过程叫做工作量证明。 签名签名用的是私钥，验证签名使用的是公钥 如何保证两个人的公钥，私钥都是不一样的？理论上可以做到，但是实际上不可能，其概率比地球爆炸的概率还要低 数据结构hash pointers：与普通的指针的区别：普通的指针指向所在内存的起始地址，但是hash 指针除了保存起始地址之外，还会保存指向内存的hash值，这样就可以很容易的验证这块内存（这个区块）是否被篡改。 tamper-evident log : 防篡改log，就是利用了区块链中后一个区块中会保存前一个区块中的hash值。牵一发动全身！ Merkle tree ： 典型的属于以时间换空间。与链式的区块一样，其中root hash也能够检测出这个区块链是否被篡改。可以用logn的时间定位出那个区块被修改. 全节点：包含真正交易数据的节点。 轻节点：只包含hash header的节点。 merkle proof （proof of membership） : 证明merkle tree中包含了某个交易。比如，某个全节点A向一个轻节点B转了一笔账，所以A需要向B发送一个merkel tree，然后B就可以来证明这个merkle tree是否是正确的。时间复杂度O（log n） （proof of no membership） :遍历全部的区块，O(n)。但是，如果所有的区块是按照hash值排序的话（sorted merkle tree），那么就可以利用O（log n）的时间来验证某个交易是否在这个merkel tree中。 备注： hash 指针不能够有环。否则就会造成循环依赖，造成死锁了。 共识协议双花攻击（double spending attack）转账的时候，输入A要说明两个东西：（1）币的来源，来证明我有钱可以给你转账。（2）A的公钥 （3）A的公钥的hash，也就是之前币的来源的那个区块的输出的hash值。这样通过（2）和（3）来验证是否是真的A转账，还是某一个A’冒充A，像用A来进行转账骗钱。 分布式共识consensus in bitcoin sybil attack(女巫攻击)：某个超级计算机产生的账户个数超过了总数的一半。所以比特币的投票机制并不是利用账户的数目来投票，而是按照计算力来投票。 分叉攻击（forking attack） 正常分叉两个机器同时挖到矿了，就有可能同时产生两个区块都连接在合法链的最后。 比特币-实现比特币：Transaction-based ledger以太坊：account-based ledger UTXO(Unspent Transaction Output)指的是那些收到的比特币，但是没有花出去的数据，全部保存在UTXO的数据结构中。目的：为了防止双花，用来校验双花的。也就说是UTXO保留了所有的那些没有花出去的比特币。 Transaction Fee (交易费)为什么大家都争抢着去拥有这个记账权呢？因为获得记账权的那个节点能够获得两方面的奖励（1）区块奖励，这个占据了大头（2）交易费 Bernoulli trialBernoulli trial：a random experiment with binary outcome]]></content>
  </entry>
  <entry>
    <title><![CDATA[jupyter安装一些坑]]></title>
    <url>%2F2019%2F03%2F07%2Fjupyter%E5%AE%89%E8%A3%85%E4%B8%80%E4%BA%9B%E5%9D%91%2F</url>
    <content type="text"><![CDATA[mac 上的jupyter安装上之后，启动jupyter失败。报如下错误nicodeDecodeError: ‘ascii’ codec can’t decode byte 0xe5 in position 4: ordinal not in range(128)是因为语言编码的问题。于是尝试使用如下命令打开jupyter:1LANG=zn jupyter notebook 参考链接：Jupyter打开出错：’ascii’ codec can’t decode byte 0xe5 in position 4: ordinal not in range(128)]]></content>
      <tags>
        <tag>坑</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法-08 | 栈：如何实现浏览器的前进和后退功能?]]></title>
    <url>%2F2019%2F03%2F07%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-08-%E6%A0%88%EF%BC%9A%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E6%B5%8F%E8%A7%88%EF%A8%B8%E7%9A%84%E5%89%8D%E8%BF%9B%E5%92%8C%E5%90%8E%E9%80%80%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[栈是大家都比较熟悉的数据结构了，个人认为本篇文章的重点在于课后的那两个思考题。 为什么函数调用要用“栈”来保存临时变量呢？其他的数据结构不可以吗？ JVM中的堆栈的概念与平时我们说的堆、栈是一样的意思吗？ 为什么函数调用要用“栈”来保存临时变量呢？其他的数据结构不可以吗？栈有一个很重要的特性就是先进后出，这与函数调用时候的关系如出一辙，假设函数A调用函数B，显然也是先把B函数中的一些临时变量计算完成之后，在去计算A的。这有点像递归调用的味道。 从调用函数进入被调用函数，对于数据来说，变化的是什么呢？是作用域。所以根本上，只要能保证每进入一个新的函数，都是一个新的作用域就可以。而要实现这个，用栈就非常方便。在进入被调用函数的时候，分配一段栈空间给这个函数的变量，在函数结束的时候，将栈顶复位，正好回到调用函数的作用域内。 知乎一篇文章说明了为什么函数调用用栈实现，说的也是上面的意思。为什么函数调用要用栈实现？ JVM中的堆栈的概念与平时我们说的堆、栈是一样的意思吗？JVM中的JAVA内存区域组成部分也较多，可以简单的就理解为两大部分：堆和栈。 堆：主要存放java的对象，是被所有的线程共享的。（区别数据结构的堆，它是一棵平衡二叉树） The Java Virtual Machine has a heap that is shared among all Java Virtual Machine threads. The heap is the run-time data area from which memory for all class instances and arrays is allocated.The heap is created on virtual machine start-up. Heap storage for objects is reclaimed by an automatic storage management system (known as a garbage collector); objects are never explicitly deallocated. The Java Virtual Machine assumes no particular type of automatic storage management system, and the storage management technique may be chosen according to the implementor’s system requirements. The heap may be of a fixed size or may be expanded as required by the computation and may be contracted if a larger heap becomes unnecessary. The memory for the heap does not need to be contiguous. 栈：主要存放线程内的一些临时变量，或是线程调用帧，每个线程独享的。这个栈就相当于函数调用的栈。 Each Java Virtual Machine thread has a private Java Virtual Machine stack, created at the same time as the thread. A Java Virtual Machine stack stores frames (§2.6). A Java Virtual Machine stack is analogous to the stack of a conventional language such as C: it holds local variables and partial results, and plays a part in method invocation and return. Because the Java Virtual Machine stack is never manipulated directly except to push and pop frames, frames may be heap allocated. The memory for a Java Virtual Machine stack does not need to be contiguous. 当然，JAVA内存模型中还有其他的部分，比如：方法区，本地方法栈等。 方法区：可以理解为一块堆，被所有的线程共享。常用来存储一些类信息，常量，静态变量，即时编译器编译后的代码等数据。 本地方法栈：只不过是为了使得用非java语言能够调用java语言逻辑上的一个栈，但实际上可以与虚拟机栈是同一块物理内存。 An implementation of the Java Virtual Machine may use conventional stacks, colloquially called “C stacks,” to support native methods (methods written in a language other than the Java programming language). Native method stacks may also be used by the implementation of an interpreter for the Java Virtual Machine’s instruction set in a language such as C. Java Virtual Machine implementations that cannot load native methods and that do not themselves rely on conventional stacks need not supply native method stacks. If supplied, native method stacks are typically allocated per thread when each thread is created. 参考文献：Java虚拟机的堆、栈、堆栈如何去理解？为什么函数调用要用栈实现？]]></content>
  </entry>
  <entry>
    <title><![CDATA[第一性原理到底是什么？]]></title>
    <url>%2F2019%2F03%2F05%2F%E7%AC%AC%E4%B8%80%E6%80%A7%E5%8E%9F%E7%90%86%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[【转载】第一性原理到底是什么？ 自从“钢铁侠”伊隆·马斯克在一次采访中，吹牛逼吹出个大家听都没听过的哲学名词——第一性原理（厉害哦，老铁，哲学你都懂），后来，很多媒体和看瓜群众不断地讨论“第一性原理”，出现了各种各样的解读版本。我想，既然你看了这么多版本，也不在乎多看一个胡说八道的版本吧。]]></content>
      <tags>
        <tag>哲学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法-06 | 链表（上）：如何实现LRU缓存淘汰算法?]]></title>
    <url>%2F2019%2F03%2F05%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-06-%E9%93%BE%E8%A1%A8%EF%BC%88%E4%B8%8A%EF%BC%89%EF%BC%9A%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0LRU%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[缓存、链表形式的缓存、数组形式的缓存、回文的判定 什么是缓存？缓存是一种提高数据读取性能的技术，在硬件设计、软件开发中都有着非广泛的应用，比如常见的CPU缓存、数据库缓存、浏览器缓存等等。 为什么使用缓存？即缓存的特点缓存的大小是有限的，当缓存被用满时，哪些数据应该被清理出去，哪些数据应该被保留？就需要用到缓存淘汰策略。 什么是缓存淘汰策略？指的是当缓存被用满时清理数据的优先顺序。 有哪些缓存淘汰策略？常见的3种包括先进先出策略FIFO（First In，First Out）、最少使用策略LFU（Least Frenquently Used）、最近最少使用策略LRU（Least Recently Used）。 链表实现LRU缓存淘汰策略当访问的数据没有存储在缓存的链表中时，直接将数据插入链表表头，时间复杂度为O(1)；当访问的数据存在于存储的链表中时，将该数据对应的节点，插入到链表表头,时间复杂度为O(n)。如果缓存被占满，则从链表尾部的数据开始清理，时间复杂度为O(1)。 数组实现LRU缓存淘汰策略方式一：首位置保存最新访问数据，末尾位置优先清理当访问的数据未存在于缓存的数组中时，直接将数据插入数组第一个元素位置，此时数组所有元素需要向后移动1个位置，时间复杂度为O(n)；当访问的数据存在于缓存的数组中时，查找到数据并将其插入数组的第一个位置，此时亦需移动数组元素，时间复杂度为O(n)。缓存用满时，则清理掉末尾的数据，时间复杂度为O(1)。 方式二：首位置优先清理，末尾位置保存最新访问数据当访问的数据未存在于缓存的数组中时，直接将数据添加进数组作为当前最有一个元素时间复杂度为O(1)；当访问的数据存在于缓存的数组中时，查找到数据并将其插入当前数组最后一个元素的位置，此时亦需移动数组元素，时间复杂度为O(n)。缓存用满时，则清理掉数组首位置的元素，且剩余数组元素需整体前移一位，时间复杂度为O(n)。（优化：清理的时候可以考虑一次性清理一定数量，从而降低清理次数，提高性能。） 如何通过单链表实现“判断某个字符串是否为水仙花字符串”？（比如 上海自来水来自海上, ABCBA） 核心思想是用两个快慢指针，快指针是慢指针移动速度的两倍，当快指针到末尾的时候，慢指正正好到中点位置； 创建一个指针A，每次移动的时候，都把慢指针访问过的节点翻转。 遍历创建节点和慢节点，看其访问的value是否一致。（此时创建的指针A从中间访问到链表头，而慢节点是从中间访问到链表尾部） 附录：判断一个链表是否是回文的代码`/** Implement a function to check if a linked list is a palindrome don’t forget import statements! 判断一个链表是否是一个回文（回文指的是正着读，倒着读都是一样的文字）*/public class IsPalindrome { /** 这个方法是把所有的数据取出来放入到一个栈中。然后在遍历栈看是否与之前遍历的数据一致。 时间 O（n） 空间 O（n）*/public static boolean isPalindrome(LinkedListNode head) { Stack stack = new Stack(); StringBuilder sb = new StringBuilder(); while (head != null) { stack.add(head.getValue()); head = head.getPost(); sb.append(head.getValue()); } StringBuilder sb2 = new StringBuilder(); while (!stack.empty()) { sb2.append(stack.pop()); } return sb.toString().equals(sb2.toString());} /** 递归版本 时间 O(n) 空间 O(1)*/LinkedListNode left; public boolean isPalindrome2(LinkedListNode head) { left = head; boolean result = helper(head); return result; } public boolean helper(LinkedListNode right) { if (right == null) { return true; } boolean x = helper(right.getPost()); if (!x) { return false; } boolean y = (left.getValue() == right.getValue()); left = left.getPost(); return y; } /** 使用快慢两个指针找到链表中点，慢指针每次前进一步，快指针每次前进两步。 在慢指针前进的过程中，同时修改其 next 指针，使得链表前半部分反序。最后比较中点两侧的链表是否相等。 时间 O(n) 空间 O(1)* @return true or false*/public boolean isPalindrome3(LinkedListNode head) { if (head == null || head.getPost() == null) { return true; } LinkedListNode pre = null; LinkedListNode fast = head; LinkedListNode slow = head; while (fast != null &amp;&amp; fast.getPost() != null) { fast = fast.getPost().getPost(); LinkedListNode next = slow.getPost(); slow.setPost(pre); pre = slow; slow = next; } if (fast != null) { slow = slow.getPost(); } while (slow != null) { if (slow.getValue() != pre.getValue()) { return false; } slow = slow.getPost(); pre = pre.getPost(); } return true;} `/** Implement a function to check if a linked list is a palindrome don’t forget import statements! 判断一个链表是否是一个回文（回文指的是正着读，倒着读都是一样的文字）*/public class IsPalindrome { /** 这个方法是把所有的数据取出来放入到一个栈中。然后在遍历栈看是否与之前遍历的数据一致。 时间 O（n） 空间 O（n）*/public static boolean isPalindrome(LinkedListNode head) { Stack stack = new Stack(); StringBuilder sb = new StringBuilder(); while (head != null) { stack.add(head.getValue()); head = head.getPost(); sb.append(head.getValue()); } StringBuilder sb2 = new StringBuilder(); while (!stack.empty()) { sb2.append(stack.pop()); } return sb.toString().equals(sb2.toString());} /** 递归版本 时间 O(n) 空间 O(1)*/LinkedListNode left; public boolean isPalindrome2(LinkedListNode head) { left = head; boolean result = helper(head); return result; } public boolean helper(LinkedListNode right) { if (right == null) { return true; } boolean x = helper(right.getPost()); if (!x) { return false; } boolean y = (left.getValue() == right.getValue()); left = left.getPost(); return y; } /** 使用快慢两个指针找到链表中点，慢指针每次前进一步，快指针每次前进两步。 在慢指针前进的过程中，同时修改其 next 指针，使得链表前半部分反序。最后比较中点两侧的链表是否相等。 时间 O(n) 空间 O(1)* @return true or false*/public boolean isPalindrome3(LinkedListNode head) { if (head == null || head.getPost() == null) { return true; } LinkedListNode pre = null; LinkedListNode fast = head; LinkedListNode slow = head; while (fast != null &amp;&amp; fast.getPost() != null) { fast = fast.getPost().getPost(); LinkedListNode next = slow.getPost(); slow.setPost(pre); pre = slow; slow = next; } if (fast != null) { slow = slow.getPost(); } while (slow != null) { if (slow.getValue() != pre.getValue()) { return false; } slow = slow.getPost(); pre = pre.getPost(); } return true;}]]></content>
      <tags>
        <tag>极客时间</tag>
        <tag>数据结构与算法</tag>
        <tag>06 | 链表（上）：如何实现LRU缓存淘汰算法?</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法-05-数组-为什么很多编程语言中数组都从0开始编号]]></title>
    <url>%2F2019%2F03%2F01%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-05-%E6%95%B0%E7%BB%84-%E4%B8%BA%EF%A7%BD%E4%B9%88%E5%BE%88%E5%A4%9A%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80%E4%B8%AD%E6%95%B0%E7%BB%84%E9%83%BD%E4%BB%8E0%E5%BC%80%E5%A7%8B%E7%BC%96%E5%8F%B7%2F</url>
    <content type="text"><![CDATA[本小节讲解了如下几个问题 如何实现随机访问？ 数组的删除和插入操作的复杂度？ 为什么下标从0开始？ 严格控制数组的越界访问！ 容器和数组的关系? JVM和数组有什么相同之处呢？ 下面依次回答以上几个问题 如何实现随机访问？大家都知道数组能够实现随机快速访问(指的是读)；数组与其他的数据结构有什么区别呢？ 数组的定义：数组（Array）是一种 线性表 数据结构。它用一组 连续的内存空间 ，来存储一组具有 相同类型 的数据。线性表：线性表就是数据排成像一条线一样的结构。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。非线性表：二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。 数组的删除和插入操作的复杂度？插入数据x到位置i读取位置i的数据 有序数组的插入，当插入一个数据之后，要把插入位置之后的所有数据全部往后移动一个位置。插入:O(n)读取:O(1) 无序数组的插入，当想把数据x插入到位置i；只需要将数据x插入到数组最后一个位，然后在与位置i的数据交换即可。插入:O(1)读取:O(1) 删除操作 与插入类似 但是，当只是标记删除，并不移动其他数据呢？ 比如数组 a[10] 中存储了 8 个元素：a，b，c，d，e，f，g，h。现在，我们要依次删除 a，b，c 三个元素。为了避免 d，e，f，g，h 这几个数据会被搬移三次，我们可以先记录下已经删除的数据。每次的删除操作并不是真正地搬移数据，只是记录数据已经被删除。当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。 感受到了和JVM垃圾回收机制有没有什么相似之处呢？ 为什么下标从0开始？从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”。前面也讲到，如果用 a 来表示数组的首地址，a[0] 就是偏移为 0 的位置，也就是首地址，a[k] 就表示偏移 k 个 type_size 的位置，所以计算 a[k] 的内存地址只需要用这个公式：1a[k]_address = base_address + k * type_size 但是，如果数组从 1 开始计数，那我们计算数组元素 a[k] 的内存地址就会变为：1a[k]_address = base_address + (k-1)*type_size 对比两个公式，我们不难发现，从 1 开始编号，每次随机访问数组元素都多了一次减法运算，对于 CPU 来说，就是多了一次减法指令。 当然，也有可能是因为历史原因。。 控制数组的越界访问！这个不用说了吧，对于java来说，数组越界会进行越界检查。但是对于C等语言来说，数组越界就会产生未定义行为。 容器和数组的关系? java种ArrayList是一种不用定义大小的数组，也能够很方便的进行数据的插入和数据的删除。但是由于其长度的动态变化，所以当在知道数据大小的前提下，还是使用数组比较好。或者直接指定ArrayList的大小。同时ArrayList种的对象也只能够是封装好的Integer，Long等类型， Java ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。 如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。 还有一个是我个人的喜好，当要表示多维数组时，用数组往往会更加直观。比如Object[][] array；而用容器的话则需要这样定义：ArrayList array。 JVM和数组有什么相同之处呢？JVM的标记-整理算法：会先把垃圾进行标记，然后让所有存活的对象都向一端移动，然后直接清理掉边界以外的内存。]]></content>
      <tags>
        <tag>极客时间</tag>
        <tag>数据结构与算法</tag>
        <tag>05-数组-为什么很多编程语言中数组都从0开始编号</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法-04-复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度]]></title>
    <url>%2F2019%2F02%2F28%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-04-%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%8B%EF%BC%89%EF%BC%9A%E6%B5%85%E6%9E%90%E6%9C%80%E5%A5%BD%E3%80%81%E6%9C%80%E5%9D%8F%E3%80%81%E5%B9%B3%E5%9D%87%E3%80%81%E5%9D%87%E6%91%8A%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[本小节主要讲解了分析时间复杂度的几个点：最好情况的时间复杂度(best case time complexity)，最坏情况的时间复杂度(worst case time complexity)，平均情况的时间复杂度(average case time complexity)，均摊时间复杂度(amortized time complexity)。 最好情况的时间复杂度最好情况，显然就是当算法中的n取某个值的时候，使得这个算法执行的时间周期最短，就是最好情况的时间复杂度。也就是在最理想的情况下，执行这段代码的时间复杂度。 最坏情况的时间复杂度最坏情况，显然，就是在最糟糕的情况下，这段代码执行的时间复杂度。 平均情况的时间复杂度平均情况复杂度，并不是简单的（最好情况+最坏情况）/2 即可。而是需要判断出n的某个取值的概率，然后乘以此时的时间复杂度。得出来的就是平均情况的时间复杂度。 举个例子：对于一个数据规模为n的算法。假设n取每个值的概率为p(i)，当n取值为i的时候的时间复杂度为O(i)，则其这个算法的时间复杂度为：$\sum_{i=0}^n\p(i)*O(i)$ 均摊时间复杂度首先明确一点，均摊时间复杂度就是平均情况时间复杂度的一种特殊情况。 举个例子：1234567891011121314151617// array 表示一个长度为 n 的数组// 代码中的 array.length 就等于 nint[] array = new int[n];int count = 0;void insert(int val) &#123; if (count == array.length) &#123; int sum = 0; for (int i = 0; i &lt; array.length; ++i) &#123; sum = sum + array[i]; &#125; array[0] = sum; count = 1; &#125; array[count] = val; ++count;&#125; 这段代码是插入一个数字，当数组中有空闲时候，就直接插入到这个数组中；当数组中没有空闲的时候，把这个数组求和，放到数组的第一个array[0]中，然后把新来的数据插入到后面的数组中。（先不care这个代码是否合理） 显然：最理想情况下，就是数组没有满的时候，时间复杂度为O(1)；最坏的情况，就是当数组满的时候，时间复杂度为O(n)；那么平均呢？假设数组的长度是 n，根据数据插入的位置的不同，我们可以分为 n 种情况，每种情况的时间复杂度是 O(1)。除此之外，还有一种“额外”的情况，就是在数组没有空闲空间时插入一个数据，这个时候的时间复杂度是 O(n)。而且，这 n+1 种情况发生的概率一样，都是1/(n+1)。所以，根据加权平均的计算方法，我们求得的平均时间复杂度就是： 但是这种特殊的场景，还有一个更加简单的分析方法：摊还分析法，也叫作均摊时间复杂度。我们还是继续看在数组中插入数据的这个例子。每一次 O(n) 的插入操作，都会跟着 n-1次 O(1) 的插入操作，所以把耗时多的那次操作均摊到接下来的 n-1 次耗时少的操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是 O(1)。这就是均摊分析的大致思路。你都理解了吗？ 课后思考你可以用今天学习的知识，来分析一下下面这个 add() 函数的时间复杂度 12345678910111213141516171819202122// 全局变量，大小为 10 的数组 array，长度 len，下标 i。int array[] = new int[10];int len = 10;int i = 0;// 往数组中添加一个元素void add(int element) &#123; if (i &gt;= len) &#123; // 数组空间不够了 // 重新申请一个 2 倍大小的数组空间 int new_array[] = new int[len*2]; // 把原来 array 数组中的数据依次 copy 到 new_array for (int j = 0; j &lt; len; ++j) &#123; new_array[j] = array[j]; &#125; // new_array 复制给 array，array 现在大小就是 2 倍 len 了 array = new_array; len = 2 * len; &#125; // 将 element 放到下标为 i 的位置，下标 i 加一 array[i] = element; ++i;&#125; 答案：最好O(1)，最差O(n)，均摊O(1)]]></content>
      <tags>
        <tag>极客时间</tag>
        <tag>数据结构与算法</tag>
        <tag>04-复杂度分析（下）：浅析最好、最坏、平均、均摊时间复杂度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法-03-复杂度分析(上)-如何分析、统计算法的执行效率和资源消耗?]]></title>
    <url>%2F2019%2F02%2F27%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-03-%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90-%E4%B8%8A-%E5%A6%82%E4%BD%95%E5%88%86%E6%9E%90%E3%80%81%E7%BB%9F%E8%AE%A1%E7%AE%97%E6%B3%95%E7%9A%84%E6%89%A7%E8%A1%8C%E6%95%88%E7%8E%87%E5%92%8C%E8%B5%84%E6%BA%90%E6%B6%88%E8%80%97%2F</url>
    <content type="text"><![CDATA[本小结主要讲解了时间复杂度和空间复杂度。主要解释了如下几个问题 为什么需要复杂度分析？ 大O复杂度表示法 时间复杂度分析的方法 空间复杂度的分析方法 为什么需要复杂度分析？Q：直接把程序拿到环境中跑一下不就可以了吗？A：不可以，因为每个机器的环境不同，数据不同，无法对比。虽然这种方法竟然也被称为 事后统计法 大O复杂度表示法注意：大O时间复杂度表示法 实际上并具体表示代码的真正执行时间，而是表示代码执行时间随数据的规模增长的变化趋势，所以，也叫作 渐进时间复杂度，简称为复杂度 时间复杂度分析的方法 只关注循环中次数最多的一段代码 加法法则：总复杂度等于量级最大的那段代码的复杂度如果 T1(n)=O(f(n))，T2(n)=O(g(n))那么 T(n)=T1(n)+T2(n)=max(O(f(n), O(g(n))) =O(max(f(n), g(n))) 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么 T(n)=T1(n)*T2(n)=O(f(n)*g(n)). 几种常见时间复杂度实例分析 注意，看如下代码,我们能够很容易的知道，这个时间复杂度是O(log2 n) = O(log n) 1234i=1;while (i &lt;= n) &#123; i = i * 2;&#125; 再看如下代码，我们也能够很容易的知道其复杂度为O(log3 n)，有因为O(log3 n) = O(log3 2) * O(log2 n),因为分析时间复杂度的时候，会忽略常量系数，所以最终的时间复杂度还是O(log n)1234i=1;while (i &lt;= n) &#123; i = i * 3;&#125; 空间复杂度分析空间复杂度分析就是看其所占用的空间与数据规模n的关系，我们常见的空间复杂度就是O(1)、O(n)、O(n2)，像 O(logn)、O(nlogn) 这样的对数阶复杂度平时都用不到。 总结复杂度也叫渐进复杂度，包括时间复杂度和空间复杂度，用来分析法执行效率与数据规模之间的增长关系，可以粗略地表示，越高阶复杂度的算法，执行效率越低。常见的复杂度并不多，从低阶到高阶有：O(1)、O(logn)、O(n)、O(nlogn)、O(n2 ) 课后思考有人说，我们项目之前都会进行性能测试，再做代码的时间复杂度、空间复杂度分析，是不是多此一举呢？而且，每段代码都分析一下时间复杂度、空间复杂度，是不是很浪费时间呢？你怎么看待这个问题呢？ 个人认为：项目之间会评估整个项目的架构是否合理，性能预估是否达标，但是并不代表项目做出来之后性能就是好的，当在写项目代码的时候，如果能够把时间复杂度和空间复杂度这两个概念贯穿整个编程的脑海中，那么第一能够锻炼自己的思维，第二也能够写出高质量的代码，能够提升性能，毕竟对一个公司来说，性能提升可能就意味着利润的上涨。]]></content>
      <tags>
        <tag>极客时间</tag>
        <tag>数据结构与算法</tag>
        <tag>03-复杂度分析(上)-如何分析、统计算法的执行效率和资源消耗?</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构与算法-02-如何抓住重点，系统高效地学习数据结构与算法]]></title>
    <url>%2F2019%2F02%2F26%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-02-%E5%A6%82%E4%BD%95%E6%8A%93%E4%BD%8F%E9%87%8D%E7%82%B9%EF%BC%8C%E7%B3%BB%E7%BB%9F%E9%AB%98%E6%95%88%E5%9C%B0%E5%AD%A6%E4%B9%A0%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[本系列是阅读极客时间专栏《数据结构与算法》的读书笔记，希望能够记录自己学习过程中的感想和所学，努力提升自己。 在02篇中，作者主要讲解了在本专栏中，学什么？怎么学？的问题。 学什么？ 时间复杂度和空间复杂度 首先要知道什么样的数据结构和算法是好的，什么是不好的，怎么样衡量好不好呢？那就是用时间复杂度和空间复杂度来分析。时间复杂度来分析算法的快慢，空间复杂度来分析数据结构所占用的空间。 10个数据结构和10个算法10个数据结构：算法、链表、栈、队列、散列表、二叉树、堆、跳表、图、Tire树10个算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配。 现有的数据结构和算法非常多，如下图所示，都学肯定不现实，先学习上述所说的10中数据结构和10中算法。 怎么学 边学边练，适度刷题 多问，多思考，多互动 打怪升级学习法坚持，写博客，做笔记 知识沉淀遇到不懂得，多看同类问题，多看几遍。]]></content>
      <tags>
        <tag>极客时间</tag>
        <tag>数据结构与算法</tag>
        <tag>02如何抓住重点，系统高效的学习数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Column-Stores vs. Row-Stores: How Different Are They Really]]></title>
    <url>%2F2019%2F02%2F21%2FColumn-Stores-vs-Row-Stores-How-Different-Are-They-Really%2F</url>
    <content type="text"><![CDATA[概述从论文的标题可以看出这篇论文不是陈述一种新的技术、架构，而更偏议论文一点，它主要的目的在于搞清楚对于分析类的查询为什么Column-Store比Row-Store好那么多？好在哪里？一般认为原因是: 分析类查询往往只查询一个表里面很少的几个字段，Column-Store只需要从磁盘读取用户查询的Column，而Row-Store读取每一条记录的时候你会把所有Column的数据读出来，在IO上Column-Store比Row-Store效率高很多，因此性能更好。 而本文的目的是要告诉你Column-Store在存储格式优势只是一方面，如果没有查询引擎上其它几个优化措施的配合，性能也不会太好的，这篇论文认为Column-Store在查询引擎层有以下几种大的优化手段: 块遍历(Block Iteration) 压缩(Compression) 延迟物化(Late Materialization) Invisible Join 其中前三点是前人就已经总结过的、在现有Column-Store上实现过了的，而最后一点是本论文的创新。下面我们一一看一下这几种优化手段的细节，最后再看看它们优化效果的对比。 四大优化策略详解块遍历块遍历(Block Iteration)是相对于单记录遍历(per-tuple iteration)而言的，其实说白了就是一种批量化的操作。单记录遍历的问题在于对于每个条数据，我们都要从Row数据里面抽取出我们需要的column(针对Row-Store来说)，然后调用相应的函数去处理，函数调用的次数跟数据的条数成是 1:1的，在大数据量的情况下这个开销非常可观。而块遍历，因为是一次性处理多条数据，函数调用次数被降下来，当然可以提高性能。 这种提高性能的方法在Row-Store里面是case-by-case实现的(不是一种共识), 而对于Column-Store来说已经形成共识，大家都是这么做的。而如果column的值是字节意义上等宽的，比如数字类型，Column-Store可以进一步提高性能，因为查询引擎要从一个Block里取出其中一个值进行处理的时候直接用数组下标就可以获取数据，进一步提升性能。而且以数组的方式对数据进行访问使得我们可以利用现代CPU的一些优化措施比如SIMD(Single Instruction Multiple Data)来实现并行化执行，进一步提高性能。 压缩压缩这种优化的方法对于Column-Store比对Row-Store更有效，原因很简单，我个人对压缩的理解是: 对数据进行更高效的编码, 使得我们可以以更少的空间表达相同的意思。而能够进行更高效编码的前提是这个数据肯定要有某种规律，比如有很多数据一样，或者数据的类型一样。而Column-Store正好符合这个特点，因为Column-Store是把同一个Column – 也就是相同类型的数据保存在一起，当然比Row-Store把一条记录里面不同类型的字段值保存在一起更有规律，更有规律意味着可以有更高的压缩比。 但是为什么压缩就能带来查询的高效呢？压缩首先带来的硬盘上存储空间的降低，但是硬盘又不值钱。它的真正意义在于：数据占用的硬盘空间越小，查询引擎花在IO上的时间就越少(不管是从硬盘里面把数据读入内存，还是从内存里面把数据读入CPU)。同时要记住的是数据压缩之后，要进行处理很多时候要需要解压缩(不管是Column-Store还是Row-Store), 因此压缩比不是我们追求的唯一，因为后面解压也需要花时间，因此一般会在压缩比和解压速度之间做一个权衡。 高压缩比的典型如Lempel-Ziv, Huffman, 解压快的典型如: Snappy, Lz2前面提到解压缩，有的场景下解压缩这个步骤可以彻底避免掉，比如对于采用Run-Length编码方式进行压缩的数据，我们可以直接在数据压缩的格式上进行一些计算: Run-Length的大概意思是这样的, 对于一个数字序列: 1 1 1 1 2 2 2, 它可以表达成 1x4, 2x3这样不管进行 count (4 + 3), sum (1 x 4 + 2 x 3) 等等都可以不对数据进行解压直接计算，而且因为扫描的数据比未压缩的要少，从而可以进一步的提升性能。文中还提到对于Column-Store应用压缩这种优化最好的场景是当数据是经过排序的，道理很简单，因为如果没有经过排序，那么数据就没那么“有规律”，也就达不到最好的压缩比。 延迟物化要理解延迟物化(Late Materialization), 首先解释一下什么是物化：为了能够把底层存储格式(面向Column的), 跟用户查询表达的意思(Row)对应上，在一个查询的生命周期的某个时间点，一定要把数据转换成Row的形式，这在Column-Store里面被称为物化(Materization)。 理解了物化的概念之后，延迟物化就很好理解了，意思是把这个物化的时机尽量的拖延到整个查询生命周期的后期。延迟物化意味着在查询执行的前一段时间内，查询执行的模型不是关系代数，而是基于Column的(我也不知道怎么更好的表达这种“模型”，如果有知道的朋友欢迎告知)。下面看个例子, 比如下面的查询:1234SELECT nameFROM personWHERE id &gt; 10 and age &gt; 20 一般(Naive)的做法是从文件系统读出三列的数据，马上物化成一行行的person数据，然后应用两个过滤条件: id &gt; 10 和 age &gt; 20 , 过滤完了之后从数据里面抽出 name 字段，作为最后的结果，大致转换过程如下图: 而延迟物化的做法则会先不拼出行式数据，直接在Column数据上分别应用两个过滤条件，从而得到两个满足过滤条件的bitmap, 然后再把两个bitmap做位与(bitwise AND)的操作得到同时满足两个条件的所有的bitmap，因为最后用户需要的只是 name 字段而已，因此下一步我们拿着这些 position 对 name 字段的数据进行过滤就得到了最终的结果。如下图: 发现没有？整个过程中我们压根没有进行物化操作，从而可以大大的提高效率。 总结起来延迟物化有四个方面的好处: 关系代数里面的 selection 和 aggregation 都会产生一些不必要的物化操作，从一种形式的tuple, 变成另外一种形式的tuple。如果对物化进行延迟的话，可以减少物化的开销(因为要物化的字段少了)，甚至直接不需要物化了。 如果Column数据是以面向Column的压缩方式进行压缩的话，如果要进行物化那么就必须先解压，而这就使得我们之前提到的可以直接在压缩数据上进行查询的优势荡然无存了。 操作系统Cache的利用率会更好一点，因为不会被同一个Row里面其它无关的属性污染Cache Line。 块遍历 的优化手段对Column类型的数据效果更好，因为数据以Column形式保存在一起，数据是定长的可能性更大，而如果Row形式保存在一起数据是定长的可能性非常小(因为你一行数据里面只要有一个是非定长的，比如VARCHAR，那么整行数据都是非定长的)。 Invisible Join最后本文提出了一个具有创新性的性能优化措施: Invisible Join。Invisible Join 针对的场景是数仓里面的星型模型(Star Schema), 如果用户查询符合下面的模式就可以应用Invisible Join: 利用事实表的里面的外键跟维度表的主键进行JOIN的查询的, 最后select出一些column返回给用户。所谓的星型模型指的是一个事实表(fact table), 周围通过外键关联一堆维度表(dimension table)的这么一种模型， 为了介绍Invisible Join的思路，我们要先介绍一下两种传统的方案，通过对比我们才能看Invisible Join方案的优点。 传统方案一: 按Selectivity依次JOIN传统方案一最简单，按照 Selectivity 从大到小对表进行JOIN: 传统方案二: 延迟物化方案二比较有意思, 它应用了延迟物化的策略，它先不进行JOIN，而是先在维度表上对数据进行过滤，拿到对应表的 POSITION, 然后把表的主键跟事实表的外键进行JOIN，这样我们就可以拿到两类POSITION: 事实表的POSITION和维度表的POSITION, 然后我们通过这些POSITION把数据提取出来就完成了一次JOIN, 重复以上的操作我们就可以完成整个查询。 Invisible Join详解上面两种方案都各有各的缺点: 传统方案一因为一开始就做了JOIN，享受不了延迟物化的各种优化。 传统方案二在提取最终值的时候对很多Column的提取是乱序的操作，而乱序的提取性能是很差的(随机IO)。 下面正式介绍一下我们的主角: Invisible JOIN Invisible JOIN其实是对传统方案二的一种优化，传统方案二的精髓在于延迟物化，但是受制于大量的值的提取还是乱序的，性能还是不是最好。Invisible JOIN把能这种乱序的值提取进一步的减少, 它的具体思路如下: 把所有过滤条件应用到每个维度表上，得到符合条件的维度表的主键(同时也是事实表的外键)。 遍历事实表，并且查询第一步得到的所有外键的值，得到符合条件的bitmap(s), 这里会有多个bitmap，因为维度表可能有多个。 对第二步的多个bitmap做AND操作，得到最终事实表里面符合过滤条件的bitmap。 根据第三步的事实表的bitmap以及第一步的符合条件的维度表的主键值，组装出最终的返回值。 如果只是这样的话Invisible JOIN可能比上面的第二种方案好不了多少，论文认为在很多时间维度表里面符合过滤条件的数据往往是连续的，连续的好处在于，它能把lookup join变成一个值的范围检查，范围检查比lookup join要快，原因很简单，范围检查只需要所算数运算就好了，不需要做lookup，因此可能大幅度的提高性能。 性能对比从论文提供的性能对比数字来看，这几大优化策略里面延迟物化的效果最好，能够提升性能3倍以上；压缩的优化效果次之: 两倍以上；Invisible JOIN 再次之：50% 到 70%；块遍历则能性能 5% 到 50%。 而如果把这些优化手段都去掉，Column-Store的性能跟一个普通的Row-Store就没什么区别了。 总结读这篇论文最大的收获是首先知道了到底哪些因素促使Column-Store对分析类查询性能可以大幅优于Row-Store: 延迟物化、压缩、Invisible Join以及块遍历。 特别佩服这篇论文的是很多人稍微想一下就会觉得Column-Store在分析类场景下性能优于Row-Store是天经地义的: 更少的IO，但是这篇论文详细的对各种场景进行了测试、论证，同时对Row-Store应用类似的性能优化的手段再进行测试、对比，掰开了揉碎了的分析。最终告诉我们: Column-Store的优点不止在于它的存储格式，查询引擎层的各种优化也同样关键，而由于Row-Store本身存储格式的限制，即使在Row-Store上使用这些优化，效果也不好。 转载链接Column-Stores vs. Row-Stores 读后感]]></content>
      <tags>
        <tag>转载</tag>
        <tag>列式存储</tag>
        <tag>行式存储</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux/UNIX 系统中的find命令]]></title>
    <url>%2F2019%2F02%2F18%2FLinux-UNIX-%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84find%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[The Linux find command comes in handy when looking for files directly from the command line. The find command is given search criteria such as permissions, ownership, modification, size, time, and date among others to locate the file or directory in question. The find command is available in all Linux distros by default, therefore, there’s no need of installing special packages to use it. Due to its significance, the find command is an essential command to learn if you want to know more about the command line navigations on any Linux distribution. We will highlight some of the find command examples and explain the various options that you can use. Syntax 1$ find location comparison-criteria search-term Listing files in the current directoryTo list all files in a directory including files inside folders, run the command below.1$ find . Sample output Searching files within a specified directoryIf you want to search all files in a given directory, use the find command as follows 1$ find directory_name For example, to search for all files in /boot execute the command1$ find /boot Sample output Searching files using the filename within a specified directoryIf you want to specify the search criteria using the name of the file in a directory, the syntax will be as follows1$ find directory_name -name &quot;file_name&quot; For example, to search for Apache2 files in /etc directory run1$ find /etc -name &quot;apache2&quot; Output Recursively find all files with a specified file extensionIf you want to search for particular files bearing a specific extension, in a given directory, the syntax will be as follows1$ find directory_name -name &quot;*.extension&quot; For example, to search for all configuration files (.conf) in /etc directory, execute1$ find /etc -name &quot;*.conf&quot; Sample output Limiting depth of searchYou can decide to limit the depth of your file search in directories. For example, if you want to limit your file search to the first level of the directory, the syntax will be 1$ find directory_name -maxdepth 1 -name &quot;*.conf&quot; So, if you want to limit the file search to the first level directory in /etc for files with .conf extension execute:1$ find /etc -maxdepth 1 -name &quot;*.conf&quot; Sample output As seen in the output above, the file search is limited to the /etc directory level. If you want to perform a more intensive search and go deeper within other directories within the /etc directory, increase the maxdepth value. For instance, to search for files with .conf extension up to the 3rd directory run 1$ find /etc -maxdepth 3 -name &quot;*.conf&quot; Sample outputAs seen from the above output, the search goes up to the 2nd and 3rd directories. Invert search resultsYou can also search for files that do not meet given criteria with the find command. This mode is helpful when you want to eliminate known files from the search pattern. To do this, use the -not -name attribute as shown 1$ find /etc -maxdepth 1 -not -name &quot;*.conf&quot; Sample outputThe above output prints all the files that do not have the .conf fie extension. Using find with OR operatorYou can choose to combine search results with find by using the OR operator which is symbolized by -o flag shown in the example below 1$ find /etc -maxdepth 3 -name &quot;cron&quot; -o -name &quot;ssh&quot; The above command searches for files bearing the name ssh OR cron in the /etc directory Searching for files only or directories onlyIf you want to search for files only, use the - type f attribute as shown in the example below1$ find /etc -type f -name &quot;ssh&quot; Sample output If you want to search for directories only, use the - type d attribute as shown in the example below. 1$ find /etc -type d -name &quot;ssh&quot; Sample output Searching for files owned by a particular userTo search for files owned by a particular user in a specific directory, use the syntax:1$ find /path -user username For instance, to find files owned by user james in /home directory run the command below1$ find /home -user james Sample output Searching for files with certain file permissionsTo search for files with specific file permissions, use the syntax below 1$ find /directory_name -type f -perm value For example, to search for files with permissions 755 in /etc directory, run:1$ find /etc -type f -perm 755 Sample output Searching for files with certain files sizes or a range of filesLinux find command also offers users a chance to search files according to their file sizes. Search files of N sizeFor example, to search for files which are 10kb run:1$ find /etc -type f -size 10k Sample output To search files greater than 10kb run 1find /etc -type f -size +10k Sample outputTo search files less than 10kb run1find /etc -type f -size -10k Sample output SummaryThat was a quick overview of the Linux find command examples. As already shown searching files and directories on the command line is very easy. Knowing how the command operates is an essential tool for all system administrators.Feel free to try out the above find command examples and let us know how it went. 原作者链接Find Command in Linux/UNIX]]></content>
      <tags>
        <tag>find命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的定时任务-ScheduledExecutorService的坑]]></title>
    <url>%2F2019%2F01%2F17%2FJava%E4%B8%AD%E7%9A%84%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1-ScheduledExecutorService%E7%9A%84%E5%9D%91%2F</url>
    <content type="text"><![CDATA[在做码农的日子里面，如果不跟线程打交道，那真的连入门都算不上了，如果你还仅仅是简单的new Thread，那么你就是跟我一样的小白了；怎么也得弄点高大上的线程池吧，用线程池肯定就少不了java concurrent包中的ExecutorService了；这里面的学问还是挺大的。以后有机会慢慢品读；在你的任务中，肯定也有定时任务的吧，如果你的定时还用Timer的化，那么你真的就跟我一样out了，具体原因请google下；说到Java的定时任务，肯定是非ScheduledExecutorService莫属了。这个用法是相当简单的。。。 正常运行12345678910111213141516171819202122package qhl.silver.ScheduledExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;public class BadAssTask implements Runnable &#123; public void run() &#123; System.out.println(&quot;开始做任务了，好开心。。。。&quot;); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;没有有bug(error/exception)出现，我可以很开心的继续执行了 &quot;); &#125; public static void main(String[] args) &#123; Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(new BadAssTask(), 1, 1, TimeUnit.SECONDS); &#125;&#125; 上述任务会每秒钟定时执行，输出如下所示： 遇到异常情况但是，假如你的任务有问题呢？有bug，有异常呢？请看如下测试代码1234567891011121314151617181920212223package qhl.silver.ScheduledExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.TimeUnit;public class BadAssTask implements Runnable &#123; public void run() &#123; System.out.println(&quot;开始做任务了，好开心。。。。&quot;); try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(&quot;如果有bug(error/exception)出现话，这个定时任务还能不能继续执行呢? &quot;); throw new RuntimeException(&quot;卧槽。。出现bug了，你竟然不catch！！！！!&quot;); &#125; public static void main(String[] args) &#123; Executors.newSingleThreadScheduledExecutor().scheduleAtFixedRate(new BadAssTask(), 1, 1, TimeUnit.SECONDS); &#125;&#125; 以上测试代码就会出现如下输出，也就是仅仅运行了一次就挂了。这不符合预期啊！ 问题原因查找打开jdk的官方文档!, 可以看到如下解释以上标注内容翻译为人话就是：如果执行过程中遇到了问题（error/exception）,那么后面的定时任务也就不会继续执行了这显然不符合预期啊。这简直就是fuck egg的事情，哪有程序不会遇到点bug，遇到点异常呢？当时执行遇到异常，也许以后就好了呢？毕竟编程是一门神学，你不能因为一次异常，而放弃执行之后的定时任务啊！！！！ 解决办法那怎么解决这个问题呢。很显然，既然ScheduledExecutorService有可能在运行任务的过程中。任务（继承Runnable接口的）有可能抛出异常，那就catch这个异常呗。 方法1在run方法的外部，使用try catch语句catch可能的异常，仅仅catch 异常（Exception）还是不够的，有可能还有error，所以都需要catch的，代码如下1234567try &#123; throw new RuntimeException(&quot;卧槽。。出现bug了，你竟然不catch！！！！!&quot;); &#125; catch (Error e) &#123; System.out.println(&quot;error occurred, e=&quot; + e); &#125; catch (Exception e) &#123; System.out.println(&quot;exception occurred, e=&quot; + e); &#125; 当然了，由于Error和Exception都继承了Throwable，所以，只需要catch Throwable一个就可以了，所以以上代码可以简化为如下形式：12345try &#123; throw new RuntimeException(&quot;卧槽。。出现bug了，你竟然不catch！！！！!&quot;);&#125; catch (Throwable t) &#123; System.out.println(&quot;some thing wrong happened, error=&quot; + t);&#125; 但是在每个run方法中都要try catch，也是很痛苦的事情，这得多了多少代码啊！！！此时，方法2就要现身了！！！ 方法2编写一个wrap类，封装这个ScheduledThreadPoolExecutor，在这个类里面进行try/catch。这样外部就不用try/catch了；当然你也可以在这个类里面把异常继续向上抛出，如果选择继续把异常向上抛出，那么外部必须选择try/catch此异常，否则，还是会造成后续定时任务不会执行 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package qhl.silver.ScheduledExecutorService;import java.util.concurrent.ScheduledFuture;import java.util.concurrent.ScheduledThreadPoolExecutor;import java.util.concurrent.TimeUnit;public class WrappingScheduledExecutor extends ScheduledThreadPoolExecutor &#123; public WrappingScheduledExecutor(int corePoolSize) &#123; super(corePoolSize); &#125; @Override public ScheduledFuture scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit) &#123; return super.scheduleAtFixedRate(wrapRunnable(command), initialDelay, period, unit); &#125; @Override public ScheduledFuture scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit) &#123; return super.scheduleWithFixedDelay(wrapRunnable(command), initialDelay, delay, unit); &#125; private Runnable wrapRunnable(Runnable command) &#123; return new LogOnExceptionRunnable(command); &#125; private class LogOnExceptionRunnable implements Runnable &#123; private Runnable theRunnable; public LogOnExceptionRunnable(Runnable theRunnable) &#123; super(); this.theRunnable = theRunnable; &#125; public void run() &#123; try &#123; theRunnable.run(); &#125; catch (Throwable t) &#123; System.err.println(&quot;error in executing: &quot; + theRunnable + &quot;, the error = &quot; + t); /** *重要，如果你选择继续向上抛出异常，则在外部必须能够catch住这个异常，否则还是会造成后续任务不会执行 * IMPORTANT: if you thrown exception. then in the out, you have to try/catch the exception, * otherwise, the executor will stop */ // throw new RuntimeException(e); &#125; &#125; &#125; public static void main(String[] args) &#123; new WrappingScheduledExecutor(1).scheduleAtFixedRate(new BadAssTask(), 1, 1, TimeUnit.SECONDS); &#125;&#125; 问题延伸-看看知名开源软件怎么玩的google guava 中的WrappingExecutorService，WrappingScheduledExecutorService也是简单的封装了，而且是abstract类，用户还无法直接使用，必须要有一个实现类implement这个类方可使用。思想与上述解决方案2一致。 画外音线程池中的ExecutorService也是有这个问题的。请看如下代码,就如同注释中说的，如果不try catch，则如果遇到问题就不会继续执行了。12345678910111213141516171819202122232425262728293031package qhl.silver.ScheduledExecutorService;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class TestExecutor &#123; private ExecutorService executeProcessor; TestExecutor() &#123; executeProcessor = Executors.newSingleThreadScheduledExecutor(); executeProcessor.execute(this::taskRun); &#125; public void taskRun() &#123; while (true) &#123; System.out.println(&quot;正常执行&quot;); try &#123; Thread.sleep(1 * 1000); throw new RuntimeException(&quot;出错了，但是被我catch住之后，还是会继续执行的!!!&quot;); &#125; catch (Throwable e) &#123; System.out.println(&quot;error occurred = &quot; + e); &#125; System.out.println(&quot;由于下面的错误没有被catch，所以这个任务就不会被继续执行了&quot;); // throw new RuntimeException(&quot;出错了，没有被catch，所以就不会继续执行了!!!&quot;); &#125; &#125; public static void main(String args[]) &#123; new TestExecutor(); &#125;&#125; 结论凡事使用ExecutorService的，都要try catch 参考文献ScheduledExecutorService Exception handlingMother F**k the ScheduledExecutorService!]]></content>
      <tags>
        <tag>JAVA 线程池，定时任务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang-goland导入k8s源码]]></title>
    <url>%2F2018%2F10%2F15%2FGolang-goland%E5%AF%BC%E5%85%A5k8s%E6%BA%90%E7%A0%81%2F</url>
    <content type="text"><![CDATA[前言最近在调研 Google kubernetes 开源的容器编排平台，刚好也在学习 Go 语言，所以想看看 Google 这样的大厂是怎么撸 Go 语言的，本文简单介绍如何下载 k8s 源代码，导入 Idea GoLand（对，我是搞 Java的～），对于这么庞大的项目，没有 IDE 看起来还是很费劲的，当然牛人除外。 下载源代码这个不用说了，地球村的人应该都知道 1234mkdir -p /opt/kubernetes-src/src/k8s.iocd /opt/kubernetes-src/src/k8s.io/git clone https://github.com/kubernetes/kubernetes.gitgit checkout release-1.12（checkout 最新版本的分支即可） 因为等下在 GoLand 里面会配置 Project GOPATH（每个工程私有的 GOPATH）为 /opt/kubernetes-src，所以在 /opt/kubernetes-src 下建了 src/k8s.io 目录，至于为什么目录名叫 k8s.io，这个翻翻代码中的 import 就明白了，如果你不想导入代码后出现各种找不到导入包（符号）的化。 导入 GoLand在 GoLand 之前一直用的 Intellij + Go 插件，GoLand 出了之后立马下载下来体验，感觉还是不错的 打开 GoLand选择 New Project将目标文件夹指向 /opt/kubernetes-src确认之后会提示文件夹不为空，是否继续，点击确定就行慢慢等待 IDE 完成对源代码的索引。 注意：一定要设置导入源码的 Project GOPATH为当前路径/opt/kubernetes-src/src。路径为preferences-&gt;Go-&gt;GOPATH。 总结本文介绍了如何将 k8s 源码导入到 GoLand，为后续深入学习源码做好准备]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>goland</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes Controllers 窥探]]></title>
    <url>%2F2018%2F10%2F12%2FKubernetes%20Controllers%20%E7%AA%A5%E6%8E%A2%2F</url>
    <content type="text"></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>controller manager</tag>
        <tag>api-server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[存储计算分离]]></title>
    <url>%2F2018%2F08%2F08%2F%E5%AD%98%E5%82%A8%E8%AE%A1%E7%AE%97%E5%88%86%E7%A6%BB%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[一句话：学术界往往要比工业界提前，就像google永远比其他公司提前一样。 在本人的硕士毕设中，标题是“面向存储计算分离的云化大规模数据库缓存系统”，故整理如下内容。 其实早在2009年，就有人提出针对Hadoop提出了SuperDataNode的概念，提出了计算分离的想法。具体请参考论文。往往学术界要比工业界提前好几年，等到真正技术成熟的时候，就是工业界开始尝试的时候。最近炒的火热的存储计算分离也开始在工业界中使用了。这篇论文提出9年后的今天，hadoop 3.0 正式提出了存储计算分离的概念。Hadoop 3.0 and the Decoupling of Hadoop Compute from Storage]]></content>
      <categories>
        <category>存储计算分离</category>
      </categories>
      <tags>
        <tag>存储计算分离</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Alluxio技术内幕:高性能的异步读缓存]]></title>
    <url>%2F2018%2F08%2F08%2FAlluxio%E6%8A%80%E6%9C%AF%E5%86%85%E5%B9%95-%E9%AB%98%E6%80%A7%E8%83%BD%E7%9A%84%E5%BC%82%E6%AD%A5%E8%AF%BB%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[概览Alluxio服务通过连接底层持久化存储并按需将数据缓存至内存中，为不同的应用作业提供了一个可以高速并发访问的数据层。在Alluxio中，每一个文件根据其大小在逻辑上被划分成不同的“块”（默认512MB）。块在Alluxio中是缓存的最小单位。当Alluxio客户端用从Alluxio服务中读取文件时，如果被请求的数据块并未缓存在Alluxio中，则会触发缓存操作。将数据缓存在Alluxio空间后可以极大地提高后续分析作业的性能。 在Alluxio v1.7之前，当客户端向Alluixo请求读的数据没有命中缓存时，Alluxio客户端会默认地在读数据的时候同步地执行缓存操作。但当应用程序第一次读取一个文件并且仅需要读取该文件的一小部分的时候，这种同步缓存的操作可能对应用程序的性能造成损失。具体地说，在缓存没有命中的情况下，Alluxio客户端会（通过worker）去底层存储系统读取一个完整的数据块，将其缓存至Alluxio，同时把所需数据返回给应用程序。例如，对于很多SQL类型的作业，客户端常常只需要读取Parquet文件的表尾（不超过数MB大小），但第一次请求Alluxio时，Alluxio会尝试读取并缓存一个完整的数据块，以便于响应未来的请求。 Alluxio v1.7开始实现了优化的异步数据缓存操作。客户端不再同时负责数据的读取和缓存，而是将缓存操作交给worker节点与客户端的读操作异步进行。这一改进极大的简化了应用程序客户端在数据读取时的角色，并且由于客户端不等待缓存操作完成，可以显著提高某些类型作业的性能。本文解释了Alluxio v1.7及更高版本中的数据缓存操作，并对如何最大化地利用该功能提供了建议。本文涵盖的主题包括： 异步缓存策略 调整和配置异步缓存 异步缓存的优点 异步缓存策略异步缓存将缓存操作的开销由客户端转移到worker。客户端读数据的同时，缓存数据块的任务被交给worker在后台异步来处理（除非用户指定读取类型为”NO_CACHE”）。不论是读取完整或部分数据块，缓存操作对客户端性能均没有额外的影响，用户也不再需要像在Alluxio 1.7以前那样设置参数“alluxio.user.file.cache.partially.read.block”来启动或者关闭对只读了一部分的数据块的缓存。 Worker内部也利用帮客户端读取底层存储的过程中获取的数据做出了优化：如果客户端使用读取类型”CACHE”，从头到尾顺序从底层存储读取一个完整数据块，那么worker会在帮助客户端读底层存储系统的过程中积累了完整的数据。因此在客户端读顺序取完一个数据块以后，worker就可以直接缓存该数据了。 图1显示了读取数据块过程中的这一同步优化的部分。客户端只读取所需数据（步骤1，2，4，5），而worker在读取的时候顺便缓存(步骤3)。 当客户端读取完整的块，则将整个数据块会被缓存（步骤6）。 但如果worker发现，客户端只是读取数据块的一部分，或者正在以非顺序的方式读取数据块内部数据，那么worker便会放弃在读的时候顺便缓存。而客户端则会在读取完成后向worker节点发送异步缓存命令并继续。之后worker节点再从底层存储获取完整的块。 如图2所示，在客户端和worker节点间的异步缓存请求使用轻量级RPC通信（步骤1）。在worker确认请求后（步骤2），客户端可以立即继续运行，而步骤3和4可以在worker后台异步进行。 调整和配置异步缓存Alluxio worker节点在后台并行执行异步缓存，同时也服务于来自客户端的同步读取请求。每个worker节点有一个线程池，其大小由参数“alluxio.worker.network.netty.async.cache.manager.threads.max”指定。 参数默认值为8，这意味着worker最多可以使用8个核从其他worker或底层存储系统下载块，并在本地缓存以供将来使用。可以调高此值以加快后台异步缓存速度，但CPU使用率会增加。 降低该值则会减慢异步缓存速度，同时也释放了CPU资源。 异步缓存的优点这里我们提供一个简单的例子来展示异步缓存的好处：用户需要先从作为底层存储系统的S3中读取某一文件的一小部分，但之后还会有更多的读取请求，因此还是打算缓存整个文件。(1) 使用异步缓存之后，从S3中读取文件的前5KB只需要大约几秒钟，客户端从而可以在下载完5KB后立即返回。(2) 而对于Alluxio 1.7之前版本中，由于要缓存整个块，第一次读取这5KB数据将花费大约几分钟（速度将取决于网络连接条件）。在这两种情况下，数据块在初始请求后的几分钟内将完全缓存到Alluxio。但使用异步缓存后，客户端在初始读取所需的数据后可以继续执行，同时可以在worker后台缓存完整数据块。 异步缓存极大地提高了工作负载冷读取的性能，它们不需要完整顺序地读取数据块，比如在Presto或SparkSQL等计算框架上运行SQL工作负载。使用异步缓存，第一次查询将和直接从连接存储读取数据花费相同的时间量，并且随着数据异步缓存到Alluxio中，集群的整体性能将逐渐提高。 未来工作管理Alluxio存储是Alluxio系统的一个重要部分。 在将来的版本中，异步缓存机制将得到进一步改进，包括： 删除被动缓存的概念（ALLUXIO-3136） 更细粒度地控制资源使用情况，如网络带宽，异步缓存（ALLUXIO-3137） 改进worker间的数据传输机制（ALLUXIO-3138） 优化异步缓存数据读取（ALLUXIO-3141）]]></content>
      <categories>
        <category>分布式存储</category>
      </categories>
      <tags>
        <tag>Alluxio</tag>
        <tag>缓存</tag>
        <tag>异步</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[预取技术-prefetching]]></title>
    <url>%2F2018%2F04%2F27%2F%E9%A2%84%E5%8F%96%E6%8A%80%E6%9C%AF-prefetching%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[并行编程有那么难吗？-计数(Counting)]]></title>
    <url>%2F2018%2F04%2F24%2F%E5%B9%B6%E8%A1%8C%E7%BC%96%E7%A8%8B%E6%9C%89%E9%82%A3%E4%B9%88%E9%9A%BE%E5%90%97%EF%BC%9F-%E8%AE%A1%E6%95%B0-Counting%2F</url>
    <content type="text"><![CDATA[计数可以说是比较常见也是并发的书本中最开始讲述的例子，本章将会讲述简单的计数需要面临的问题。 简单的计数操作非原子计数操作123456789101112131415161718192021222324252627282930313233#include &lt;iostream&gt;#include &lt;pthread.h&gt;int64_t counter = 0;int nTimes = 100000;void *inc_count(void *)&#123; for(int i=0; i&lt;nTimes; i++)&#123; counter++; &#125;&#125;int64_t read_count(void)&#123; return counter;&#125;int main(int argc, char* argv[])&#123; int nThreads = 2; pthread_t *tid =static_cast&lt;pthread_t*&gt; (malloc(nThreads*sizeof(pthread_t))); /* create number of nThreads threads*/ for(int j= 0; j &lt; nThreads; j++)&#123; pthread_create(&amp;tid[j], NULL, inc_count, NULL); &#125; /* wait for the threads to finish worrk*/ for(int j= 0; j &lt; nThreads; j++)&#123; pthread_join(tid[j], NULL); &#125; int res = read_count(); std::cout &lt;&lt; &quot;the final counter value = &quot; &lt;&lt; res &lt;&lt; std::endl;&#125; 以上是一个多线程并发程序，对一个变量进行++操作，可以看出，通过运行结果可以看出，其最后的输出结果不等于nThreads*nTimes。这就可以看出在非原子操作的情况下。这种并发是无法满足精确地++操作的。 原子操作的计数1234567891011121314151617181920212223242526272829303132333435#include &lt;iostream&gt;#include &lt;pthread.h&gt;#include &lt;atomic&gt;#include &quot;linux/atomic.h&quot;using namespace std;atomic_t counter = new ATOMIC_INIT(0);int nTimes = 100000;void *inc_count(void *)&#123; for(int i=0; i&lt;nTimes; i++)&#123; atomic_inc(&amp;counter); &#125;&#125;int64_t read_count(void)&#123; return atomic_read(&amp;counter);&#125;int main(int argc, char* argv[])&#123; int nThreads = 2; pthread_t *tid =static_cast&lt;pthread_t*&gt; (malloc(nThreads*sizeof(pthread_t))); for(int j= 0; j &lt; nThreads; j++)&#123; pthread_create(&amp;tid[j], NULL, inc_count, NULL); &#125; for(int j= 0; j &lt; nThreads; j++)&#123; pthread_join(tid[j], NULL); &#125; int res = read_count(); std::cout &lt;&lt; &quot;the final counter value = &quot; &lt;&lt; res &lt;&lt; std::endl;&#125; 以上是使用原子操作的代码。但是很遗憾。一直没有运行起来，报错就说 error: ‘atomic_t’ does not name a type 可以看出来。理想状况下，延迟增加图应该是靠近x轴的曲线，但是实际情况是当CPU核数增加，其延迟也在增加，并没有起到并发的作用。这就给并发带来了挑战！ 可以看一个对全局变量进行原子操作的例子，如图2所示，In order for each CPU to get a chance to increment a given global variable, the cache line containing that variable must circulate among all the CPUs, as shown by the red arrows. Such circulation will take significant time, resulting in the poor performance seen in Figure 5.1(图1)。]]></content>
      <categories>
        <category>并行编程</category>
      </categories>
      <tags>
        <tag>并行</tag>
        <tag>计数</tag>
        <tag>count</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[楚门的世界]]></title>
    <url>%2F2018%2F04%2F22%2F%E6%A5%9A%E9%97%A8%E7%9A%84%E4%B8%96%E7%95%8C%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[git 命令相关]]></title>
    <url>%2F2018%2F04%2F20%2Fgit-%E5%91%BD%E4%BB%A4%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[一般的github协作流程： fork一个自己的项目 master分支保持不变，去follow最新变更 每次开发新功能的时候创建新分支branch1 在新branch1分支commit 代码 在push branch1的代码之前回到master分支pull最新的远端代码 回到branch1运行rebase master命令，保证自己的所有代码提交都在master之后最新 push branch1到自己的github项目库 到github界面上提交pull request选择远端master分支 分支(branch)操作相关命令 查看本地分支：$ git branch 查看远程分支：$ git branch -r 创建本地分支：$ git branch [name] —-注意新分支创建后不会自动切换为当前分支切换分支：$ git checkout [name] 创建新分支并立即切换到新分支：$ git checkout -b [name] 删除分支：$ git branch -d [name] —- -d选项只能删除已经参与了合并的分支，对于未有合并的分支是无法删除的。如果想强制删除一个分支，可以使用-D选项 合并分支：$ git merge [name] —-将名称为[name]的分支与当前分支合并 git pull push 相关命令 git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt; git push &lt;远程主机名&gt; &lt;本地分支名&gt;:&lt;远程分支名&gt;]]></content>
      <categories>
        <category>常用命令</category>
      </categories>
      <tags>
        <tag>协作</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BDTC2017总结]]></title>
    <url>%2F2018%2F04%2F20%2FBDTC2017%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[罗韩梅-资源调度项目背景 目标是整合集群硬件资源, 对外提供统一的标准接口, 海量任务的管理以及资源调配. 自研 vs 开源. 开源主要是关注Yarn, Mesos, swarm, kubernetes. 使用开源, 减少用户迁移的成本. 规模: 1.7亿container, 10000+资源, 微服务 介绍总体架构, 使用在线和离线混部, 利用不同任务的资源使用特点. 研发介绍kernel方面 增加了资源隔离的唯独, 包括了CPU, 内存, 磁盘容量, 网络出带宽, 网络入带宽, Disk IO, Buffer IO的控制. 首先对于网络IO的控制, 主要是多个进程竞争网络带宽的时候, 需要提供带宽和时延的保证. 设计的时候, 希望能够设置弹性目标, 充分利用资源, 又保证配额不被占用, 并且支持优先级. 具体实现: ECN标记, 滑动窗口, 令牌桶…(tc+cgroups) 对于Disk IO的控制: cgroup 通过识别pid控制磁盘IO. 但在buffer io中没有pid. 这方面做了修改. 并且解决了当前cgroups对io控制模式是hard(不会超过配额)的问题.以及解决io weight 通过cfq机制分割导致的数据波动我难题. docker方面 bug fix 热升级 网络插件 弹性内存控制 RBD插件 ceph使用方便, 非hadoop 生态的东西, 用ceph比较多. k8s quota APP引入Tapp 网络模式 网络模式多种支持, 包括NAT, Host, Floating IP等. 磁盘管理 GPU应用 registry: 做了相关的优化(P2P的分发等) 参考文献原文slice 徐东-MaxComputemax_compute 结构介绍总体结构, metadata存储统计等信息. 底层使用分布式文件系统, kv等作为存储. 任务执行使用fuxi等调度系统. 上层提供了计算接口, 包括SQL, 图等. SQL的软件层次sql的三个层次, compiler(语法解析), optimizer(基于代价优化)以及runtime(用于执行具体的执行plan) (参考 llvm, hive) 数据分布 做数据分布, 一方面是因为数据量的问题, 一方面是为了并行处理 简单一致性模型, 认为多个分片的数据完全独立, 不考虑使用多个分片之间的关系. 数据分布的方式有5种: hash, 排序按range, any(任意分), broadcast, singleton(类型spark中的collect, 集中到单机处理) 多种数据分片方式之间可以通过一定的关系进行转化 可以利用数据分布, 来对sql做查询优化, 举例来说: 这里, 下层先做table scan, 然后提供结构做join, 然后上层做aggregate. 如果使用的是sort merge join算法, 那么下层的数据就是排序好的, 在做aggregate的时候, 如果能够知道数据已经排序好, 就不用继续调用排序操作了. 这就是一个已知数据分布的优化方法. 这个优化要求各层操作之间能够互相传递消息. 所以进行分布式计算过程中, 每个操作子可以选择算法, 可以选择数据分布的方式, 这些选择, 可以结合cost的预测, 做cost-based的优化 ###分布特性搜集 做查询优化, 需要有统计信息, 需要数据分布的信息, 这些信息有两个来源: 用户指定(也就是建表的时候, 通过sql语句指定分布方式) 信息传递, 比如用了sort merge join, 后续可以记住这个sort的特点, 用起来 参考文献slice 链接 曹龙-Hbase阿里云三组件使用 组件 内部规模 公有云产品 主要功能 ODPS 7w MaxCompute 离线计算&amp;机器学习 HBase 1.2+W 云Hbase 在线存储 Flink 数千 StreamCompute 实时计算 其中Hbase有几百个集群, 从4太到2000台都有, 数据量从几百G到10P. Hbase使用场景和各个系统搭配使用, 不同的场景有各自的需求. Hbase部署模式包括了线下物理机部署, 以及基于云的部署. 需要考虑售出价格机器使用率等因素. 最佳组合下面的图展示了不同组件如何搭配, 以及数据流动. 常见的模型有 kafka =&gt; 流计算做实时ETL或者离线计算做ETL=&gt;存储结果到HBASE spark连接Hbase&amp;Phoenix, 做HTAP, 进行一些spark方面的优化如谓词下推 Hbase 给MR提供数据. 或者通过消息中间件, 进一步导入ODPS&amp;ES, 进行计算, 报表生成. 真实案例 车联网公司, 上传数据, 通过流计算做数据清洗, 然后导入Hbase, 提供给spark做分析. 数据的特点: RowKey设计, 每辆车10s上传1K数据, 1年数据量3P, 100W台车 安骑士: APP到HBase, 然后计算报表. 数据特点: 200T soul社交: spark streaming 实时写入Hbase, Hbase用主备. 输入推荐结果到客户. 数据特点: 30T, QPS高峰 800w+ 金融公司: 导入Hbase做数据查询. 数据特点: 单表10000亿+. 数据量100T. 有多个字段的二级索引. 内核优化 减少java的gc. 实现了ccsmap. 自己管理写缓存生命周期 jvm申请一块不用归还的内存自己管理 JVM的GC的优化 HDFS的串行pipeline 改为并发Quorum机制, 降低写抖动. 这边优化前后, YGCT 从120ms 降低到5ms. 使用了ZSTD 压缩算法. 使用indexable delta encoding, 添加了认证机制(user/password), 支持混合网访问 使用bloom filter, 提高读性能. 减少写入量,提高写性能. 平台能力 未来 现在是行存,OLAP不好, 增强分析的支持(如kudo,列存储, 索引) hbase+spark htap 计算和存储分离 参考文献原始slice链接 推荐学习资料 金海-内存计算背景以及为什么会出现内存计算 淘宝的例子, 每秒12w到25w笔交易, 数据量大以及实时性的要求, 需要低延迟的系统. 使用内存计算, 相比基于disk的系统, 延迟从秒级到纳秒 内存计算最早80年代就有, 现在兴起原因: 64位系统, 以及1T的内存的机器可以搭建了. 并且内存价格下降(今年变成4倍), 促进内存计算的发展. 内存计算的好处: 比如SAP HANA, 以及内存文件系统为例子说明 内存计算的挑战主要有四点: DRAM介质易失性 DRAM的存储密度低, 要一个T就比较大了 DRAM的功耗高(数据:容量增大的时候,占系统功耗的46%, 其中用于刷新的静态功耗又占DRAM功耗的50%) 内存子系统成本高(??)//查找IBM power 7子系统能耗比例 NVM相关 international technology roadmap for semiconductor 里面给出了集中存储介质的比较. 比如Memristor, PCM, STT-RAM,DRAM,FLASH, 以及HD. 现有的实际产品是Intel的3D xPoint. SCM具有如下的特性: 字节寻址, 持久存储, 写比读高10倍延迟. 读延迟和DRAM接近. 比flash快1000倍, 存储密度以及耐久性都比NAND的flash高1000倍.并且0静态功耗. 使用这种器件, 有可能可以解决内存计算的 易失性, 存储密度低, 功耗高的问题. 很容易构建一台上T内存的机器. SCM的存在, 从计算机的存储结构上来看, 有可能可以替代DISK, 从而改变整个存储层次. 另外一种可能的情况就是, 把PCM放在和DRAM一样的层次上, 出现计算和数据相结合的情况(也就是存储设备有计算能力) 一个使用NVM的内存计算的例子是HP:The Machine. 40个节点共享160T的内存, 是一个内存为中心的计算系统, 在各方面性能测试都很牛x. NVM带来的挑战对编程模型的影响https://www.snia.org/forums/sssi/nvmp 有一个SNLA NVM Programming TWG这样的组织. 出了一个NVM Programming Model 这样的手册. HP 的工程师也做了相关的报告, 在这里 从例子来看, 就是传统的应用IO路径发生了变化, 比如绕过了文件系统层, 或者针对NVM做的文件系统优化. 混合架构带来的几个影响和挑战对体系结构, 操作系统, 编程模型, 数据管理都有影响. 体系结构: 1. 大内存与多和的内存带宽问题(带宽瓶颈, 做内存并行?). 2. 异构存储的管理问题, 比如NVM和DRAM是层次化组织, 还是采用并列的方式组织. 操作系统: 1. 大内存, 页表大, 需要大页. 但是页太大, 有并发锁粒度大的问题. 2. 混合内存的情况下, 系统如何做任务的调度 数据管理: NVM中的数据放置问题,用什么样的结构, 比如KV, 文件系统 编程模型: 不再需要考虑磁盘IO以及数据持久化的问题 另外, 数据非易失, 存在安全性的问题, 有相关的攻击方式. 自己的一些工作.混合内存模拟器 HME: A Lightweight Emulator for Hybrid Memory Memory equalizer for lateral management of heterogeneous memory hardware/software cooperative caching for hybrid dram/nvm memory architectures MALRU: Miss-penalty aware LRU-based cache replacement for hybrid memory systems Lifetime-based memory management for distributed data processing systems mammoth gearing hadoop towards memory-intensive mapreduce applications(这里给出一些常见系统的分类统计) 分别是基于DRAM, NVM, 以及DISK的系统. 认为发展趋势是存算一体, 也就是NVM本身有计算功能. 原始slice链接 舒继武-文件系统背景等 磁盘的局限, 包括能耗和体积. 出现了flahs, 以及SCM. 对DRAM, PCM, NAND Flash, HDD的性能比较 带来的挑战: 1. 存储设备访问变快, 软件的相对开销变大. 2. 现有的软件基于老的硬件做设计, 不能充分发挥当前新硬件的特性: 所以, 对于大数据处理, 以及新硬件这块, 主要有两点变革: 1. 硬件方面存储结构的变革 2. 系统软件的变革. 对于存储结构的变革, 主要是集中flash, pcie, sata接口等. 以及NVM的相关技术指标的介绍. 这些改变了存储层次结构. 对于软件的变革, 主要:1. 硬件变快, 软件相对开销占比变大. 2. 硬件提供了新的接口给软件使用. 相关工作 基于flahs的kv store(WiscKey,NVMKV,FlashKv) 基于flash的文件系统(DFS,F2FS) 基于flahs 的事务管理 分布式闪存 NVM相关研究 大数据,实时要求. 以及大内存, NVM的出现, 开始做内存计算 相关的方向有: NVM的编程模型, NVM的内存空间管理, NVM的文件系统. 分布式存储的研究 基于RMDA而不是以太网做互联. 速度更快, 软件的相对开销占比例更大. 代表的有Octopus 分布式文件系统 相关资料slice链接]]></content>
      <tags>
        <tag>会议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hash join算法总结]]></title>
    <url>%2F2018%2F01%2F04%2Fhash%20join%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[先给出一些hash join比较不错的连接。 hash join wiki; 常见的join算法 hive Hybrid Hybrid Grace Hash Join 一个总结的ppt。 ;]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>hash join</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux C ftruncate函数清空文件注意事项_要使用lseek重置偏移量]]></title>
    <url>%2F2017%2F12%2F19%2FLinux%20C%20ftruncate%E5%87%BD%E6%95%B0%E6%B8%85%E7%A9%BA%E6%96%87%E4%BB%B6%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9_%E8%A6%81%E4%BD%BF%E7%94%A8lseek%E9%87%8D%E7%BD%AE%E5%81%8F%E7%A7%BB%E9%87%8F%2F</url>
    <content type="text"><![CDATA[ftruncate是改变文件长度的一个函数，一般在操作文件的时候用到，例如我之前写的log的时候，由于log是append only的，所以log的record会越老越多，但是某个时间点的时候，需要把文件中的log都读出来，并且汇总成一条log，也就是log的compat，这个时候，当把所有的log读出来的时候，就需要把以前的log全部删掉，并写入新compact的log。所以此时就会用到ftruncate。但是千万要注意的事情就是，用这个函数并不会真的把文件的offset置为0，需要用lseek重新seek一下。这篇文章介绍的很详细。当时主要还是函数说明没有看仔细。 之前有个要把打开的文件清空，然后重新写入的需求，但是使用 ftruncate(fd, 0)后，并没有达到效果，反而文件头部有了’\0’，长度比预想的大了。究其原因是没有使用 lseek 重置文件偏移量，是我太天真了，以为清空文件就会从头开始写入。 ————————————- 我是解释分割线 ————————————– 首先 man ftruncate 看下帮助手册 NAME&emsp;&emsp;truncate, ftruncate - truncate a file to a specified length SYNOPSIS&emsp;&emsp;int truncate (const char* path, off_t length);&emsp;&emsp;int ftruncate(int fd, off_t length); DESCRIPTION&emsp;&emsp;The truncate() and ftruncate() functions cause the regular file named by path or referenced by fd to be truncated to a size of precisely length bytes.&emsp;&emsp;If the file previously was larger than this size, the extra data is lost. If the file previously was shorter, it is extended, and the extended part reads as null bytes (‘\0’).&emsp;&emsp;The file offset is not changed.&emsp;&emsp;If the size changed, then the st_ctime and st_mtime fields (respectively, time of last status change and time of last modification; see stat(2)) for the file are updated, and the set-user-ID and set-group-ID permission bits may be cleared.&emsp;&emsp;With ftruncate(), the file must be open for writing; with truncate(), the file must be writable. 之前就是因为没有看到红色那行字，导致我产生了文件开头的错误，都说了文件偏移量是不会改变的！ 实验如下： 12345678910111213141516171819202122232425262728#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/stat.h&gt;#include &lt;fcntl.h&gt;int main(void)&#123; int fd; const char *s1 = &quot;0123456789&quot;; const char *s2 = &quot;abcde&quot;; fd = open(&quot;test.txt&quot;, O_CREAT | O_WRONLY | O_TRUNC, 0666); /* if error */ write(fd, s1, strlen(s1)); ftruncate(fd, 0); // lseek(fd, 0, SEEK_SET); write(fd, s2, strlen(s2)); close(fd); return 0;&#125; 去掉lseek(fd, 0, SEEK_SET); 的注释后，效果如下： 结论： 从以上两张图中，可以看出，不用 lseek 的文件大小为15，用 xxd 查看16进制格式看到 文件头有10个 ‘\0’ 填充。 而重置文件偏移量后，文件大小为5，内容也正确。 因此，在用 ftruncate 函数时，再次写入一定要重新设置文件偏移量（在 ftruncate 之前或之后都行，用 lseek 或 rewind 都可以）。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ftruncate</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The History of RocksDB]]></title>
    <url>%2F2017%2F12%2F14%2Fthe%20history%20of%20rocksdb%2F</url>
    <content type="text"><![CDATA[这篇文章讲述了rocksdb的由来。 首先确定了为什么要做成Embedded database. 而不是其他server类型的数据库，具体请看Dhruba Borthakur的一个presentation-Tech Talk: RocksDB Slides by Dhruba Borthakur &amp; Haobo Xu of Facebook. 为什么不使用leveldb，而是使用rocksdb。文章详情请戳THE HISTORY OF ROCKSDB.]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>rocksdb</tag>
        <tag>leveldb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用命令]]></title>
    <url>%2F2017%2F12%2F13%2F%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[这篇文章是记录一些常用命令的以便查找。 vncserver centos 7 修改屏幕分辨率 vncserver -geometry 1440x900 ubutun 修改屏幕分辨率 vncserver -geometry 1440x900 linux查看系统内存大小 free -m linux下将用户添加到sudoers中linux下将用户添加到sudoers中 root 账户键入visudo即可进入sudo配置，这个命令要比vim /etc/sudoers要好很多，因为使用visudo进行sudo配置，将会得到很多提示.键入visudo后, 在编辑器下键入 /root 寻找root，找到第三个root的那一行 1root ALL=(ALL) AL 按yyp键复制并在粘贴在下一行，在这一行的 root处输入cw将root替换为你所需要添加用户的账户名，比如blinux，结果就是 12root ALL=(ALL) ALLblinux ALL=(ALL) ALL 如果你希望之后执行sudo命令时不需要输入密码，那么可以形如 12root ALL=(ALL) ALLblinux ALL=(ALL) NOPASSWD:ALL 输入:wq保存即可。 查看端口被占用情况1netstat -tunpl | grep port 根据进程号查看进程的详细信息Linux在启动一个进程时，系统会在/proc下创建一个以PID命名的文件夹，在该文件夹下会有我们的进程的信息，其中包括一个名为exe的文件即记录了绝对路径，通过ll或ls –l命令即可查看。 1ll /proc/PID cwd符号链接的是进程运行目录； exe符号连接就是执行程序的绝对路径； cmdline就是程序运行时输入的命令行命令； environ记录了进程运行时的环境变量； fd目录下是进程打开或使用的文件的符号连接。 Linux下vim快捷键多行注释： 首先按esc进入命令行模式下，按下Ctrl + v，进入列（也叫区块）模式; 在行首使用上下键选择需要注释的多行; 按下键盘（大写）“I”键，进入插入模式； 然后输入注释符（“//”、“#”等）; 最后按下“Esc”键。注意：在按下esc键后，会稍等一会才会出现注释，不要着急~~时间很短的 删除多行注释： 首先按esc进入命令行模式下，按下Ctrl + v, 进入列模式; 选定要取消注释的多行; 按下“x”或者“d”.注意：如果是“//”注释，那需要执行两次该操作，如果是“#”注释，一次即可 vim多行文本删除 首先在命令模式下，输入“：set nu”显示行号； 通过行号确定你要删除的行； 命令输入“：32,65d”,回车键，32-65行就被删除了，很快捷吧注意：如果无意中删除错了，可以使用‘u’键恢复（命令模式下） vim行内跳转 （mac 下） w：跳转到下一个单词的开始 e：跳到单词的结束 b：向后跳 CTRL+E：向下一行 CTRL+Y：向上一行 ^：行首 $：行尾 shell行内跳转 CTRL+a：行首 CTRL+e：行尾 CTRL+b：往回（左）移动一个字符 CTRL+f：往后（右）移动一个字符 ALT+b：往回（左）移动一个单词 ALT+F：往后（右）移动一个单词 vim中json格式化:%!python -m json.tool grep 快捷键 grep -C 5 foo file 显示file文件中匹配foo字串那行以及上下5行 grep -B 5 foo file 显示foo及前5行 grep -A 5 foo file 显示foo及后5行 linux ls 命令linux ls 按文件大小排序 ls -Sl 其是按照由大到小排序 ls -Slr 按照由小到大排序 再者，如果想要输入是按照“便于人类阅读的方式”，那么就再加一个-h，表示”–human-readable” 这样单位就是k或者M ，比较容易看清楚结果。 mac 下快捷键]]></content>
      <categories>
        <category>常用命令</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>vncserver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Top 5 Reasons for Choosing S3 over HDFS]]></title>
    <url>%2F2017%2F12%2F08%2FTop%205%20Reasons%20for%20Choosing%20S3%20over%20HDFS%2F</url>
    <content type="text"><![CDATA[这篇文章是databricks的一篇博客，主要讲解了为什么在存储计算分离之后，选用S3而不选用HDFS的原因。简单说就是Cost, elasticity, availability, durability, performance, and data integrity。后面是原文地址Top 5 Reasons for Choosing S3 over HDFS.At Databricks, our engineers guide thousands of organizations to define their big data and cloud strategies. When migrating big data workloads to the cloud, one of the most commonly asked questions is how to evaluate HDFS versus the storage systems provided by cloud providers, such as Amazon’s S3, Microsoft’s Azure Blob Storage, and Google’s Cloud Storage. In this blog post, we share our thoughts on why cloud storage is the optimal choice for data storage. In this discussion, we use Amazon S3 as an example, but the conclusions generalize to other cloud platforms. We compare S3 and HDFS along the following dimensions: Cost Elasticity SLA (availability and durability) Performance per dollar Transactional writes and data integrity CostLet’s consider the total cost of storage, which is a combination of storage cost and human cost (to maintain them). First, let’s estimate the cost of storing 1 terabyte of data per month. As of May 2017, S3’s standard storage price for the first 1TB of data is $23/month. Note that depending on your usage pattern, S3 listing and file transfer might cost money. On the other hand, cold data using infrequent-access storage would cost only half, at $12.5/month. For the purpose of this discussion, let’s use $23/month to approximate the cost. S3 does not come with compute capacity but it does give you the freedom to leverage ephemeral clusters and to select instance types best suited for a workload (e.g., compute intensive), rather than simply for what is the best from a storage perspective. For HDFS, the most cost-efficient storage instances on EC2 is the d2 family. To be generous and work out the best case for HDFS, we use the following assumptions that are virtually impossible to achieve in practice: A crystal ball into the future to perfectly predict the storage requirements three years in advance, so we can use the maximum discount using 3-year reserved instances. Workloads are stable with a peak-to-trough ratio of 1.0. This means our storage system does not need to be elastic at all. Storage utilization is at 70%, and standard HDFS replication factor set at 3. With the above assumptions, using d2.8xl instance types ($5.52/hr with 71% discount, 48TB HDD), it costs 5.52 x 0.29 x 24 x 30 / 48 x 3 / 0.7 = $103/month for 1TB of data. (Note that with reserved instances, it is possible to achieve lower price on the d2 family.) So in terms of storage cost alone, S3 is 5X cheaper than HDFS. Based on our experience managing petabytes of data, S3’s human cost is virtually zero, whereas it usually takes a team of Hadoop engineers or vendor support to maintain HDFS. Once we factor in human cost, S3 is 10X cheaper than HDFS clusters on EC2 with comparable capacity. ElasticityCapacity planning is tough to get right, and very few organizations can accurately estimate their resource requirements upfront. In the on-premise world, this leads to either massive pain in the post-hoc provisioning of more resources or huge waste due to low utilization from over-provisioning upfront. One of the nicest benefits of S3, or cloud storage in general, is its elasticity and pay-as-you-go pricing model: you are only charged what you put in, and if you need to put more data in, just dump them there. Under the hood, the cloud provider automatically provisions resources on demand. Simply put, S3 is elastic, HDFS is not. SLA (Availability and Durability)Based on our experience, S3’s availability has been fantastic. Only twice in the last six years have we experienced S3 downtime and we have never experienced data loss from S3. Amazon claims 99.999999999% durability and 99.99% availability. Note that this is higher than the vast majority of organizations’ in-house services. The official SLA from Amazon can be found here: Service Level Agreement – Amazon Simple Storage Service (S3). For HDFS, in contrast, it is difficult to estimate availability and durability. One could theoretically compute the two SLA attributes based on EC2’s mean time between failures (MTTF), plus upgrade and maintenance downtimes. In reality, those are difficult to quantify. Our understanding working with customers is that the majority of Hadoop clusters have availability lower than 99.9%, i.e. at least 9 hours of downtime per year. With cross-AZ replication that automatically replicates across different data centers,S3’s availability and durability is far superior to HDFS’. Performance per DollarThe main problem with S3 is that the consumers no longer have data locality and all reads need to transfer data across the network, and S3 performance tuning itself is a black box. When using HDFS and getting perfect data locality, it is possible to get ~3GB/node local read throughput on some of the instance types (e.g. i2.8xl, roughly 90MB/s per core). DBIO, our cloud I/O optimization module, provides optimized connectors to S3 and can sustain ~600MB/s read throughput on i2.8xl (roughly 20MB/s per core). That is to say, on a per node basis, HDFS can yield 6X higher read throughput than S3. Thus, given that the S3 is 10x cheaper than HDFS, we find that S3 is almost 2x better compared to HDFS on performance per dollar. However, a big benefit with S3 is we can separate storage from compute, and as a result, we can just launch a larger cluster for a smaller period of time to increase throughput, up to allowable physical limits. This separation of compute and storage also allow for different Spark applications (such as a data engineering ETL job and an ad-hoc data science model training cluster) to run on their own clusters, preventing concurrency issues that affect multi-user fixed-sized Hadoop clusters. This separation (and the flexible accommodation of disparate workloads) not only lowers cost but also improves the user experience. One advantage HDFS has over S3 is metadata performance: it is relatively fast to list thousands of files against HDFS namenode but can take a long time for S3. However, the scalable partition handling feature we implemented in Apache Spark 2.1 mitigates this issue with metadata performance in S3. Stay tuned for announcements in the near future that completely eliminates this issue with DBIO. Transactional Writes and Data IntegrityMost of the big data systems (e.g., Spark, Hive) rely on HDFS’ atomic rename feature to support atomic writes: that is, the output of a job is observed by the readers in an “all or nothing” fashion. This is important for data integrity because when a job fails, no partial data should be written out to corrupt the dataset. S3’s lack of atomic directory renames has been a critical problem for guaranteeing data integrity. This has led to complicated application logic to guarantee data integrity, e.g. never append to an existing partition of data. Today, we are happy to announce the support for transactional writes in our DBIO artifact, which features high-performance connectors to S3 (and in the future other cloud storage systems) with transactional write support for data integrity. See this blog post for more information. Other Operational ConcernsSo far, we have discussed durability, performance, and cost considerations, but there are several other areas where systems like S3 have lower operational costs and greater ease-of-use than HDFS: Encryption, access control, and auditing: S3 supports multiple types of encryption, with both AWS- and customer-managed keys, and has easy-to-configure audit logging and access control capabilities. These features make it easy to meet regulatory compliance needs, such as PCI or HIPAA compliance.Backups and disaster recovery: S3’s opt-in versioning feature automatically maintains backups of modified or deleted files, making it easy to recover from accidental data deletion. Cross-region replication can be used to enhance S3’s already strong availability guarantees in order to withstand the complete outage of an AWS region.Data lifecycle management: S3 can be configured to automatically migrate objects to cold storage after a configurable time period. In many organizations, data is read frequently when it is new and is read significantly less often over time. S3’s lifecycle management policies can automatically perform migration of old objects to Infrequent Access storage in order to save on cost, or to Glacier to achieve even larger cost savings; the latter is useful for organizations where regulatory compliance mandates long-term storage of data.Supporting these additional requirements on HDFS requires even more work on the part of system administrators and further increases operational cost and complexity. ConclusionIn this blog post we used S3 as the example to compare cloud storage vs HDFS: Item S3 HDFS S3 vs HDFS Elasticity Yes No S3 is more elastic Cost/TB/month $23 $206 10X Availability 99.99% 99.9% (estimated) 10X Durability 99.999999999% 99.9999% (estimated) 10X+ Transactional writes Yes with DBIO Yes Comparable To summarize, S3 and cloud storage provide elasticity, with an order of magnitude better availability and durability and 2X better performance, at 10X lower cost than traditional HDFS data storage clusters. Hadoop and HDFS commoditized big data storage by making it cheap to store and distribute a large amount of data. However, in a cloud native architecture, the benefit of HDFS is minimal and not worth the operational complexity. That is why many organizations do not operate HDFS in the cloud, but instead use S3 as the storage backend. With Databricks’ DBIO, our customers can sit back and enjoy the merits of performant connectors to cloud storage without sacrificing data integrity.]]></content>
      <categories>
        <category>云存储</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
        <tag>对象存储</tag>
        <tag>S3</tag>
        <tag>存储计算分离</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[希腊神话故事]]></title>
    <url>%2F2017%2F11%2F28%2F%E5%B8%8C%E8%85%8A%E7%A5%9E%E8%AF%9D%E6%95%85%E4%BA%8B%2F</url>
    <content type="text"><![CDATA[本文主要讲解希腊神话故事的一些名词以及解释。 人名 哈迪斯(Hades):希腊冥界老大 珀尔赛福涅(Persephone):被Hades拐走当妻子的少女 得墨忒耳(Demeter):掌农业，结婚，丰饶之女神;Persephone的妈妈。 克洛诺斯（Kronos）:统治希腊天神的首领，哈迪斯的爸爸，代表这泰坦（Titans）族诸神的势力。 宙斯(Zeus):最后推翻他老爸克洛诺斯的天神，建立了新的王朝，代表着奥林匹斯（Olympians）诸神； 卡戎（Charon）:摆渡的船夫；冥府渡神 西西弗斯（Sisyphus）:古希腊神话中一位邪恶的国王，他死后得到的惩罚是把一块巨石滚上陡峭的山坡。每次快要滚到山顶，巨石都会滚落回山脚，他又得重新开始，如此永远反复下去。一项不可能完成的艰巨任务有时被形容为 a Sisyphean task. 动物名 刻尔泊洛斯（Cerberus）：把守冥府的三头犬 俄耳甫斯(Orpheus):希腊神话人物，一个出色的音乐家 欧律狄刻（Eurydice）：欧律狄刻; 一林中仙女，俄耳甫斯之妻，在其死后，俄耳甫斯到冥府去寻找她；俄耳甫斯未能遵守禁令回头看了欧律狄刻，最后欧律狄刻仍被抓回阴间 Satyrs：色情狂（satyr的复数形式）；希腊神话中的半人半兽 地名 长春花平原(Asphodel): 阴间，大多数无名小辈死后居住的地方 塔尔塔洛斯(Tartarus)：天上的神犯了重大错误待的地方，也就是早期基督教中的地域 天堂(paradise):那些达官显贵等人死后待的地方 多利斯(Diros):希腊地下延绵数英里的洞穴群，应该是哈迪斯统治的地方（地狱）的一个现实版的对应，]]></content>
      <categories>
        <category>神话</category>
      </categories>
      <tags>
        <tag>希腊神话</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[B tree 与B+Tree的区别以及原理和应用场景]]></title>
    <url>%2F2017%2F06%2F26%2FB-tree-%E4%B8%8EB-Tree%E7%9A%84%E5%8C%BA%E5%88%AB%E4%BB%A5%E5%8F%8A%E5%8E%9F%E7%90%86%E5%92%8C%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%2F</url>
    <content type="text"><![CDATA[B-tree的由来？为什么非得是树呢，而不是直接是数组。Memory locality &amp; the magic of B-Trees!:说了很清楚，就是因为在申请内存的时候，不知道要申请多大的内存，所以没办法申请很大的一块内存，所以就变成了一个数组被打断为好几段，然后每段用链表连接起来，这其实就是树的基本模型。 B-tree和B+tree的区别是什么？ B-tree中非叶子节点可以存值；但是B+tree非叶子节点不可以存值，只能存key，值只存在叶子节点中。 B-tree中叶子节点没有用指针连接起来；而B+tree中的叶子节点用指针连接起来，所以B+tree的查询很快，当定位到叶子节点后，只需要遍历叶子节点即可。B+树相比于B树能够更加方便的遍历。 B+树简单的说就是变成了一个索引一样的东西。 B+的搜索与B-树也基本相同，区别是B+树只有达到叶子结点才命中（B-树可以在非叶子结点命中），B+树的性能相当于是给叶子节点做一次二分查找。 B+树的查找算法：当B+树进行查找的时候，你首先一定需要记住，就是B+树的非叶子节点中并不储存节点，只存一个键值方便后续的操作，所以非叶子节点就是索引部分，所有的叶子节点是在同一层上，包含了全部的关键值和对应数据所在的地址指针。这样其实，进行 B+树的查找的时候，只需要在叶子节点中进行查找就可以了。 各自的应用场景有什么不同?mysql的MyISAM和InnoDB两个存储引擎的索引实现方式： MyISAM引擎使用B+ Tree作为索引结构，叶节点存放的是数据记录的地址。 MyISAM引擎的辅助索引（二级索引）和主索引在结构上没有区别，只是辅助索引的key可以重复，叶节点上存放的也是数据记录的地址。 MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。 InnoDB中表数据本身就是按B+ Tree组织的一个索引结构，叶节点存放的就不是数据记录的地址，而是完整的数据记录。所以InnoDB这种存储方式，又称为聚集索引，使得按主键的搜索十分高效，但二级索引搜索需要检索两遍索引：首先二级索引获得主键，然后用主键到主索引中检索到数据记录。 因为主键是InnoDB表记录的”逻辑地址“，所以InnoDB要求表必须有主键，MyISAM可以没有。 自己的一点想法看了很多的文章，心中一直在问自己一个问题，既然B+树那么好，那么B树存在的意义什么呢？因为innodb存储引擎和MyISAM存储都是使用的B+树。我自己的理解是：B树适合那种访问之后能够把相应的数据一起返回的数据结构。 那么，什么样子的才是访问到key之后把相应的value数据一起就返回的呢？上面也提到了因为B+树非叶子节点不存储value，所以占得空间小，所以相同的内存大小，B+树存储的节点更多，这说明了能够放在内存中的非叶子节点就更多。那么，B树存在的意义又是什么呢？ 我觉得B树存在的意义是当都在内存中的时候，若是能够根据key直接把value得到然后返回，不用去磁盘查找， 明显要快很多，因为B+树的数据(value)是存在叶子节点中的，而叶子节点是存在于磁盘中的。也就是说当数据很小的时候，使用B数把数据跟key一起存放在非叶子节点中，可以更快的加速查找。 参考文献 https://yq.aliyun.com/articles/65126 http://www.itwendao.com/article/detail/68322.html https://loveforprogramming.quora.com/Memory-locality-the-magic-of-B-Trees http://www.cnblogs.com/yanghuahui/p/3483047.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[进程间通信方式-java实现]]></title>
    <url>%2F2017%2F06%2F12%2F%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E6%96%B9%E5%BC%8F-java%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[UNIX 为实现这样的进程间通信 提供了多种技术。一些技术提供同一主机上的进程间通信，其他技术可以实现主机到主机的信息交换。另外，各种技术的速度不同，所以必须选择最合适自己需求的技术。还必须进行协调（实施时间控制和排他控制）。例如，如果一个应用程序产生数据，另一个应用程序消费数据，那么当读完共享池时消费者必须停下来等待生产者。另一方面，如果消费者无法足够快地读取池，生产者必须慢下来或暂停。 表 1 总结在典型的 UNIX 系统上可用的进程间通信形式。 名称 说明 范围 用途 文件 在典型的 UNIX 文件中读写数据。任意数量的进程都可以互操作。 本地 共享大数据集 管道 使用专用的文件描述符在两个进程之间传输数据。通信只在父进程和子进程之间进行。 本地 简单的数据共享，比如生产者和消费者 命名管道 通过专用的文件描述符在进程之间交换数据。通信可以在同一主机上的任意两个对等进程之间进行。 本地 生产者和消费者或命令-控制，比如 MySQL 和它的命令行查询工具 信号 通过中断通知应用程序某一情况。 本地 无法在信号中传输数据，所以信号主要用于进程管理 共享内存 通过在同一内存段中读写数据共享信息。 本地 任何类型的协作，尤其适合需要安全性的情况 套接字 完成特殊的设置过程之后，使用一般的输入/输出操作传输数据。 本地或远程 FTP、ssh 和 Apache Web Server 等网络服务 管道管道的相关概念操作系统中的管道是linux支持的最初的Unix IPC 形式之一，且具有以下特点 管道是半双工的，数据只能向一个方向流动；需要双方通信时，需要建立两个管道。 只能用于父子进程或是兄弟进程之间的通信（具有亲缘关系的进程） 单独构成一个独立的文件系统：管道对于管道两端的进程而言，就是一个文件，但是他不是普通的文件，它不属于某种文件系统，而是自立门户，单独构成一种文件系统，并且只存在内存中。 数据的读出和写入：一个进程向管道中写的内容被管道另一端的进程读出。写入的内容每次都添加在管道缓冲区的末尾，并且每次都是从缓冲区的头部读出数据。 管道注意的点Java 中的管道操作系统中默认的管道是进程间的通信当时，但是在java中，管道是进程内线程之间的通信方式。这是java中的管道和操作系统中的管道的最大的区别。 如果是在同一个线程中去使用java中的管道还会引起引起死锁，主要的原因就是java中申请的管道默认最大为1k即1024B.当缓冲区满，并且写还没写完的时候，就会阻塞。直至等待读，但此时因为在同一个线程，读也被阻塞。所以，就会造成死锁。详细请看文章 Java里的管道输入流PipedInputStream与管道输出流 PipedOutputStream 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package ipc;import java.io.IOException;import java.io.PipedInputStream;import java.io.PipedOutputStream;public class PipeExample &#123; public static void main(String[] args) &#123; final PipedOutputStream out = new PipedOutputStream(); final PipedInputStream in = new PipedInputStream(); Thread thread1 = new Thread(new Runnable()&#123; @Override public void run() &#123; try&#123; while(true)&#123; out.write(&quot;hello world,pile!&quot;.getBytes()); &#125; //如果上面不是while(true)循环，并且还没有执行out.close()方法的话，则就会报错图1的错误 //原因就是：PipedInputStream throws this exception on read() //when there is no writing thread and writer is not properly closed //所以，当一直是while(true)不断写东西的时候，是不会报错的// out.close(); &#125;catch (IOException e)&#123; e.printStackTrace(); &#125; &#125; &#125;); Thread thread2 = new Thread(new Runnable()&#123; @Override public void run() &#123; try &#123; int data = in.read(); while(data!=-1)&#123; System.out.println((char)data); data = in.read(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;); try &#123; in.connect(out); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; thread1.start(); thread2.start(); try &#123; //等待线程执行完 thread1.join(); thread2.join(); in.close(); out.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 共享内存请参考此文章:对话 UNIX: 通过共享内存进行进程间通信 参考文献 Linux环境进程间通信（一） Java IO:Pipes Java里的管道输入流 PipedInputStream与管道输出流PipedOutputStream Inter-Process Communication 进程间通信 对话 UNIX: 通过共享内存进行进程间通信]]></content>
      <categories>
        <category>操作系统</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>面试</tag>
        <tag>操作系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP三次握手[转载]]]></title>
    <url>%2F2017%2F05%2F19%2FTCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%2F</url>
    <content type="text"><![CDATA[TCP是什么？具体的关于TCP是什么，我不打算详细的说了；当你看到这篇文章时，我想你也知道TCP的概念了，想要更深入的了解TCP的工作，我们就继续。它只是一个超级麻烦的协议，而它又是互联网的基础，也是每个程序员必备的基本功。首先来看看OSI的七层模型： 我们需要知道TCP工作在网络OSI的七层模型中的第四层——Transport层，IP在第三层——Network层，ARP在第二层——Data Link层；在第二层上的数据，我们把它叫Frame，在第三层上的数据叫Packet，第四层的数据叫Segment。 同时，我们需要简单的知道，数据从应用层发下来，会在每一层都会加上头部信息，进行封装，然后再发送到数据接收端。这个基本的流程你需要知道，就是每个数据都会经过数据的封装和解封装的过程。 在OSI七层模型中，每一层的作用和对应的协议如下： TCP是一个协议，那这个协议是如何定义的，它的数据格式是什么样子的呢？要进行更深层次的剖析，就需要了解，甚至是熟记TCP协议中每个字段的含义。哦，来吧。 上面就是TCP协议头部的格式，由于它太重要了，是理解其它内容的基础，下面就将每个字段的信息都详细的说明一下。 Source Port和Destination Port:分别占用16位，表示源端口号和目的端口号；用于区别主机中的不同进程，而IP地址是用来区分不同的主机的，源端口号和目的端口号配合上IP首部中的源IP地址和目的IP地址就能唯一的确定一个TCP连接； Sequence Number:用来标识从TCP发端向TCP收端发送的数据字节流，它表示在这个报文段中的的第一个数据字节在数据流中的序号；主要用来解决网络报乱序的问题； Acknowledgment Number:32位确认序列号包含发送确认的一端所期望收到的下一个序号，因此，确认序号应当是上次已成功收到数据字节序号加1。不过，只有当标志位中的ACK标志（下面介绍）为1时该确认序列号的字段才有效。主要用来解决不丢包的问题； Offset:给出首部中32 bit字的数目，需要这个值是因为任选字段的长度是可变的。这个字段占4bit（最多能表示15个32bit的的字，即4*15=60个字节的首部长度），因此TCP最多有60字节的首部。然而，没有任选字段，正常的长度是20字节； TCP Flags:TCP首部中有6个标志比特，它们中的多个可同时被设置为1，主要是用于操控TCP的状态机的，依次为URG，ACK，PSH，RST，SYN，FIN。每个标志位的意思如下： URG：此标志表示TCP包的紧急指针域（后面马上就要说到）有效，用来保证TCP连接不被中断，并且督促中间层设备要尽快处理这些数据； ACK：此标志表示应答域有效，就是说前面所说的TCP应答号将会包含在TCP数据包中；有两个取值：0和1，为1的时候表示应答域有效，反之为0； PSH：这个标志位表示Push操作。所谓Push操作就是指在数据包到达接收端以后，立即传送给应用程序，而不是在缓冲区中排队； RST：这个标志表示连接复位请求。用来复位那些产生错误的连接，也被用来拒绝错误和非法的数据包； SYN：表示同步序号，用来建立连接。SYN标志位和ACK标志位搭配使用，当连接请求的时候，SYN=1，ACK=0；连接被响应的时候，SYN=1，ACK=1；这个标志的数据包经常被用来进行端口扫描。扫描者发送一个只有SYN的数据包，如果对方主机响应了一个数据包回来 ，就表明这台主机存在这个端口；但是由于这种扫描方式只是进行TCP三次握手的第一次握手，因此这种扫描的成功表示被扫描的机器不很安全，一台安全的主机将会强制要求一个连接严格的进行TCP的三次握手； FIN： 表示发送端已经达到数据末尾，也就是说双方的数据传送完成，没有数据可以传送了，发送FIN标志位的TCP数据包后，连接将被断开。这个标志的数据包也经常被用于进行端口扫描。 Window:窗口大小，也就是有名的滑动窗口，用来进行流量控制；这是一个复杂的问题，这篇博文中并不会进行总结的；好了，基本知识都已经准备好了，开始下一段的征程吧。 三次握手又是什么？TCP是面向连接的，无论哪一方向另一方发送数据之前，都必须先在双方之间建立一条连接。在TCP/IP协议中，TCP协议提供可靠的连接服务，连接是通过三次握手进行初始化的。三次握手的目的是同步连接双方的序列号和确认号并交换 TCP窗口大小信息。这就是面试中经常会被问到的TCP三次握手。只是了解TCP三次握手的概念，对你获得一份工作是没有任何帮助的，你需要去了解TCP三次握手中的一些细节。先来看图说话。 多么清晰的一张图，当然了，也不是我画的，我也只是引用过来说明问题了。 第一次握手：建立连接。客户端发送连接请求报文段，将SYN位置为1，Sequence Number为x；然后，客户端进入SYN_SEND状态，等待服务器的确认； 第二次握手：服务器收到SYN报文段。服务器收到客户端的SYN报文段，需要对这个SYN报文段进行确认，设置Acknowledgment Number为x+1(Sequence Number+1)；同时，自己自己还要发送SYN请求信息，将SYN位置为1，Sequence Number为y；服务器端将上述所有信息放到一个报文段（即SYN+ACK报文段）中，一并发送给客户端，此时服务器进入SYN_RECV状态； 第三次握手：客户端收到服务器的SYN+ACK报文段。然后将Acknowledgment Number设置为y+1，向服务器发送ACK报文段，这个报文段发送完毕以后，客户端和服务器端都进入ESTABLISHED状态，完成TCP三次握手。完成了三次握手，客户端和服务器端就可以开始传送数据。以上就是TCP三次握手的总体介绍。 那四次分手呢？当客户端和服务器通过三次握手建立了TCP连接以后，当数据传送完毕，肯定是要断开TCP连接的啊。那对于TCP的断开连接，这里就有了神秘的“四次分手”。 第一次分手：主机1（可以使客户端，也可以是服务器端），设置Sequence Number和Acknowledgment Number，向主机2发送一个FIN报文段；此时，主机1进入FIN_WAIT_1状态；这表示主机1没有数据要发送给主机2了； 第二次分手：主机2收到了主机1发送的FIN报文段，向主机1回一个ACK报文段，Acknowledgment Number为Sequence Number加1；主机1进入FIN_WAIT_2状态；主机2告诉主机1，我“同意”你的关闭请求； 第三次分手：主机2向主机1发送FIN报文段，请求关闭连接，同时主机2进入LAST_ACK状态； 第四次分手：主机1收到主机2发送的FIN报文段，向主机2发送ACK报文段，然后主机1进入TIME_WAIT状态；主机2收到主机1的ACK报文段以后，就关闭连接；此时，主机1等待2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，主机1也可以关闭连接了。至此，TCP的四次分手就这么愉快的完成了。当你看到这里，你的脑子里会有很多的疑问，很多的不懂，感觉很凌乱；没事，我们继续总结。 为什么要三次握手白话例子1但是为什么一定要进行三次握手来保证连接是双工的呢，一次不行么？两次不行么？我们举一个现实生活中两个人进行语言沟通的例子来模拟三次握手。 引用网上的一些通俗易懂的例子，虽然不太正确，后面会指出，但是不妨碍我们理解，大体就是这么个理解法。 第一次对话： 老婆让甲出去打酱油，半路碰到一个朋友乙，甲问了一句：哥们你吃饭了么？ 结果乙带着耳机听歌呢，根本没听到，没反应。甲心里想：跟你说话也没个音，不跟你说了，沟通失败。说明乙接受不到甲传过来的信息的情况下沟通肯定是失败的。 如果乙听到了甲说的话，那么第一次对话成功，接下来进行第二次对话。 第二次对话： 乙听到了甲说的话，但是他是老外，中文不好，不知道甲说的啥意思也不知道怎样回答，于是随便回答了一句学过的中文 ：我去厕所了。甲一听立刻笑喷了，“去厕所吃饭”?道不同不相为谋，离你远点吧，沟通失败。说明乙无法做出正确应答的情况下沟通失败。 如果乙听到了甲的话，做出了正确的应答，并且还进行了反问：我吃饭了，你呢？那么第二次握手成功。 通过前两次对话证明了乙能够听懂甲说的话，并且能做出正确的应答。 接下来进行第三次对话。 第三次对话： 甲刚和乙打了个招呼，突然老婆喊他，“你个死鬼，打个酱油咋这么半天，看我回家咋收拾你”，甲是个妻管严，听完吓得二话不说就跑回家了，把乙自己晾那了。乙心想：这什么人啊，得，我也回家吧，沟通失败。说明甲无法做出应答的情况下沟通失败。 如果甲也做出了正确的应答：我也吃了。那么第三次对话成功，两人已经建立起了顺畅的沟通渠道，接下来开始持续的聊天。 通过第二次和第三次的对话证明了甲能够听懂乙说的话，并且能做出正确的应答。 可见，两个人进行有效的语言沟通，这三次对话的过程是必须的。 为了保证服务端能收接受到客户端的信息并能做出正确的应答而进行前两次(第一次和第二次)握手，为了保证客户端能够接收到服务端的信息并能做出正确的应答而进行后两次(第二次和第三次)握手。 例子2在谢希仁的《计算机网络》中是这样说的： 为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误。 在书中同时举了一个例子，如下： “已失效的连接请求报文段”的产生在这样一种情况下：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。” 这就很明白了，防止了服务器端的一直等待而浪费资源。 总结在Google Groups的TopLanguage中看到一帖讨论TCP“三次握手”觉得很有意思。贴主提出“TCP建立连接为什么是三次握手？”的问题，在众多回复中，有一条回复写道：“这个问题的本质是, 信道不可靠, 但是通信双发需要就某个问题达成一致. 而要解决这个问题, 无论你在消息中包含什么信息, 三次通信是理论上的最小值. 所以三次握手不是TCP本身的要求, 而是为了满足”在不可靠信道上可靠地传输信息”这一需求所导致的. 请注意这里的本质需求,信道不可靠, 数据传输要可靠. 三次达到了, 那后面你想接着握手也好, 发数据也好, 跟进行可靠信息传输的需求就没关系了. 因此,如果信道是可靠的, 即无论什么时候发出消息, 对方一定能收到, 或者你不关心是否要保证对方收到你的消息, 那就能像UDP那样直接发送消息就可以了.”。这可视为对“三次握手”目的的另一种解答思路。 为什么要四次分手那四次分手又是为何呢？TCP协议是一种面向连接的、可靠的、基于字节流的运输层通信协议。TCP是全双工模式，这就意味着，当主机1发出FIN报文段时，只是表示主机1已经没有数据要发送了，主机1告诉主机2，它的数据已经全部发送完毕了；但是，这个时候主机1还是可以接受来自主机2的数据；当主机2返回ACK报文段时，表示它已经知道主机1没有数据发送了，但是主机2还是可以发送数据到主机1的；当主机2也发送了FIN报文段时，这个时候就表示主机2也没有数据要发送了，就会告诉主机1，我也没有数据要发送了，之后彼此就会愉快的中断这次TCP连接。如果要正确的理解四次分手的原理，就需要了解四次分手过程中的状态变化。 FIN_WAIT_1: 这个状态要好好解释一下，其实FIN_WAIT_1和FIN_WAIT_2状态的真正含义都是表示等待对方的FIN报文。而这两种状态的区别是：FIN_WAIT_1状态实际上是当SOCKET在ESTABLISHED状态时，它想主动关闭连接，向对方发送了FIN报文，此时该SOCKET即进入到FIN_WAIT_1状态。而当对方回应ACK报文后，则进入到FIN_WAIT_2状态，当然在实际的正常情况下，无论对方何种情况下，都应该马上回应ACK报文，所以FIN_WAIT_1状态一般是比较难见到的，而FIN_WAIT_2状态还有时常常可以用netstat看到。（主动方） FIN_WAIT_2：上面已经详细解释了这种状态，实际上FIN_WAIT_2状态下的SOCKET，表示半连接，也即有一方要求close连接，但另外还告诉对方，我暂时还有点数据需要传送给你(ACK信息)，稍后再关闭连接。（主动方） CLOSE_WAIT：这种状态的含义其实是表示在等待关闭。怎么理解呢？当对方close一个SOCKET后发送FIN报文给自己，你系统毫无疑问地会回应一个ACK报文给对方，此时则进入到CLOSE_WAIT状态。接下来呢，实际上你真正需要考虑的事情是察看你是否还有数据发送给对方，如果没有的话，那么你也就可以 close这个SOCKET，发送FIN报文给对方，也即关闭连接。所以你在CLOSE_WAIT状态下，需要完成的事情是等待你去关闭连接。（被动方） LAST_ACK: 这个状态还是比较容易好理解的，它是被动关闭一方在发送FIN报文后，最后等待对方的ACK报文。当收到ACK报文后，也即可以进入到CLOSED可用状态了。（被动方） TIME_WAIT: 表示收到了对方的FIN报文，并发送出了ACK报文，就等2MSL后即可回到CLOSED可用状态了。如果FINWAIT1状态下，收到了对方同时带FIN标志和ACK标志的报文时，可以直接进入到TIME_WAIT状态，而无须经过FIN_WAIT_2状态。（主动方） CLOSED: 表示连接中断。 TIME_WAIT状态的含义简单来说，TIME_WAIT有两个作用，如下： 1） 确认主动关闭的一方最后发出的ack能够到达被动关闭的一方。 TCP协议在关闭连接的四次握手过程中，最终的ACK是由主动关闭连接的一端（后面统称A端）发出的，如果这个ACK丢失，对方（后面统称B端）将重发出最终的FIN，因此A端必须维护状态信息（TIME_WAIT）允许它重发最终的ACK。如果A端不维持TIME_WAIT状态，而是处于CLOSED 状态，那么A端将响应RST分节，B端收到后将此分节解释成一个错误（在java中会抛出connection reset的SocketException)。因而，要实现TCP全双工连接的正常终止，必须处理终止过程中四个分节任何一个分节的丢失情况，主动关闭连接的A端必须维持TIME_WAIT状态 。 2）允许老的重复分节在网络中消逝 TCP分节可能由于路由器异常而“迷途”，在迷途期间，TCP发送端可能因确认超时而重发这个分节，迷途的分节在路由器修复后也会被送到最终目的地，这个迟到的迷途分节到达时可能会引起问题。在关闭“前一个连接”之后，马上又重新建立起一个相同的IP和端口之间的“新连接”，“前一个连接”的迷途重复分组在“前一个连接”终止后到达，而被“新连接”收到了。为了避免这个情况，TCP协议不允许处于TIME_WAIT状态的连接启动一个新的可用连接，因为TIME_WAIT状态持续2MSL，就可以保证当成功建立一个新TCP连接的时候，来自旧连接重复分组已经在网络中消逝。 参考资料http://www.jellythink.com/archives/705http://www.cnblogs.com/techzi/archive/2011/10/18/2216751.htmlhttps://github.com/jawil/blog/issues/14]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>TCP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tcp以及udp的区别[转载]]]></title>
    <url>%2F2017%2F05%2F17%2Ftcp%E4%BB%A5%E5%8F%8Audp%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[基本概念： 面向报文 面向报文的传输方式是应用层交给UDP多长的报文，UDP就照样发送，即一次发送一个报文。因此，应用程序必须选择合适大小的报文。若报文太长，则IP层需要分片，降低效率。若太短，会是IP太小。UDP对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。这也就是说，应用层交给UDP多长的报文，UDP就照样发送，即一次发送一个报文。 面向字节流 面向字节流的话，虽然应用程序和TCP的交互是一次一个数据块（大小不等），但TCP把应用程序看成是一连串的无结构的字节流。TCP有一个缓冲，当应用程序传送的数据块太长，TCP就可以把它划分短一些再传送。如果应用程序一次只发送一个字节，TCP也可以等待积累有足够多的字节后再构成报文段发送出去。 下图是TCP和UDP协议的一些应用。 下图是TCP和UDP协议的比较。 这里再详细说一下面向连接和面向无连接的区别： 面向连接举例：两个人之间通过电话进行通信; 面向无连接举例：邮政服务，用户把信函放在邮件中期待邮政处理流程来传递邮政包裹。显然，不可达代表不可靠。 TCP/UDP编程模型从程序实现的角度来看，可以用下图来进行描述。 下图是TCP和UDP协议的比较。 从上图也能清晰的看出，TCP通信需要服务器端侦听listen、接收客户端连接请求accept，等待客户端connect建立连接后才能进行数据包的收发（recv/send）工作。而UDP则服务器和客户端的概念不明显，服务器端即接收端需要绑定端口，等待客户端的数据的到来。后续便可以进行数据的收发（recvfrom/sendto）工作。在前面讲解UDP时，提到了UDP保留了报文的边界，下面我们来谈谈TCP和UDP中报文的边界问题。在默认的阻塞模式下，TCP无边界，UDP有边界。对于TCP协议，客户端连续发送数据，只要服务端的这个函数的缓冲区足够大，会一次性接收过来，即客户端是分好几次发过来，是有边界的，而服务端却一次性接收过来，所以证明是无边界的；而对于UDP协议，客户端连续发送数据，即使服务端的这个函数的缓冲区足够大，也只会一次一次的接收，发送多少次接收多少次，即客户端分几次发送过来，服务端就必须按几次接收，从而证明，这种UDP的通讯模式是有边界的。 TCP/UDP的优缺点：TCP的优点：可靠，稳定TCP的可靠体现在TCP在传递数据之前，会有三次握手来建立连接，而且在数据传递时，有确认、窗口、重传、拥塞控制机制，在数据传完后，还会断开连接用来节约系统资源。 TCP的缺点：慢，效率低，占用系统资源高，易被攻击TCP在传递数据之前，要先建连接，这会消耗时间，而且在数据传递时，确认机制、重传机制、拥塞控制机制等都会消耗大量的时间，而且要在每台设备上维护所有的传输连接，事实上，每个连接都会占用系统的CPU、内存等硬件资源。而且，因为TCP有确认机制、三次握手机制，这些也导致TCP容易被人利用，实现DOS、DDOS、CC等攻击。 UDP的优点：快，比TCP稍安全UDP没有TCP的握手、确认、窗口、重传、拥塞控制等机制，UDP是一个无状态的传输协议，所以它在传递数据时非常快。没有TCP的这些机制，UDP较TCP被攻击者利用的漏洞就要少一些。但UDP也是无法避免攻击的，比如：UDP Flood攻击…… UDP的缺点：不可靠，不稳定因为UDP没有TCP那些可靠的机制，在数据传递时，如果网络质量不好，就会很容易丢包。 TCP/UDP应用场景：基于上面的优缺点，那么： 什么时候应该使用TCP：当对网络通讯质量有要求的时候，比如：整个数据要准确无误的传递给对方，这往往用于一些要求可靠的应用，比如HTTP、HTTPS、FTP等传输文件的协议，POP、SMTP等邮件传输的协议。在日常生活中，常见使用TCP协议的应用如下： 浏览器，用的HTTPFlashFXP，用的FTPOutlook，用的POP、SMTPPutty，用的Telnet、SSHQQ文件传输………… 那么什么时候应该使用UDP：当对网络通讯质量要求不高的时候，要求网络通讯速度能尽量的快，这时就可以使用UDP。比如，日常生活中，常见使用UDP协议的应用如下： QQ语音QQ视频TFTP…… 参考文献：http://blog.csdn.net/ce123_zhouwei/article/details/8976006http://zhidao.baidu.com/link?url=lMFVNDmMnTe_c3a66Zj0tXaqiIp_VO_mmT2_3fdfgx-GJNnpsUOroV_-uDEZ9FgoPcSU-CAM8ZPAdgW3iGyKiKhttp://blog.csdn.net/u013777351/article/details/49226101]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>TCP</tag>
        <tag>UDP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试准备基础知识]]></title>
    <url>%2F2017%2F05%2F16%2F%E9%9D%A2%E8%AF%95%E5%87%86%E5%A4%87%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[计算机网络 TCP/IP卷一（一定要看，优先看17-24章tcp那块的内容，看完之后，对TCP的理解会提高很多） TCP和UDP区别,分别适用于哪些场景 三次握手和四次握手，是否可以两次握手，为何要四次握手？TIME_WAIT状态 Get和post区别 PAWS，几种定时器，close_wait状态。 必须能记住几张图，并且知道大多数细节 TCP拥塞控制。 操作系统 IPC PIPE 共享内存 信号量，介绍一下信号量和互斥锁 Socket 服务器客户端通信 多线程 多进程模型 IO多路复用 select poll epoll 线程和进程有什么区别？ 协议栈实现原理推荐 阿里陶辉 的博客 。网络编程基础推荐，游双的linux高性能服务器编程，最好理解全文。 Java 如何在Java中实现线程？ java多线程会有同步的问题，讲一下synchronized和lock的区别？ 知道cookie么？讲一下（cookie是重要考点，一定要可以讲的超级清楚） jvm:full gc,内存泄漏，CMS收集器 jvm内存区域和GC，finalize方法，哪些对象可以作为GC Root NIO的DirectByteBuffer和HeapByteBuffer，同步和异步，阻塞和非阻塞 ReentrantLock源码，AQS，synchronized实现，乐观锁和悲观锁，volatile，ThreadLocal原理，线程池工作流程 HashMap和ConcurrentHashMap基本原理，扩容机制等 jre和jdk的区别 jvm内存模型。每个区什么用。区别。 讲下堆的作用。 堆的垃圾回收对象选择原则。 垃圾回收算法。谈到了年轻代和年老代。 年轻代怎么分。为何分为eden和s区。大小比例为何是1：8,还有别的区分方法吗？各个代的垃圾回收算法。 是否可以在static环境中访问非static变量？ 数据库 innodb和myisam区别? B+树性质 多列索引及最左前缀原则和其他使用场景 发散问题 Linux命令相关，问有一个文件A.txt，里面有许多行，找出其中带关键字’B’的行，并统计重复度。我问了下重复度是指啥，他说，这样吧，假设每一行都是由空格分隔开的若干字符，若整个文件中，有2行的最后一个字符都是’10’，你就输出 “10” : 2。 假如在服务器上执行一个进程时，你发现服务器很卡顿，你会怎么查找原因。感觉这道题真有点坑，本来就有点蒙，针对你的一些回答，还会多问一些东西。 有8个小球，其中七个的重量是相同的，有一个较轻。给你一个天平，问秤几次能找出那个较轻的小球，若天平只能秤两次，又该怎么秤（这个问题当时居然没答出来，真是悲剧，后来回去再想了想，发现其实很简单的。。。） 参考面经http://m.nowcoder.com/discuss/26140?type=0&amp;order=0&amp;pos=12&amp;page=1http://m.nowcoder.com/discuss/25665https://www.nowcoder.com/discuss/25226https://www.nowcoder.com/discuss/22089]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[看过这两张图，就明白 Buffer 和 Cache 之间区别[转载]]]></title>
    <url>%2F2017%2F04%2F26%2F%E7%9C%8B%E8%BF%87%E8%BF%99%E4%B8%A4%E5%BC%A0%E5%9B%BE%EF%BC%8C%E5%B0%B1%E6%98%8E%E7%99%BD-Buffer-%E5%92%8C-Cache-%E4%B9%8B%E9%97%B4%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[看过这两张图，就明白 Buffer 和 Cache 之间区别]]></content>
      <categories>
        <category>缓存</category>
      </categories>
      <tags>
        <tag>缓存</tag>
        <tag>buffer</tag>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS集中式的缓存管理原理与代码剖析[转载]]]></title>
    <url>%2F2017%2F04%2F26%2FHDFS%E9%9B%86%E4%B8%AD%E5%BC%8F%E7%9A%84%E7%BC%93%E5%AD%98%E7%AE%A1%E7%90%86%E5%8E%9F%E7%90%86%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[HDFS集中式的缓存管理原理与代码剖析]]></content>
      <categories>
        <category>缓存</category>
      </categories>
      <tags>
        <tag>HDFS</tag>
        <tag>集中式缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式系统面试清单]]></title>
    <url>%2F2017%2F04%2F26%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E9%9D%A2%E8%AF%95%E6%B8%85%E5%8D%95%2F</url>
    <content type="text"><![CDATA[操作系统系统（基本知识） GFS（high avaialbe, scalable, data replication, erasure code） ext4 (disk layout, io scheduler, performance tunning) 了解 btrfs(the last file sytem, which is now being developed. You take some knowledge about the disk layout, snapshot, data integrity. You can search btrfs wiki on google ) 了解 os kernel(page cache) 了解 进程(通信机制包括消息队列，共享内存, pipeline等，进程线程区别) 存储（基本知识、设计） 内存 k-v(redis) (memory data structure, hash algorithm, distributed data partition alogrithm, data avaiable) 分布式数据库(Hbase) (CAP, BASE, multi-version concurrent control, WAL, LSM tree) MySQL (InnoDB log, InnoDB data structure) 网络 TCP/IP 三次握手(three-way handshake, four-way finalization, TCP reset packet, TCP timeout, TCP RTT, TCP MSL, TCP state machine) zero copy (sendfile system call, the os kernel data copy path) RPC (epoll, select, poll mechanic, interrupt mechaic) 数据分发 路由 数据迁移 (load balance, multi-tier data storage including memory, ssd, disk) 计算 mapreduce(Hadoop, Spark) (RDD, Shuffle, distributed computing framework task scheduler, which includes FAIR, FIFO, CFQ) 流计算(Flink, HERO, Spark Streaming) (difference between real-time computing and batch compputing) 图计算 (GraphX, Dremel) (BSP, SSP model) 高并发 生产者、消费者模式 无锁数据结构(linkedlist, map) (wikipedia word item, Consistency) CAS (volatile cost overhead) 可重入 (reentrant wikepeida’s word item) kafka raftor模式]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>分布式系统</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka深度解读[转载]]]></title>
    <url>%2F2017%2F04%2F26%2Fkafka%E6%B7%B1%E5%BA%A6%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[kafka数据可靠性深度解读]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>kafka</tag>
        <tag>可靠性</tag>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bandwidth教程]]></title>
    <url>%2F2017%2F03%2F07%2Fbandwidth%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本文主要讲述一下内存吞吐量的测试。在开发，特别是压力测试的时候，需要知道测试的瓶颈在哪里(是在内存、网络还是磁盘呢？)，所以就需要对内存、网络以及磁盘的真实的吞吐量有一定的了解。本文主要讲述内存顺序随机等读写测试。Bandwidth[1]是一个内存测试工具。本文主要讲述bandwidth安装以及测试步骤，以及结果查看方式。 1、点击下载[2]之后。解压到目录下。 2、安装nasm以及gcc。yum install nasm； yum install gcc 3、编译：输入make会提示你在不同机器上不同的编译指令。在centos x86_64 上面的命令是: make bandwidth64 备注：第3步有可能出现stropts.h: No such file or directory的错误。解决方法就是此文章[3]说的[编译程序时，出现了这个错误，因为linux不支持STREAMS，缺少这个文件。stropts.h是POSIX XSR的一部分，但是linux不支持。解决办法很简单，在/usr/include目录下创建一个空的stropts.h文件.] 4、运行：当编译成功后，直接运行./bandwidth64 即可。会产生一个图片文件。你还可以重定向把日志输出到一个日志文件中，以便查看日志。./bandwidth64 &gt; out.log 5、运行结果查看上图可以看出来在三个地方曲线有明显的转折。分别是32k、256K、12MB。这分别是三级缓存的大小。就跟下图所示一样 也就是说：当读写的size大小落在相应的cache里面的时候，吞吐量是最大的。当读写的大小的size超过这个cache，而落入到下一个cache的时候，吞吐量就会降低。内存是最低的。 名字解释：non-temporal[4]. 说白了non-temporal就是读写数据不经过cache，而是直接去内存中。所以，反应的能力就是内存的能力，而不是cache的能力。 [备注]non-temporal解释：Non-Temporal SSE instructions (MOVNTI, MOVNTQ, etc.), don’t follow the normal cache-coherency rules. Therefore non-temporal stores must be followed by an SFENCE instruction in order for their results to be seen by other processors in a timely fashion. When data is produced and not (immediately) consumed again, the fact that memory store operations read a full cache line first and then modify the cached data is detrimental to performance. This operation pushes data out of the caches which might be needed again in favor of data which will not be used soon. This is especially true for large data structures, like matrices, which are filled and then used later. Before the last element of the matrix is filled the sheer size evicts the first elements, making caching of the writes ineffective. For this and similar situations, processors provide support for non-temporal write operations. Non-temporal in this context means the data will not be reused soon, so there is no reason to cache it. These non-temporal write operations do not read a cache line and then modify it; instead, the new content is directly written to memory. 参考文献 [1] http://zsmith.co/bandwidth.html [2] http://zsmith.co/archives/bandwidth-1.3.2.tar.gz [3] http://blogger.org.cn/blog/more.asp?name=hongrui&amp;id=52186 [4] http://stackoverflow.com/questions/37070/what-is-the-meaning-of-non-temporal-memory-accesses-in-x86]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>bandwidth</tag>
        <tag>内存测试</tag>
        <tag>memory benchmark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解JAVA虚拟机(2)-Java 内存区域与内存溢出异常]]></title>
    <url>%2F2017%2F02%2F16%2FJava-jvm-2%2F</url>
    <content type="text"></content>
      <categories>
        <category>深入理解JAVA虚拟机</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>虚拟机</tag>
        <tag>内存溢出</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[鸡汤-TED]]></title>
    <url>%2F2017%2F02%2F15%2F%E9%B8%A1%E6%B1%A4%2F</url>
    <content type="text"><![CDATA[20岁光阴不再来: Thrity is not the new 20, so claim your adulthood, get some identity captial, use your weak ties, pick your family. Don’t be defined by what you didn’t know or didn’t do. You’re deciding your life right now. 人生就是一列开往坟墓的列车，路途上会有很多站，很难有人可以自始至终陪着走完。当陪你的人要下车时，即使不舍也该心存感激，然后挥手道别。 如何掌控你的自由时间]]></content>
      <categories>
        <category>鸡汤</category>
      </categories>
      <tags>
        <tag>鸡汤</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[索引优化与设计(4)-为SELECT语句创建理想的索引]]></title>
    <url>%2F2017%2F02%2F12%2Findex-4%2F</url>
    <content type="text"><![CDATA[本章节主要覆盖如下内容： 影响表以及索引扫描性能的主要因素列表 随机/顺序读时间以及CPU成本 根据三个最重要的需求为查询语句的索引指定星级 三星索引的设计-最理想的索引设计 宽索引 为查询语句设计最好索引的算法 根据现存索引情况设计最使用的索引，将CPU时间、磁盘读时间和耗费时间(elapsed time)考虑在内 从维护开销来看，对现有索引进行所建议的改变可能产生的结果 响应时间、驱动负载和磁盘代价 一些建议 简介在SQL查询中，当程序中的SQL使用了一个或者多个索引的时候，许多的DBA就会对此表示满意，但是，使用一个不合适的索引有可能会导致比全表扫描更差的性能。 接下来，本章就会重点分析这其中的缘由，首先，给出我们接下来分析所依赖的前提。 磁盘以及CPU时间的基础假设下图是给出的磁盘和CPU的基础假设。 评注：由于作者写作的时间是2005年，所以这些数据有的已经发生变化，磁盘顺序读可以达到100M/S。具体请参考磁盘性能指标–IOPS、吞吐量及测试[1]。关于CPU time的定义，请参考此文章[2] 不合适索引(inadequate index)此处给出了一个例子，就是说明，使用索引并不一定比全表扫描好。假设有如下的一个查询语句，仅有两个合理的访问路径：1、使用索引(LNAME)2、全表扫描 索引列是LNAME 和 FNAME,并且过滤因子是1%。在这个查询语句中，显然只能够使用索引列LNAME。 对于第一种情况来说，DBMS会选择谓词条件LNAME=:LNAME扫描索引片。对于索引片中的每一个索引行，DBMS都必须回到表中校验CITY字段的值。由于表中的行是根据CNO字段而不是LNAME字段来聚簇的，所以这个校验操作需要做一次磁盘随机读[评注:即先读取了索引数据，然后再去表中进行随机读]。 假设索引(LNAME,FNAME)的总大小是1000,000&times;100byte = 100M。包括数据以及分散的空闲时间，另外，在假设顺序读是40MB/s[评注：现在可以达到100MB/s].那么读取宽度为1%的索引片，即1MB，需要花费10ms+1MB/40MB/s=35ms。这显然没有问题，但是10,000次[评注：因为索引总条数为1000,000条，过滤因子为1%，所以结果条数为10,000.相应的，去表中随机读的次数就为10,000次]随机读将花费10,000*10ms=100s。所以，总的花费时间为100s+35ms。这种方式太慢了。 对于第二种方式来说，只需要第一个页是随机读。如果表的大小为1000,000&times;600byte=600MB,包括分散的空闲时间，那么花费的IO时间将会是10ms+600MB/40MB/s=15s，虽然仍然比较慢，但是相比使用索引已经变得快多了。 第二种方案的CPU时间将会比第一种方案的CPU时间长很多，因为DBMS必须对1000,000行而不是20,000行[评注：索引10,000行；数据表10,000行]，并且还需要对这些行进行排序[评注：因为SQL语句有ORDER BY]。从另一个角度来说，由于是顺序读，CPU时间可以与IO时间交叠。在这个场景下，全表扫描比在不合适的索引上扫描要快很多，但是这还不够快，仍然需要一个更好的索引。 三星索引(three-star index)-查询语句的理想索引前面讨论了一个非常不合适的索引，这一小节，我们来讨论 # 参考文献[1] http://wushank.blog.51cto.com/3489095/1708168[2] https://www.techopedia.com/definition/2858/cpu-time]]></content>
      <categories>
        <category>索引优化与设计</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>索引</tag>
        <tag>三星索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[索引优化与设计(3)-SQL处理过程]]></title>
    <url>%2F2017%2F02%2F11%2Findex-3%2F</url>
    <content type="text"><![CDATA[本章主要讲解了一个查询语句过来之后，DBMS内部是如何处理这一过程的。但是本章并没有细致的讲解，主要是给出了一些术语和基本的概念。本章覆盖的术语主要如下： SQL处理的一些概念 谓词(Predicates) 优化器以及访问路径(Optimizers and access paths) 索引片(Index slices)，匹配索引扫描(matching index scans)，匹配列(matching columns)，索引过滤(indexscreening)和过滤列(screening columns) 优化器何时确定访问路径 监控优化器 使用统计信息和所需FETCH调用的次数来指导优化器的行为 过滤因子(filter factor)。选择性(selectivity)和基数(cardinality)的概念以及对索引设计的影响 结果集物化(materialization)以及影响 谓词说白了谓词就是WHERE后面的条件 优化器以及访问路径用户一条查询语句下来，用户并不需要关系这条语句内部怎么执行的。但是这部分东西是交给DBMS来做的。在SQL语句真正能够被执行之前，优化器必须确定如何访问数据，这包括：应该使用哪一个索引，索引的访问方式如何以及是否需要辅助式随机读等等。所有的这些细节都包含在访问路径中。 索引片及匹配列索引片，说白了感觉跟谓词的个数有关系。如果WHERE后面只有一个谓词，那么显然索引片就只有一个。所以，有另外一种使用较为广泛的描述索引片的方法是定义索引匹配列的数量。 访问路径的成本很大程度上取决于索引片的厚度，即谓词表达式所确定的值域范围。索引片越厚，需要顺序扫描的索引页就会越多，需要处理的索引记录也就越多，当然最大的开销还是来自于对表的同步读操作上。相应的，如果索引片比较窄，就会显著减少索引访问的那部分开销，但是主要的成本节省还是在更少的对表的同步读取上面。 索引过滤以及过滤列有时候，列可能既存在于WHERE子句中(也就是谓词中)，也存在于索引中，但是这个列却不能参与索引片的定义(这里面我也没有弄明白为什么，希望能够在接下来的学习中明白其原因，此处借用原作者的一句话就是：现在我们只需要知道并不是所有的索引列都能够定义索引片的大小)。不过这些列仍然能够减少对表进行同步读操作的次数，所以这些列仍然扮演着重要的角色，我们称这些列为过滤列(screening columns)。 那么什么样的谓词能够定义索引片的大小；什么样的谓词不能够定义索引片的大小，只能够作为过滤列呢？附录1给了简单的一个例子。 访问路径中的术语(Access Path Terminology)在这里需要提及另外一个术语，就是执行计划(execution plan)。这两个术语比较相近，在本书中。使用访问路径来描述数据访问的方式，使用执行计划来描述DBMS提供的EXPLAIN工具所产生的输出结果。接下来进入我们关注的重点。 匹配谓词有时候也称作为范围限定谓词(range delimiting predicates)。当有合适的索引存在时，如果优化器能够识别到某些谓词为匹配谓词，那么我们就称这个谓词是可索引的(indexable,有时也叫可搜索的(sargable))。 SQL Server中使用表查找(table lookup)这一术语来描述使用索引并且需要读取表行的访问路径。这不同于使用索引的访问路径。消除表访问的最显而易见的方式就是将缺失的列添加至索引上。许多SQL Server的书籍将这种能够避免某个SELECT调用的表访问的索引成为覆盖索引(covering index)。[评注：很多NoSql数据库也是这么做的，例如HBase上面的Phoenix就有这种技术，CCIndex[1]论文就是在分布式顺序表上面提供覆盖索引(聚簇索引)的典型技术之一]。 监控优化器当发现一个慢SQL的时候，首先被怀疑的对象就是优化器，可能优化器选择了错误的访问路径。关系型数据库中的DBMS提供了一个叫做EXPLAIN的工具，用于解释优化器决定使用某个访问路径的原因。 帮助优化器(统计信息)优化器是否能够做出很好的决定取决于优化器进行成本估算的时候所使用的信息是否完整。正常情况下，优化器默认能够使用采集的信息包括如下：每张表的记录数和表页数、叶子页数、每个索引的聚簇率、某些列或者某列组的不同值个数（基数），以及某些列的最大最小值。其他可选的统计信息的选项能够提供更多关于列和列组的值分布情况。 过滤因子(filter factors)说白了就是描述谓词的选择性，即表中满足谓词条件的记录行数所占的比例，它主要依赖于列值的分布情况。 物化结果集物化结果集意味着执行必要的数据库访问来构建结果集。在最好的情况下，只需要简单的从数据库缓冲池向应用程序返回一条记录。在最坏的情况下，数据库管理系统需要发起大量的磁盘读取。 参考文献[1]Zou Y, Liu J, Wang S, et al. CCIndex: a complemental clustering index on distributed ordered tables for multi-dimensional range queries[C]// Ifip International Conference on Network and Parallel Computing. Springer-Verlag, 2010:247-261. 附录附录1]]></content>
      <categories>
        <category>索引优化与设计</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>索引</tag>
        <tag>谓词</tag>
        <tag>访问路径</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[技术博客收藏]]></title>
    <url>%2F2017%2F01%2F19%2Fcollect-blog%2F</url>
    <content type="text"><![CDATA[HDFS集中式的缓存管理原理与代码剖析 看过这两张图，就明白 Buffer 和 Cache 之间区别 Pinpoint is an open source APM (Application Performance Management) tool for large-scale distributed systems written in Java]]></content>
      <categories>
        <category>收藏博客</category>
      </categories>
      <tags>
        <tag>技术收藏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[索引优化与设计(2)-表以及索引的内部组织]]></title>
    <url>%2F2017%2F01%2F10%2Findex-2%2F</url>
    <content type="text"><![CDATA[本篇主要分析以下几个问题 索引表和原表的物理组织是什么样子的； 索引和表的页(page)、索引和表的行(rows)、缓冲池(buffer pool)以及disk cache使用的数据结构； 磁盘顺序IO和随机IO的特性； 一些辅助式顺序或随机读：包括跳跃式顺序读(skip-sequential), 列表预读(list prefetch)以及数据块预读(data block prefetching); 同步IO和异步IO的重要性； 各种数据库管理系统的相同点和不同点； 页和表聚簇，索引行，索引组织表以及页邻接 B-Tree的代替者 位图索引(bitmap-index)和哈希(hash) 介绍首先要明确的是，在数据库系统中，索引和数据中的数据全部在一个叫做页(page)的结构中，一般页的大小为4KB，这个大小并不是唯一的。页的大小影响着每个页中的数据的条数(record)以及总的页的数量。并且，每个页都会留出一定比例的空余空间，以满足向其添加新的表行或是索引行的需求。 索引行（index rows）索引列一般分为两种，根据索引列的值的唯一性分为唯一的和不唯一的两种。主键就是其中一个特殊的唯一的索引列，像学生的学号，或是身份证号码等，也一般都是唯一的，不唯一的就更多的了。对于唯一的索引列来说，一个索引行等同于叶子节点中的一个索引条目，这个索引条目的值就是索引列的值，然后每个索引条目都有一个指向原表中记录的指针。一般来说，页表编号(table page number)是这个指针的一部分。对于非唯一索引来说，一个特定的索引值所对应的索引行应该被想象成独立的索引条目(individual index entries),每一个都含有相同的值，但是却有不同的指针。大多数情况下，非唯一索引的实际存储方式是一个相同的值后带着多个不同的指针，指向原表中的记录。之所以将这些非唯一的索引记录想象成独立的索引条目的好处将在后面介绍。 索引结构(Index structure)索引的结构一般是B-Tree结构，非叶子节点包含着一个键值以及指向下一层节点的指针，该键值是下一层页的最大值，如图1所示。B-Tree 是一种平衡索引树，因为通过这种索引来查找任何一条记录都需要访问相同数量的非叶子页。 表行(Table Rows)图1中的每一个索引行都指向了表中的一行记录，这个指针通常都是能够定位这个记录所在的页以及它在页中的位置。同时表中的每一行除了存储行的字段之外，还包含了一些控制信息用于定义行并帮助DBMS处理插入或是删除的操作。 表行在表中存储的顺序，可以和某一个索引的顺序相同，我们平常都叫这个索引为主键。很显然，在众多的索引中，只能够有一个索引的顺序和表的顺序一模一样。其他的索引就无能为力了。这个和表中的数据一个顺序的索引（主键），可以加快查找的过程。而且非常的高效。其他的索引就没有这个高效。。为什么呢？举例来说，在一个没有和表中的数据保持同一个顺序组织的索引，第一条索引行可能指向了数据表的页17，第二个索引行指向了页2，第三个索引行指向了页85…等等。现在虽然索引是顺序的。但是因为索引和数据表并不是按照相同的顺序组织的，使得去访问数据表的时候变成了随机的。这就会非常的低效。 缓冲池和磁盘IO说白了，缓冲池就是为了能够加快响应时间，并且减少磁盘IO的。如果每次访问的页都能够在缓冲池中，那就不用去磁盘上去找了，显然减少了很多的磁盘IO.缓冲池就是干这个的。你现在仅仅需要知道，数据在不在缓冲池中，访问的成本是不一样的！ 从DBMS中的缓冲池中读数据如果一个索引或是表的页在缓冲池中在找到，那么很显然就不需要去磁盘访问了。只需要处理这些索引或是表中的记录就可以了。成本的大小取决于这些记录是否是DBMS想要的。如果不是，只需要很少的处理；如果是，可能需要稍微多一点的处理。 从磁盘驱动器进行的随机IO图2展示了一个页从磁盘中读到缓冲池中所需要等待的时间。这个时间大约是10ms。有一点需要注意的是，一个页包含很多的记录，不管你是访问一个页中全部记录，还是部分记录，甚至仅仅是其中一条的记录，你都需要把一整个页全部load到缓冲池中。所以花费都是一样的，大约是10ms。这10ms又是怎么来的呢？ 图3很好的解释了这10ms的来历。从图中可以看到，我们假设在这10ms中磁盘实际的繁忙时间是6ms。1ms左右的传输时间是将页从磁盘服务器缓存(disk server cache)移至数据库的缓冲池中. 其余的3ms是对于可能发生的排队时间的估计值，这是基于每秒50次读取的磁盘活动情况得出的。这些数字在实际情况中可能有变化，但是我们需要记住这10ms这一个粗略但是合理的数字即可。 从磁盘服务器缓存(disk server cache)进行读取磁盘服务器一般都会自带memory(cache)以降低响应时间上的巨大成本。图4展示了从磁盘服务器缓存读取一个表或者索引页的过程。磁盘缓存的作用跟数据库缓冲区的作用一样，磁盘服务器试图将频繁使用的数据保留在内存(cache)中。以降低高昂的磁盘读成本。若DBMS所需的页不在数据库缓冲池中，继而会向磁盘服务器发起请求，磁盘服务器会判断该页是否在反服务器缓存中，只有当它发现页不在磁盘缓存中的时候才从磁盘驱动器上读取该页。如果该页在磁盘服务器的缓冲中，那么花费的时间将会从10ms大幅降低为1ms。 所以，总而言之，当一个索引或表页被请求的时候，它的理想位置是在数据库的缓冲池中。如果它不在那儿，那么下一个最佳的位置就是磁盘服务器的缓存。如果它也不在那儿，那么就必须从磁盘驱动器上进行一个很慢的读取，这一过程可能要花费很长的时间等待磁盘设备空闲下来。 从磁盘驱动器进行的顺序读取到目前为止，我们仅仅考虑到了将一个索引页或是表页读取到缓冲池中。实际中，不仅仅是读一个单页那么简单。有时候要读很多的页到缓冲池中，并且顺序的处理他们。图5展示了这四种场景。DBMS会意识到多个索引或表页需要被顺序的读取，且能够识别出那些不在缓冲池中的页，随后，它将发出多页IO(multiple-page I/O)的请求，每次请求的页的数量由DBMS决定。只有那些不在缓冲池中的页才会被从磁盘服务器中读取，因为那些已经在缓冲池中的页可能包含了尚未被写入磁盘的更新数据。 顺序的读取页有两个非常重要的优势： 同时读取多个页意味着平均每个页的读取时间将会很少。在当前磁盘服务器的条件下，对于4KB大小的页而言，这一个值可能会低至0.1ms(假设磁盘是40MB/s，则一秒能够读 40MB/4KB=10000个页，则每个页花费=1s/10000=0.1ms. 现在磁盘速度已经能够达到100MB/s了，40MB/s还是2004年的数据) 预读，由于DBMS事先知道要读取哪些页，所以可以在页被真正请求之前就提前将其读取出来。 图5所使用的术语索引片(index slice)以及聚簇索引(clustering index)将在后面讨论。用于代指上述所描述的顺序读取的术语包括：顺序预读(Sequential Prefetch),多块IO(Multi-Block I/Os)以及多重顺序前读(Multiple Serial Read-Ahead Reads) 辅助式随机读(Assisted Random Reads)我们从上文中已经看到随机读的代价，并且知道了数据库的缓冲池以及磁盘缓存的作用和好处来降低成本的。还有一些其他的场景也能够降低成本。有时候是自然发生的，有时候是优化器有意为之，我们把它统称为辅助式随机读.注意这并不是DBMS中的术语 自动跳跃顺序读(Automatic Skip-Sequential)从定义的角度看，如果一系列不连续的行被按照同一个方向扫描，那么访问模式将会是跳跃式顺序的。于是，每行的平均IO时间自然比随机访问时间短，跳跃的距离越短则越节省时间。比如，当表行是通过一个聚簇索引读取时候并且筛选掉一些行的时候，访问模式就是跳跃式顺序读的。这带来的好处有以下两个方面。 磁盘服务器注意到对某一驱动器的访问是顺序的(或者几乎是顺序的)，于是服务器开始提前预读几个页。 DBMS可能注意到select语句正在以顺序或是几乎顺序的方式访问索引或页表，于是DBMS开始提前预读多个页，这在DB2 for z/OS中被称为动态预读。 列表预读(List Prefetch)在之前的例子中，由于索引行和表行的顺序都是一致的，所以很方便的做到了自动跳跃顺序读。事实上DB2 for z/OS 也能够在索引行和表行顺序不一致的情况下主动创造跳跃式顺序读。为了做到这一点，它事先需要访问满足条件的所有的索引行。然后按照表页的顺序对其进行排序后在访问表行。图2-6和图2-7对不使用和使用列表预读进行了对比，图中的数字代表了操作顺序。 数据块预读(Data Block Prefetching)当表行和索引行的访问顺序不一致的时候，Oracle中就会使用数据块预读这一特性。在这种方式下，如图2-8所示，DBMS首先从索引片上收集指针，然后在进行多重随机IO来并行的读取表行。如果第4，5，6步所代表的表行分别位于三个不同的磁盘驱动器上，那么这三个随机IO将会被并行执行。就像列表预取一样。注释：数据库预读就是并行的列表预读。 在结束辅助式随机读的话题之前，还需要考虑一下结果集的顺序。一个索引也许能够自动提供正确的顺序，但是上述所讨论的机制也许会在访问表行之前就破坏了这一顺序，因此，也就需要对结果集进行一次排序。 注释：就像设计HBase中IRIndex中索引预读，然后进行排序一样。 评注本书主要提及三种类型的读IO操作:同步读、顺序读以及辅助式随机读。需要提及的是SQL Server使用术语索引前读(Index Read-Ahead), Oracle使用术语索引跳跃扫描(Inedx Skip Scan)。前者是指提前向前读取下一组叶子页，而后者是指读取多个索引片而不进行全索引扫描。 辅助式顺序读(Assisted Sequential Reads)当要扫描一个大的表的时候，优化器可能会选择开启并行机制。例如，它可能会将一个游标拆分为多个用范围谓词限定的游标，每一个游标扫描一个索引片。当有多个处理器和磁盘驱动器可用的时候，所花费的时间将相应的减少。我们将在第15章讨论这个话题。请注意，辅助式顺序读这个术语同样也未被任何一个DBMS使用过。 同步和异步IO(Synchronous and Asynchronous I/Os)术语同步是指在进行IO操作的时候，DBMS不能继续进行其他的操作，必须等待，直至IO操作完成。 异步读指的是当前页尚在处理的时候就被提前发起了，这一处理时间和IO时间之间可能有很大的一部分重叠。理想情况下，在这些页被实际处理之前，异步IO就已经完成了。每一组页都是以这种方式被预读然后再处理。图2-9展示了这一过程。注意的是，整个预读过程从一次同步读开始，然后才开始预读过程，以此来最小化首次读取的等待时间。 #]]></content>
      <categories>
        <category>索引优化与设计</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[索引优化与设计(1)-引言]]></title>
    <url>%2F2016%2F12%2F30%2Findex-1%2F</url>
    <content type="text"><![CDATA[本系列根据Relational Database Index Design and the Optimizers而写的读书笔记，这是第一篇，主要针对的是第一章。第一章主要介绍了传统的索引的设计的误区。随着磁盘的增大，磁盘访问时间的降低以及内存增大，以前用在索引设计上的“圣经”被证明是不对的，或是有缺陷的。本书从这一点出发，逐步讲述了索引设计以及优化的问题。 几个术语第一章提到了几个术语，下面首先解释下以下几个术语。便于后面行文的理解。 buffer pool这个是数据库中的概念，笔者参考了InnoDB[1]以及IBM DB2[2]概念和MySql技术内幕[3],buffer pool就是临时存放一些数据库page的内存区域。 InnoDB存储引擎是基于磁盘的，并且将其中的记录按照页(page)的方式进行管理。因此可以将其视为磁盘的数据库系统(Disk-base Database)。在数据库系统中，由于CPU的速度和磁盘速度之间的鸿沟，基于磁盘的数据库系统通常使用缓冲池技术来提高数据库的整体性能。 缓冲池简单来说就是一块内存区域，通过内存的速度来弥补磁盘的速度较慢对数据库的影响。在数据库中进行页的操作的时候，首先将从磁盘读到的页放到缓冲池中，这个过程称为将页“FIX”在缓冲池中。下一次再次读取的时候，首先判断该页是否在缓冲池中。若在缓冲池中，称为该页在缓冲池中命中，直接读取该页。否则，读取磁盘上的页。 InnoDB默认buffer pool的大小是128M[4]，可以存放好几百的page，甚至可以达到GB，这个值可以调节。 disk cache同样的，也是用RAM来存放部分磁盘数据的一部分区域[5]。这个RAM可以存在于磁盘上(这也称为hard disk cache or buffer)[6]，或者是普通的内存的RAM(这也称为soft disk cache)。hard disk cache更加的有效，因为直接是在磁盘上的一小部分区域，但是更贵，所以，会相对来说比较小。几乎所有的HDD都会有一部分很小的hard disk cache(8-256MB)，SSD的可能达到1GB[6]。 disk cache的作用也是将一些从磁盘读取的数据先存到disk cache中，避免了再次去disk读取耗费很长的时间。作用和原理跟buffer pool一样的。 那么问题来了，buffer pool和disk cache既然作用都是相同的，那么他们的区别在哪儿呢？这个问题找了很多的资料，还是没看懂，所以，这个后面有时间在详细研究。 笔者先尝试着解答上面的问题，参考了以下资料[7,8],个人认为这里面的buffer pool是数据库的概念。而disk cache是文件系统的概念。但是作用都是一样的，所以是没有差别的。但是假设都是操作系统的概念的话，那么就有差别了。差别就是文献7,8里面讲的。 几个索引的误区第一章主要分析了几个以前索引设计的误区，例如，index层数(主要针对B-Tree来说的)最好不要超过5层，但是作者通过分析，如果把非叶子节点存放到磁盘中，那么只要访问非叶子节点，都会造成磁盘IO(10ms，这个请参考Jeff Dean的一些数据，本文给贴出来了，图1),所以索引的层数并不是很重要。还有一个就是每个表中最好不要多余6个索引列。以及不要在列值经常变化的列上面建立索引等。这些都被后面的例子证明是错误的。 总结第一章主要给出了一些引言，本文主要着重分析了几个术语便于以后的理解。至于索引的优化细节，本文会继续跟随着作者章节的脚步慢慢深入分析。 参考文献 [1]https://dev.mysql.com/doc/refman/5.5/en/innodb-buffer-pool.html [2]http://www.ibm.com/support/knowledgecenter/SSEPGG_9.5.0/com.ibm.db2.luw.admin.dbobj.doc/doc/c0052482.html [3]MySql技术内幕 p35 [4]https://dev.mysql.com/doc/refman/5.5/en/innodb-parameters.html#sysvar_innodb_buffer_pool_size [5]http://www.webopedia.com/TERM/D/disk_cache.html [6]https://en.wikipedia.org/wiki/Disk_buffer [7]https://www.quora.com/What-is-the-major-difference-between-the-buffer-cache-and-the-page-cache [8]https://www.quora.com/What-is-the-difference-between-Buffers-and-Cached-columns-in-proc-meminfo-output]]></content>
      <categories>
        <category>索引优化与设计</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器监控cacti与snmpd配置]]></title>
    <url>%2F2016%2F12%2F18%2Flinux-monitor%2F</url>
    <content type="text"><![CDATA[做系统的人，都少不了运维，也少不了对系统的监控，特别的想监控系统的CPU使用情况，磁盘使用情况以及内存使用使用情况，这三座大山(CPU,memory,disk)是必须的，所以，能够对这三个东西进行非常好的监控是一件非常好的事情，本文主要讲解监控这三个东西的软件cacti以及snmpd的配置。 在被监控机器上安装snmpd sudo apt-get install snmpd sudo apt-get install snmp sudo apt-get install snmp-mibs-downloader sudo download-mibs 然后在按照参考[1]中的安装即可。注意，然后按照[2]中的连接，运行如下命令查看是否可以。 snmpwalk -v2c -c myCommunity ip hrStorageTable 如果出现一下错误：dskTable: Unknown Object Identifier则请注释掉 /etc/snmp/snmp.conf 下面的mibs :。 变成如下： 在采集数据的机器上安装 cacti请参考[2]即可 安装disk io 监控软件请参考[2] 错误解决 最常见的错误就是dskTable: Unknown Object Identifier，此时按照上文所说的解决应该就可以。 图表不显示；有可能是因为服务器时间没有同步造成的。所以，最好运行下 ntpdate time.nist.gov 参考文献[1]http://xmodulo.com/monitor-linux-servers-snmp-cacti.html[2]https://nsrc.org/workshops/2014/pacnog16-ws/raw-attachment/wiki/Track2Agenda/4.3.2.exercises-cacti-disk.htm#hrstoragetable-.1.3.6.1.2.1.25.2.3[3]http://1161192890.blog.51cto.com/2041190/1404181/]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>监控</tag>
        <tag>monitor</tag>
        <tag>cpu</tag>
        <tag>disk</tag>
        <tag>io</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分布式中的两阶段提交和三阶段提交]]></title>
    <url>%2F2016%2F12%2F11%2Ftwopc-and-threepc%2F</url>
    <content type="text"><![CDATA[传统的关系型数据库以及很多的NewSql在事物处理方面都采用了两阶段提交协议。例如巨杉数据库[1,2]，以及小米在HBase上面利用coprocessor机制实现的强一致性的索引[3]。也就是说从关系型数据库到NoSql数据以及NewSql数据库，两阶段提交协议都在广泛的应用，那么两阶段体检协议到底是什么呢，到底神奇在什么地方呢？下面我们就在讲解一下两阶段提交协议以及它的变种，三阶段提交协议。 两阶段提交协议两阶段提交协议，从名字中也看得出来，分为两个阶段，第一阶段主要是投票阶段，第二阶段才是提交阶段。主要角色也分为两个，一个是参与者，一个是协调者。 协议步骤 第一阶段：投票阶段 协调者向所有的参与者发送一个VOTE_REQUEST的消息 当参与者接收到VOTE_REQUEST的消息后，就像协调者发送一个VOTE_COMMIT的消息通知协调者已经准备好提交本地事物，否则就返回一个VOTE_ABORT的消息。 第二阶段：提交阶段 协调者收到所有的参与者的返回的消息，如果所有的参与者都表决要提交事物的时候，那么协调者就会向每一个参与者发送一个GLOBAL_COMMIT的消息；但是只要有一个参与者表决要取消事物，那么协调者就会决定取消事物，并且多播一个GLOBAL_ABORT的消息给所有的参与者。 每个参与者等待协调者的返回消息，然后根据此消息做出最后的反应。如果参与者收到的是GLOBAL_COMMIT的消息，那么参与者就会提交本地事物；相反的，如果参与者收到时GLOBAL_ABORT的消息，那么参与者就会取消本地事物。 以上就是两阶段提交的具体步骤，可以看出来，两阶段提交其实并不复杂，也很简单。但是在分布式系统中，主要的问题就是容错，假设协调者或是其中任何一个参与者出错了会怎么办呢？下面就着重分析下，在两阶段提交的具体步骤中，每一个步骤会出现的错误以及解决办法。 首先给出2PC中的协调者与参与者的状态转换图,如下图所示（备注：横线上面的消息是来自对方的消息，横线下面的消息是对此收到的消息所做出的应答消息）图1 协调者状态转换图 图2 参与者状态转换图 容错阻塞那么下面分析2PC中的容错问题，因为此才产生了3PC，我们先从协调者这个角色开始分析，然后再分析参与者的阻塞问题 协调者阻塞情况 协调者在WAIT的时候阻塞，等待来自每个参与者的表决。这种情况的发生可能是参与者挂了，或是网络延迟造成的。解决办法就是：如果在某段时间内协调者没有收到所有的表决，那么协作者就决定中止表决，然后向所有的参与者发送GLOBAL_ABORT的消息。 参与者阻塞情况 参与者可能在INIT的状态时候阻塞，等待协调者的VOTE_REQUEST的消息。这也可能是由于协调者挂了，或是网络延迟造成的。解决办法：如果在某一段时间之内参与者没有收到协调者的这个消息，那么参与者就简单的在本地中止事物，并且向协调者发送一个VOTE_ABORT的消息。 参与者可能在READY的状态阻塞，等待协作者发送全局表决消息。这也可能是由于协调者挂了，或是网络延迟造成的。这个时候解决办法比较多，同时，也是因为这个状态阻塞后，事物的不确定性，才产生了3PC。一般的解决办法如下： 让每个参与者在协调者恢复之前一直阻塞 参与者P与参与者Q联系，然后根据Q的状态来决定做什么，那么Q主要有以下几种状态，同时每种状态的对应关系如下： Q状态为COMMIT，那么有可能就是协作者在崩溃之前把GLOBAL_COMMIT的消息发送给了Q，但是还没来得及发送给P，因此，P应该决定进行提交。 Q状态为ABORT，同样的，是因为协作者在崩溃之前把GLOBAL_ABORT的消息发送给了Q，但是还没来得及发送给P，所以，此时P应该决定ABORT。 Q状态为INIT，当协调者已经向所有的的参与者发送了VOTE_REQUEST消息，但是这个消息只到达P（然后它用VOTE_COMMIT消息作为应答），而没有到达Q时候，就是这种情况，换句话说，协调者在多播VOTE_REQUEST消息的时候崩溃了。在这种情况下，中止事务是安全的，P和Q都可以把状态转换为ABORT。 当Q处于READY状态时候，这个时候是最难判断的，特别的，如果所有的参与者都处于READY的状态，那么他们就无法做出决定。问题就是因为尽管所有的参与者都想要提交，但还是需要协调者的表决才能做出最后的决定。因此，这个协议在协调者恢复之前阻塞。 总结一下所有的Q的状态以及对应的解决办法如下： Q的状态 P采取的行为 COMMIT 转换到COMMIT ABORT 转换到ABORT INIT 转换到ABORT READY 与其他参与者联系 两阶段提交协议先写到这里，3PC之所以出现，也是因为2PC在上述表格中所有的参与者都处于READY的状态下，不知道如何进行事务；所以，才有了3PC。3PC的具体内容，以后有时间在聊。3PC在工业界用的不多。大都是2PC。 参考文献[1] http://www.sequoiadb.com/cn/[2]http://download.csdn.net/meeting/speech_preview/258[3]http://download.csdn.net/meeting/speech_preview/292]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>两阶段提交</tag>
        <tag>two pc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NewSql 崛起]]></title>
    <url>%2F2016%2F12%2F03%2FWhats-Really-New-with-NewSQL%2F</url>
    <content type="text"><![CDATA[本文主要讲述数据库的发展历程，以及实现一个数据库需要考虑的内容。 数据库发展史1960年IBM出现了第一个DBMS，叫做IMS，原因就是当时人们觉得应用应该和数据分离，这样开发者就可以把重心放到如何读数据以及取数据，就不用去考虑数据库开发的事情。 在1980年到1990年期间，人们发现这样的关系型数据库编程方式跟面向对象的编程方式违背（impedance mismatch），所以，就出现了面向对象的DBMS。但是面向对象的DBMS并没有火起来，而是出现了一些像XML等语言，这跟后来的document-oriented NoSQL很像(mongodb),后面会介绍NOSql的类型。 随着2000年之后，互联网的兴起以及数据量的增大，用户的并发性增大，以及要求时时on-line，对以往的关系型数据库提出了挑战。为了应对上述挑战，有如下两个解决办法： 使用更好的机器，更大的硬盘和CPU等。这样带来的好处很小，而且把一个数据库系统移到另外一个数据库系统工作量很大，而且比较繁琐。 数据库中间件，把一个数据库分布在一些廉价的机器上面，中间件负责做查询转发调度（请参考此文 分布式MySQL集群方案的探索与思考）但是这种方式对于跨表的join等事物支持不好。 nosql的发展。一些公司开始慢慢的向数据库中间件开始转向分布式DBMS。动机主要有以下三点： 以往的数据库都支持ACID特性，但是牺牲了可用性以及性能。但是现在的一些互联网应用更加关注可用性以及性能，他们希望能够支持更多的并发度以及on-line all the time. 很多时候，现在的web应用并不需要以前DBMS的那么多的性能作为存储。 现在的应用仅仅是简单的读写查询请求，关系型模型以及SQL复杂的语句对于现在的应用来说，就是杀鸡用牛刀。所以，NoSql崛起。NoSql主要有以下几种类型。 到2000年末的时候，虽然一些应用已经可以很好的应用到NoSql系统中，但是还有很多的应用并不能应用Nosql（比如金融系统或是订单系统），因为这些系统需要很强的一致性。而大部分的Nosql系统都是弱一致性(eventual consistent)，所以NewSql出现了。所以简单的讲，NewSql就是结合了Nosql数据库的可扩展性，同时又满足了关系型数据库中的事物特性(ACID)。 NewSql的特性新的架构Transparent Sharding MiddlewareDatabase-as-a-Service设计一个数据库系统需要考虑的问题Main Memory StoragePartitioning / ShardingConcurrency Control 两阶段锁(twophase locking (2PL) schemes) timestamp ordering (TO) concurrency control Secondary IndexesReplication active-active replication：把一个请求发到所有的副本上 activepassive replication：只把请求发给主副本上，当主副本完成后，在进行其他副本的复制。 Crash Recovery参考文献 Pavlo A, Aslett M. What’s Really New with NewSQL?[J]. Acm Sigmod Record, 2016, 45(2):45-55. Borkar D, Mayuram R, Sangudi G, et al. Have Your Data and Query It Too: From Key-Value Caching to Big Data Management[C]// International Conference on Management of Data. ACM, 2016.]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>NewSql</tag>
        <tag>big data</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka中topic创建流程]]></title>
    <url>%2F2016%2F07%2F01%2Fkafka-create-topic%2F</url>
    <content type="text"><![CDATA[本文主要讲述kafka topic的创建流程，所有的代码基于kafka_2.10-0.8.2.1。 kafka topic 创建流程首先，kafka topic的创建是有两种方式的，关于具体的方式请参考博文Kafka Topic Partition Replica Assignment实现原理及资源隔离方案.当用户执行如下面的代码的时候1$ bin/kafka-topics.sh --create --zookeeper zookeeperip:2181 --replication-factor 2 --partitions 10 --topic testtopic 打开kafka-topics.sh文件，会发现如下图片说明，创建topic的时候，走的是kafka.admin.TopicCommand接口，在这个接口中，所有的入口都是如下所示的main函数然后根据解析的命令，走到createTopic(zkClient, opts)方法，下面是createTopic方法的代码 由上面的代码可以看出，此处可以有两种创建topic的方式，如博文Kafka Topic Partition Replica Assignment实现原理及资源隔离方案所讲。实际上，第二种创建topic的方式就是由按照系统分配算法，来决定关于这个topic的所有的partition的副本数具体落在哪些broker上面。所以，第二种创建方式最终都会走到AdminUtils中的createOrUpdateTopicPartitionAssignmentPathInZK方法。所以，这里着重讲述第二种创建topic方式走的流程。我们看到，在TopicCommand中的createTopic中，首先解析了命令行中的partition的数量以及replicas副本的参数，并把这些参数传给了AdminUtils中的createTopic方法，下图是AdminUtils中的createTopic方法。在AdminUtils中的createTopic方法中，系统主要做了3件事，1：从zookeeper中获取所有的集群中broker的列表。2：根据第一步获取到的broker列表，以及传过来的partitions和和replicationFactor，按照系统分配算法对每个partition进行分配。3：当前两步完成后，把所做的更改在zookeeper中修改，然后根据zookeeper的callback(回调)，kafka自身会进行数据的迁移等等工作。 下面分别看下这三步，第一步，从zookeeper获取集群中所有的broker列表。进入到kafka.utils.ZkUtils类中的getSortedBrokerList方法，然后getSortedBrokerList调用java包中的ZkClient中的getChildren方法，传进去的参数path=“/brokers/ids”。一句话概况就是从zookeeper中获取/brokers/ids节点下面的所有的brokers。 关于第二步，分配策略。上面提及的博文讲解非常详细，具体请参考此博文。 下面，我们来看看第三步到底干了什么事情，下面是第三步执行的代码：从代码中可以看出，主要做的工作就是最后面的两个方法，writeTopicConfig以及writeTopicPartitionAssignment。writeTopicConfig最终走到了ZkUtils中的updatePersistentPath方法。而writeTopicPartitionAssignment最终走到了ZkUtils中createPersistentPath的方法，最后，ZkUtils最终跟java包中的ZkClient交互，修改或是添加zookeeper中节点的信息。kafka就是利用了zookeeper中的回调函数的概念，当zookeeper中节点变化的时候，kafka会感知变化，然后对数据做出相应的变化。 下面时序图是对上面分析的一个总结 小结有了上面的这些基础，所以，kafka中集群的概念就变的微不足道了。我们可以根据自己的生成策略，对topic的任意分区，可以指定到任意的机器broker上面，这就使得用户client有了很大的自主权利，特别是对于那些异构集群来说，或是某些业务场景比较重要，某些业务场景比较次要。这就可以对特定业务进行隔离，使得当不得不kill掉某些业务的时候，可以不牵扯到比较重要的业务。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>big data</tag>
        <tag>kafka</tag>
      </tags>
  </entry>
</search>
